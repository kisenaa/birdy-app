{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7264c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([1, 3, 224, 224]), torch.float32)\n",
      "[torch.onnx] Obtain model graph for `CustomDinoV2ClassifierWithReg([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `CustomDinoV2ClassifierWithReg([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 6 of general pattern rewrite rules.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 20},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.7.1+cu128',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"pixel_values\"<FLOAT,[1,3,224,224]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"logits\"<FLOAT,[1,131]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"backbone.embeddings.cls_token\"<FLOAT,[1,1,768]>{TorchTensor(...)},\n",
       "                %\"backbone.embeddings.register_tokens\"<FLOAT,[1,4,768]>{TorchTensor(...)},\n",
       "                %\"backbone.embeddings.position_embeddings\"<FLOAT,[1,1370,768]>{TorchTensor(...)},\n",
       "                %\"backbone.embeddings.patch_embeddings.projection.weight\"<FLOAT,[768,3,14,14]>{TorchTensor(...)},\n",
       "                %\"backbone.embeddings.patch_embeddings.projection.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.norm1.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.norm1.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.attention.attention.query.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.attention.attention.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.attention.attention.key.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.attention.attention.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.attention.attention.value.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.attention.attention.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.attention.output.dense.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.layer_scale1.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.norm2.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.norm2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.mlp.fc1.weight\"<FLOAT,[3072,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.mlp.fc1.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.mlp.fc2.weight\"<FLOAT,[768,3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.mlp.fc2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.0.layer_scale2.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.norm1.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.norm1.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.attention.attention.query.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.attention.attention.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.attention.attention.key.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.attention.attention.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.attention.attention.value.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.attention.attention.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.attention.output.dense.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.layer_scale1.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.norm2.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.norm2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.mlp.fc1.weight\"<FLOAT,[3072,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.mlp.fc1.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.mlp.fc2.weight\"<FLOAT,[768,3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.mlp.fc2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.1.layer_scale2.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.norm1.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.norm1.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.attention.attention.query.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.attention.attention.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.attention.attention.key.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.attention.attention.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.attention.attention.value.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.attention.attention.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.attention.output.dense.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.layer_scale1.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.norm2.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.norm2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.mlp.fc1.weight\"<FLOAT,[3072,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.mlp.fc1.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.mlp.fc2.weight\"<FLOAT,[768,3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.mlp.fc2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.2.layer_scale2.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.norm1.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.norm1.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.attention.attention.query.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.attention.attention.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.attention.attention.key.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.attention.attention.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.attention.attention.value.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.attention.attention.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.attention.output.dense.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.layer_scale1.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.norm2.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.norm2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.mlp.fc1.weight\"<FLOAT,[3072,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.mlp.fc1.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.mlp.fc2.weight\"<FLOAT,[768,3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.mlp.fc2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.3.layer_scale2.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.norm1.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.norm1.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.attention.attention.query.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.attention.attention.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.attention.attention.key.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.attention.attention.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.attention.attention.value.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.attention.attention.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.attention.output.dense.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.layer_scale1.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.norm2.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.norm2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.mlp.fc1.weight\"<FLOAT,[3072,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.mlp.fc1.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.mlp.fc2.weight\"<FLOAT,[768,3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.mlp.fc2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.4.layer_scale2.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.norm1.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.norm1.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.attention.attention.query.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.attention.attention.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.attention.attention.key.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.attention.attention.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.attention.attention.value.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.attention.attention.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.attention.output.dense.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.layer_scale1.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.norm2.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.norm2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.mlp.fc1.weight\"<FLOAT,[3072,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.mlp.fc1.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.mlp.fc2.weight\"<FLOAT,[768,3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.mlp.fc2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.5.layer_scale2.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.norm1.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.norm1.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.attention.attention.query.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.attention.attention.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.attention.attention.key.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.attention.attention.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.attention.attention.value.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.attention.attention.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.attention.output.dense.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.layer_scale1.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.norm2.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.norm2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.mlp.fc1.weight\"<FLOAT,[3072,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.mlp.fc1.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.mlp.fc2.weight\"<FLOAT,[768,3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.mlp.fc2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.6.layer_scale2.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.norm1.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.norm1.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.attention.attention.query.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.attention.attention.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.attention.attention.key.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.attention.attention.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.attention.attention.value.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.attention.attention.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.attention.output.dense.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.layer_scale1.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.norm2.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.norm2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.mlp.fc1.weight\"<FLOAT,[3072,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.mlp.fc1.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.mlp.fc2.weight\"<FLOAT,[768,3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.mlp.fc2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.7.layer_scale2.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.norm1.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.norm1.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.attention.attention.query.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.attention.attention.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.attention.attention.key.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.attention.attention.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.attention.attention.value.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.attention.attention.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.attention.output.dense.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.layer_scale1.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.norm2.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.norm2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.mlp.fc1.weight\"<FLOAT,[3072,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.mlp.fc1.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.mlp.fc2.weight\"<FLOAT,[768,3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.mlp.fc2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.8.layer_scale2.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.norm1.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.norm1.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.attention.attention.query.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.attention.attention.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.attention.attention.key.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.attention.attention.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.attention.attention.value.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.attention.attention.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.attention.output.dense.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.layer_scale1.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.norm2.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.norm2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.mlp.fc1.weight\"<FLOAT,[3072,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.mlp.fc1.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.mlp.fc2.weight\"<FLOAT,[768,3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.mlp.fc2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.9.layer_scale2.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.norm1.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.norm1.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.attention.attention.query.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.attention.attention.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.attention.attention.key.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.attention.attention.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.attention.attention.value.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.attention.attention.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.attention.output.dense.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.layer_scale1.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.norm2.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.norm2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.mlp.fc1.weight\"<FLOAT,[3072,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.mlp.fc1.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.mlp.fc2.weight\"<FLOAT,[768,3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.mlp.fc2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.10.layer_scale2.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.norm1.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.norm1.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.attention.attention.query.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.attention.attention.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.attention.attention.key.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.attention.attention.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.attention.attention.value.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.attention.attention.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.attention.output.dense.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.layer_scale1.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.norm2.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.norm2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.mlp.fc1.weight\"<FLOAT,[3072,768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.mlp.fc1.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.mlp.fc2.weight\"<FLOAT,[768,3072]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.mlp.fc2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.encoder.layer.11.layer_scale2.lambda1\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.layernorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"backbone.layernorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"classifier.0.weight\"<FLOAT,[256,1536]>{TorchTensor(...)},\n",
       "                %\"classifier.0.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"classifier.1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"classifier.1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"classifier.4.weight\"<FLOAT,[131,256]>{TorchTensor(...)},\n",
       "                %\"classifier.4.bias\"<FLOAT,[131]>{TorchTensor(...)}\n",
       "            ),\n",
       "        ) {\n",
       "              0 |  # node_Conv_1\n",
       "                   %\"conv2d\"<FLOAT,[1,768,16,16]> ⬅️ ::Conv(%\"pixel_values\", %\"backbone.embeddings.patch_embeddings.projection.weight\"{...}, %\"backbone.embeddings.patch_embeddings.projection.bias\"{...}) {group=1, auto_pad=NOTSET, dilations=[1, 1], strides=[14, 14], pads=[0, 0, 0, 0]}\n",
       "              1 |  # node_Constant_1023\n",
       "                   %\"val_1\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 768, 256]), name='val_1')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  1, 768, 256]), name='val_1')}\n",
       "              2 |  # node_Reshape_4\n",
       "                   %\"view\"<FLOAT,[1,768,256]> ⬅️ ::Reshape(%\"conv2d\", %\"val_1\"{[1, 768, 256]}) {allowzero=True}\n",
       "              3 |  # node_Transpose_5\n",
       "                   %\"transpose\"<FLOAT,[1,256,768]> ⬅️ ::Transpose(%\"view\") {perm=[0, 2, 1]}\n",
       "              4 |  # node_Constant_1026\n",
       "                   %\"val_4\"<INT64,[3]>{Tensor<INT64,[3]>(array([1, 1, 1]), name='val_4')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([1, 1, 1]), name='val_4')}\n",
       "              5 |  # node_Expand_9\n",
       "                   %\"expand\"<FLOAT,[1,1,768]> ⬅️ ::Expand(%\"backbone.embeddings.cls_token\"{...}, %\"val_4\"{[1, 1, 1]})\n",
       "              6 |  # node_Concat_10\n",
       "                   %\"cat\"<FLOAT,[1,257,768]> ⬅️ ::Concat(%\"expand\", %\"transpose\") {axis=1}\n",
       "              7 |  # node_Constant_11\n",
       "                   %\"val_5\"<INT64,[]>{Tensor<INT64,[]>(array(0), name=None)} ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(0), name=None)}\n",
       "              8 |  # node_Gather_24\n",
       "                   %\"select\"<FLOAT,[1,768]> ⬅️ ::Gather(%\"backbone.embeddings.position_embeddings\"{...}, %\"val_5\"{0}) {axis=1}\n",
       "              9 |  # node_Constant_1047\n",
       "                   %\"val_30\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_30')} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([1]), name='val_30')}\n",
       "             10 |  # node_Constant_1050\n",
       "                   %\"val_33\"<INT64,[1]>{Tensor<INT64,[1]>(array([9223372036854775807]), name='val_33')} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([9223372036854775807]), name='val_33')}\n",
       "             11 |  # node_Constant_1053\n",
       "                   %\"val_36\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_36')} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([1]), name='val_36')}\n",
       "             12 |  # node_Constant_46\n",
       "                   %\"val_37\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_37')} ⬅️ ::Constant() {value_ints=[1]}\n",
       "             13 |  # node_Slice_47\n",
       "                   %\"slice_3\"<FLOAT,[1,1369,768]> ⬅️ ::Slice(%\"backbone.embeddings.position_embeddings\"{...}, %\"val_30\"{[1]}, %\"val_33\"{[9223372036854775807]}, %\"val_36\"{[1]}, %\"val_37\"{[1]})\n",
       "             14 |  # node_Constant_1055\n",
       "                   %\"val_39\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1,  37,  37, 768]), name='val_39')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,  37,  37, 768]), name='val_39')}\n",
       "             15 |  # node_Reshape_50\n",
       "                   %\"view_1\"<FLOAT,[1,37,37,768]> ⬅️ ::Reshape(%\"slice_3\", %\"val_39\"{[1, 37, 37, 768]}) {allowzero=True}\n",
       "             16 |  # node_Transpose_51\n",
       "                   %\"permute\"<FLOAT,[1,768,37,37]> ⬅️ ::Transpose(%\"view_1\") {perm=[0, 3, 1, 2]}\n",
       "             17 |  # node_Constant_1060\n",
       "                   %\"val_43\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 768,  16,  16]), name='val_43')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 768,  16,  16]), name='val_43')}\n",
       "             18 |  # node_Resize_57\n",
       "                   %\"upsample_bicubic2d\"<FLOAT,[1,768,16,16]> ⬅️ ::Resize(%\"permute\", None, None, %\"val_43\"{[1, 768, 16, 16]}) {mode=cubic, cubic_coeff_a=-0.75, coordinate_transformation_mode=pytorch_half_pixel, exclude_outside=0, nearest_mode=floor, antialias=0, extrapolation_value=0.0, keep_aspect_ratio_policy=stretch}\n",
       "             19 |  # node_Transpose_59\n",
       "                   %\"permute_1\"<FLOAT,[1,16,16,768]> ⬅️ ::Transpose(%\"upsample_bicubic2d\") {perm=[0, 2, 3, 1]}\n",
       "             20 |  # node_Constant_1063\n",
       "                   %\"val_45\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1,  -1, 768]), name='val_45')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  1,  -1, 768]), name='val_45')}\n",
       "             21 |  # node_Reshape_62\n",
       "                   %\"view_2\"<FLOAT,[1,256,768]> ⬅️ ::Reshape(%\"permute_1\", %\"val_45\"{[1, -1, 768]}) {allowzero=True}\n",
       "             22 |  # node_Constant_63\n",
       "                   %\"val_46\"<INT64,[1]>{Tensor<INT64,[1]>(array([0]), name=None)} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([0]), name=None)}\n",
       "             23 |  # node_Unsqueeze_64\n",
       "                   %\"unsqueeze\"<FLOAT,[1,1,768]> ⬅️ ::Unsqueeze(%\"select\", %\"val_46\"{[0]})\n",
       "             24 |  # node_Concat_65\n",
       "                   %\"cat_1\"<FLOAT,[1,257,768]> ⬅️ ::Concat(%\"unsqueeze\", %\"view_2\") {axis=1}\n",
       "             25 |  # node_Add_66\n",
       "                   %\"add\"<FLOAT,[1,257,768]> ⬅️ ::Add(%\"cat\", %\"cat_1\")\n",
       "             26 |  # node_Constant_1075\n",
       "                   %\"val_59\"<INT64,[1]>{Tensor<INT64,[1]>(array([0]), name='val_59')} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([0]), name='val_59')}\n",
       "             27 |  # node_Constant_1078\n",
       "                   %\"val_62\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_62')} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([1]), name='val_62')}\n",
       "             28 |  # node_Constant_1081\n",
       "                   %\"val_65\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_65')} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([1]), name='val_65')}\n",
       "             29 |  # node_Constant_87\n",
       "                   %\"val_66\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_66')} ⬅️ ::Constant() {value_ints=[1]}\n",
       "             30 |  # node_Slice_88\n",
       "                   %\"slice_5\"<FLOAT,[1,1,768]> ⬅️ ::Slice(%\"add\", %\"val_59\"{[0]}, %\"val_62\"{[1]}, %\"val_65\"{[1]}, %\"val_66\"{[1]})\n",
       "             31 |  # node_Constant_1084\n",
       "                   %\"val_68\"<INT64,[3]>{Tensor<INT64,[3]>(array([1, 1, 1]), name='val_68')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([1, 1, 1]), name='val_68')}\n",
       "             32 |  # node_Expand_91\n",
       "                   %\"expand_1\"<FLOAT,[1,4,768]> ⬅️ ::Expand(%\"backbone.embeddings.register_tokens\"{...}, %\"val_68\"{[1, 1, 1]})\n",
       "             33 |  # node_Constant_1096\n",
       "                   %\"val_81\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_81')} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([1]), name='val_81')}\n",
       "             34 |  # node_Constant_1099\n",
       "                   %\"val_84\"<INT64,[1]>{Tensor<INT64,[1]>(array([9223372036854775807]), name='val_84')} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([9223372036854775807]), name='val_84')}\n",
       "             35 |  # node_Constant_1102\n",
       "                   %\"val_87\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_87')} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([1]), name='val_87')}\n",
       "             36 |  # node_Constant_112\n",
       "                   %\"val_88\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_88')} ⬅️ ::Constant() {value_ints=[1]}\n",
       "             37 |  # node_Slice_113\n",
       "                   %\"slice_7\"<FLOAT,[1,256,768]> ⬅️ ::Slice(%\"add\", %\"val_81\"{[1]}, %\"val_84\"{[9223372036854775807]}, %\"val_87\"{[1]}, %\"val_88\"{[1]})\n",
       "             38 |  # node_Concat_114\n",
       "                   %\"cat_2\"<FLOAT,[1,261,768]> ⬅️ ::Concat(%\"slice_5\", %\"expand_1\", %\"slice_7\") {axis=1}\n",
       "             39 |  # node_LayerNormalization_116\n",
       "                   %\"layer_norm\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"cat_2\", %\"backbone.encoder.layer.0.norm1.weight\"{...}, %\"backbone.encoder.layer.0.norm1.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "             40 |  # node_Transpose_117\n",
       "                   %\"val_91\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.0.attention.attention.key.weight\"{...}) {perm=[1, 0]}\n",
       "             41 |  # node_MatMul_118\n",
       "                   %\"val_92\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_91\")\n",
       "             42 |  # node_Add_119\n",
       "                   %\"linear\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_92\", %\"backbone.encoder.layer.0.attention.attention.key.bias\"{...})\n",
       "             43 |  # node_Constant_1104\n",
       "                   %\"val_94\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_94')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_94')}\n",
       "             44 |  # node_Reshape_122\n",
       "                   %\"view_3\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear\", %\"val_94\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "             45 |  # node_Transpose_123\n",
       "                   %\"permute_2\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_3\") {perm=[0, 2, 1, 3]}\n",
       "             46 |  # node_Transpose_124\n",
       "                   %\"val_95\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.0.attention.attention.value.weight\"{...}) {perm=[1, 0]}\n",
       "             47 |  # node_MatMul_125\n",
       "                   %\"val_96\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_95\")\n",
       "             48 |  # node_Add_126\n",
       "                   %\"linear_1\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_96\", %\"backbone.encoder.layer.0.attention.attention.value.bias\"{...})\n",
       "             49 |  # node_Constant_1106\n",
       "                   %\"val_97\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_97')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_97')}\n",
       "             50 |  # node_Reshape_128\n",
       "                   %\"view_4\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_1\", %\"val_97\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "             51 |  # node_Transpose_129\n",
       "                   %\"permute_3\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_4\") {perm=[0, 2, 1, 3]}\n",
       "             52 |  # node_Transpose_130\n",
       "                   %\"val_98\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.0.attention.attention.query.weight\"{...}) {perm=[1, 0]}\n",
       "             53 |  # node_MatMul_131\n",
       "                   %\"val_99\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_98\")\n",
       "             54 |  # node_Add_132\n",
       "                   %\"linear_2\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_99\", %\"backbone.encoder.layer.0.attention.attention.query.bias\"{...})\n",
       "             55 |  # node_Constant_1108\n",
       "                   %\"val_100\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_100')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_100')}\n",
       "             56 |  # node_Reshape_134\n",
       "                   %\"view_5\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_2\", %\"val_100\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "             57 |  # node_Transpose_135\n",
       "                   %\"permute_4\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_5\") {perm=[0, 2, 1, 3]}\n",
       "             58 |  # node_Constant_1115\n",
       "                   %\"val_112\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_112')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_112')}\n",
       "             59 |  # node_Reshape_151\n",
       "                   %\"val_113\"<FLOAT,[12,261,64]> ⬅️ ::Reshape(%\"permute_2\", %\"val_112\"{[-1, 261, 64]}) {allowzero=0}\n",
       "             60 |  # node_Transpose_152\n",
       "                   %\"val_114\"<FLOAT,[12,64,261]> ⬅️ ::Transpose(%\"val_113\") {perm=[0, 2, 1]}\n",
       "             61 |  # node_Constant_1116\n",
       "                   %\"val_115\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_115')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_115')}\n",
       "             62 |  # node_Reshape_154\n",
       "                   %\"val_116\"<FLOAT,[1,12,64,261]> ⬅️ ::Reshape(%\"val_114\", %\"val_115\"{[1, 12, 64, 261]}) {allowzero=0}\n",
       "             63 |  # node_Constant_1117\n",
       "                   %\"val_117\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_117')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_117')}\n",
       "             64 |  # node_Mul_156\n",
       "                   %\"val_118\"<FLOAT,[1,12,261,64]> ⬅️ ::Mul(%\"permute_4\", %\"val_117\"{0.3535533845424652})\n",
       "             65 |  # node_Constant_1120\n",
       "                   %\"val_120\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_120')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_120')}\n",
       "             66 |  # node_Mul_159\n",
       "                   %\"val_121\"<FLOAT,[1,12,64,261]> ⬅️ ::Mul(%\"val_116\", %\"val_120\"{0.3535533845424652})\n",
       "             67 |  # node_MatMul_160\n",
       "                   %\"val_122\"<FLOAT,[1,12,261,261]> ⬅️ ::MatMul(%\"val_118\", %\"val_121\")\n",
       "             68 |  # node_Softmax_161\n",
       "                   %\"val_123\"<FLOAT,[1,12,261,261]> ⬅️ ::Softmax(%\"val_122\") {axis=-1}\n",
       "             69 |  # node_MatMul_164\n",
       "                   %\"scaled_dot_product_attention\"<FLOAT,[1,12,261,64]> ⬅️ ::MatMul(%\"val_123\", %\"permute_3\")\n",
       "             70 |  # node_Transpose_165\n",
       "                   %\"transpose_1\"<FLOAT,[1,261,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention\") {perm=[0, 2, 1, 3]}\n",
       "             71 |  # node_Constant_1126\n",
       "                   %\"val_128\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_128')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_128')}\n",
       "             72 |  # node_Reshape_169\n",
       "                   %\"view_6\"<FLOAT,[1,261,768]> ⬅️ ::Reshape(%\"transpose_1\", %\"val_128\"{[1, 261, 768]}) {allowzero=True}\n",
       "             73 |  # node_Transpose_170\n",
       "                   %\"val_129\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.0.attention.output.dense.weight\"{...}) {perm=[1, 0]}\n",
       "             74 |  # node_MatMul_171\n",
       "                   %\"val_130\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"view_6\", %\"val_129\")\n",
       "             75 |  # node_Add_172\n",
       "                   %\"linear_3\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_130\", %\"backbone.encoder.layer.0.attention.output.dense.bias\"{...})\n",
       "             76 |  # node_Mul_174\n",
       "                   %\"mul\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_3\", %\"backbone.encoder.layer.0.layer_scale1.lambda1\"{...})\n",
       "             77 |  # node_Add_175\n",
       "                   %\"add_1\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul\", %\"cat_2\")\n",
       "             78 |  # node_LayerNormalization_176\n",
       "                   %\"layer_norm_1\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1\", %\"backbone.encoder.layer.0.norm2.weight\"{...}, %\"backbone.encoder.layer.0.norm2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "             79 |  # node_Transpose_177\n",
       "                   %\"val_133\"<FLOAT,[768,3072]> ⬅️ ::Transpose(%\"backbone.encoder.layer.0.mlp.fc1.weight\"{...}) {perm=[1, 0]}\n",
       "             80 |  # node_MatMul_178\n",
       "                   %\"val_134\"<FLOAT,[1,261,3072]> ⬅️ ::MatMul(%\"layer_norm_1\", %\"val_133\")\n",
       "             81 |  # node_Add_179\n",
       "                   %\"linear_4\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_134\", %\"backbone.encoder.layer.0.mlp.fc1.bias\"{...})\n",
       "             82 |  # node_Constant_180\n",
       "                   %\"val_135\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)}\n",
       "             83 |  # node_Div_181\n",
       "                   %\"val_136\"<FLOAT,[1,261,3072]> ⬅️ ::Div(%\"linear_4\", %\"val_135\"{1.4142135381698608})\n",
       "             84 |  # node_Erf_182\n",
       "                   %\"val_137\"<FLOAT,[1,261,3072]> ⬅️ ::Erf(%\"val_136\")\n",
       "             85 |  # node_Constant_183\n",
       "                   %\"val_138\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)}\n",
       "             86 |  # node_Add_184\n",
       "                   %\"val_139\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_137\", %\"val_138\"{1.0})\n",
       "             87 |  # node_Constant_185\n",
       "                   %\"val_140\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)}\n",
       "             88 |  # node_Mul_186\n",
       "                   %\"val_141\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"val_140\"{0.5}, %\"val_139\")\n",
       "             89 |  # node_Mul_187\n",
       "                   %\"gelu\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"linear_4\", %\"val_141\")\n",
       "             90 |  # node_Transpose_188\n",
       "                   %\"val_142\"<FLOAT,[3072,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.0.mlp.fc2.weight\"{...}) {perm=[1, 0]}\n",
       "             91 |  # node_MatMul_189\n",
       "                   %\"val_143\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"gelu\", %\"val_142\")\n",
       "             92 |  # node_Add_190\n",
       "                   %\"linear_5\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_143\", %\"backbone.encoder.layer.0.mlp.fc2.bias\"{...})\n",
       "             93 |  # node_Mul_191\n",
       "                   %\"mul_1\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_5\", %\"backbone.encoder.layer.0.layer_scale2.lambda1\"{...})\n",
       "             94 |  # node_Add_192\n",
       "                   %\"add_2\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_1\", %\"add_1\")\n",
       "             95 |  # node_LayerNormalization_193\n",
       "                   %\"layer_norm_2\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_2\", %\"backbone.encoder.layer.1.norm1.weight\"{...}, %\"backbone.encoder.layer.1.norm1.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "             96 |  # node_Transpose_194\n",
       "                   %\"val_146\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.1.attention.attention.key.weight\"{...}) {perm=[1, 0]}\n",
       "             97 |  # node_MatMul_195\n",
       "                   %\"val_147\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_146\")\n",
       "             98 |  # node_Add_196\n",
       "                   %\"linear_6\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_147\", %\"backbone.encoder.layer.1.attention.attention.key.bias\"{...})\n",
       "             99 |  # node_Constant_1128\n",
       "                   %\"val_148\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_148')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_148')}\n",
       "            100 |  # node_Reshape_198\n",
       "                   %\"view_7\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_6\", %\"val_148\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            101 |  # node_Transpose_199\n",
       "                   %\"permute_5\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_7\") {perm=[0, 2, 1, 3]}\n",
       "            102 |  # node_Transpose_200\n",
       "                   %\"val_149\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.1.attention.attention.value.weight\"{...}) {perm=[1, 0]}\n",
       "            103 |  # node_MatMul_201\n",
       "                   %\"val_150\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_149\")\n",
       "            104 |  # node_Add_202\n",
       "                   %\"linear_7\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_150\", %\"backbone.encoder.layer.1.attention.attention.value.bias\"{...})\n",
       "            105 |  # node_Constant_1130\n",
       "                   %\"val_151\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_151')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_151')}\n",
       "            106 |  # node_Reshape_204\n",
       "                   %\"view_8\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_7\", %\"val_151\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            107 |  # node_Transpose_205\n",
       "                   %\"permute_6\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_8\") {perm=[0, 2, 1, 3]}\n",
       "            108 |  # node_Transpose_206\n",
       "                   %\"val_152\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.1.attention.attention.query.weight\"{...}) {perm=[1, 0]}\n",
       "            109 |  # node_MatMul_207\n",
       "                   %\"val_153\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_152\")\n",
       "            110 |  # node_Add_208\n",
       "                   %\"linear_8\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_153\", %\"backbone.encoder.layer.1.attention.attention.query.bias\"{...})\n",
       "            111 |  # node_Constant_1132\n",
       "                   %\"val_154\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_154')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_154')}\n",
       "            112 |  # node_Reshape_210\n",
       "                   %\"view_9\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_8\", %\"val_154\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            113 |  # node_Transpose_211\n",
       "                   %\"permute_7\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_9\") {perm=[0, 2, 1, 3]}\n",
       "            114 |  # node_Constant_1139\n",
       "                   %\"val_163\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_163')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_163')}\n",
       "            115 |  # node_Reshape_224\n",
       "                   %\"val_164\"<FLOAT,[12,261,64]> ⬅️ ::Reshape(%\"permute_5\", %\"val_163\"{[-1, 261, 64]}) {allowzero=0}\n",
       "            116 |  # node_Transpose_225\n",
       "                   %\"val_165\"<FLOAT,[12,64,261]> ⬅️ ::Transpose(%\"val_164\") {perm=[0, 2, 1]}\n",
       "            117 |  # node_Constant_1140\n",
       "                   %\"val_166\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_166')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_166')}\n",
       "            118 |  # node_Reshape_227\n",
       "                   %\"val_167\"<FLOAT,[1,12,64,261]> ⬅️ ::Reshape(%\"val_165\", %\"val_166\"{[1, 12, 64, 261]}) {allowzero=0}\n",
       "            119 |  # node_Constant_1141\n",
       "                   %\"val_168\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_168')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_168')}\n",
       "            120 |  # node_Mul_229\n",
       "                   %\"val_169\"<FLOAT,[1,12,261,64]> ⬅️ ::Mul(%\"permute_7\", %\"val_168\"{0.3535533845424652})\n",
       "            121 |  # node_Constant_1144\n",
       "                   %\"val_171\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_171')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_171')}\n",
       "            122 |  # node_Mul_232\n",
       "                   %\"val_172\"<FLOAT,[1,12,64,261]> ⬅️ ::Mul(%\"val_167\", %\"val_171\"{0.3535533845424652})\n",
       "            123 |  # node_MatMul_233\n",
       "                   %\"val_173\"<FLOAT,[1,12,261,261]> ⬅️ ::MatMul(%\"val_169\", %\"val_172\")\n",
       "            124 |  # node_Softmax_234\n",
       "                   %\"val_174\"<FLOAT,[1,12,261,261]> ⬅️ ::Softmax(%\"val_173\") {axis=-1}\n",
       "            125 |  # node_MatMul_236\n",
       "                   %\"scaled_dot_product_attention_1\"<FLOAT,[1,12,261,64]> ⬅️ ::MatMul(%\"val_174\", %\"permute_6\")\n",
       "            126 |  # node_Transpose_237\n",
       "                   %\"transpose_2\"<FLOAT,[1,261,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_1\") {perm=[0, 2, 1, 3]}\n",
       "            127 |  # node_Constant_1150\n",
       "                   %\"val_177\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_177')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_177')}\n",
       "            128 |  # node_Reshape_240\n",
       "                   %\"view_10\"<FLOAT,[1,261,768]> ⬅️ ::Reshape(%\"transpose_2\", %\"val_177\"{[1, 261, 768]}) {allowzero=True}\n",
       "            129 |  # node_Transpose_241\n",
       "                   %\"val_178\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.1.attention.output.dense.weight\"{...}) {perm=[1, 0]}\n",
       "            130 |  # node_MatMul_242\n",
       "                   %\"val_179\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"view_10\", %\"val_178\")\n",
       "            131 |  # node_Add_243\n",
       "                   %\"linear_9\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_179\", %\"backbone.encoder.layer.1.attention.output.dense.bias\"{...})\n",
       "            132 |  # node_Mul_245\n",
       "                   %\"mul_2\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_9\", %\"backbone.encoder.layer.1.layer_scale1.lambda1\"{...})\n",
       "            133 |  # node_Add_246\n",
       "                   %\"add_3\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_2\", %\"add_2\")\n",
       "            134 |  # node_LayerNormalization_247\n",
       "                   %\"layer_norm_3\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_3\", %\"backbone.encoder.layer.1.norm2.weight\"{...}, %\"backbone.encoder.layer.1.norm2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            135 |  # node_Transpose_248\n",
       "                   %\"val_182\"<FLOAT,[768,3072]> ⬅️ ::Transpose(%\"backbone.encoder.layer.1.mlp.fc1.weight\"{...}) {perm=[1, 0]}\n",
       "            136 |  # node_MatMul_249\n",
       "                   %\"val_183\"<FLOAT,[1,261,3072]> ⬅️ ::MatMul(%\"layer_norm_3\", %\"val_182\")\n",
       "            137 |  # node_Add_250\n",
       "                   %\"linear_10\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_183\", %\"backbone.encoder.layer.1.mlp.fc1.bias\"{...})\n",
       "            138 |  # node_Constant_251\n",
       "                   %\"val_184\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)}\n",
       "            139 |  # node_Div_252\n",
       "                   %\"val_185\"<FLOAT,[1,261,3072]> ⬅️ ::Div(%\"linear_10\", %\"val_184\"{1.4142135381698608})\n",
       "            140 |  # node_Erf_253\n",
       "                   %\"val_186\"<FLOAT,[1,261,3072]> ⬅️ ::Erf(%\"val_185\")\n",
       "            141 |  # node_Constant_254\n",
       "                   %\"val_187\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)}\n",
       "            142 |  # node_Add_255\n",
       "                   %\"val_188\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_186\", %\"val_187\"{1.0})\n",
       "            143 |  # node_Constant_256\n",
       "                   %\"val_189\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)}\n",
       "            144 |  # node_Mul_257\n",
       "                   %\"val_190\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"val_189\"{0.5}, %\"val_188\")\n",
       "            145 |  # node_Mul_258\n",
       "                   %\"gelu_1\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"linear_10\", %\"val_190\")\n",
       "            146 |  # node_Transpose_259\n",
       "                   %\"val_191\"<FLOAT,[3072,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.1.mlp.fc2.weight\"{...}) {perm=[1, 0]}\n",
       "            147 |  # node_MatMul_260\n",
       "                   %\"val_192\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"gelu_1\", %\"val_191\")\n",
       "            148 |  # node_Add_261\n",
       "                   %\"linear_11\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_192\", %\"backbone.encoder.layer.1.mlp.fc2.bias\"{...})\n",
       "            149 |  # node_Mul_262\n",
       "                   %\"mul_3\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_11\", %\"backbone.encoder.layer.1.layer_scale2.lambda1\"{...})\n",
       "            150 |  # node_Add_263\n",
       "                   %\"add_4\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_3\", %\"add_3\")\n",
       "            151 |  # node_LayerNormalization_264\n",
       "                   %\"layer_norm_4\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_4\", %\"backbone.encoder.layer.2.norm1.weight\"{...}, %\"backbone.encoder.layer.2.norm1.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            152 |  # node_Transpose_265\n",
       "                   %\"val_195\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.2.attention.attention.key.weight\"{...}) {perm=[1, 0]}\n",
       "            153 |  # node_MatMul_266\n",
       "                   %\"val_196\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_195\")\n",
       "            154 |  # node_Add_267\n",
       "                   %\"linear_12\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_196\", %\"backbone.encoder.layer.2.attention.attention.key.bias\"{...})\n",
       "            155 |  # node_Constant_1152\n",
       "                   %\"val_197\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_197')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_197')}\n",
       "            156 |  # node_Reshape_269\n",
       "                   %\"view_11\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_12\", %\"val_197\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            157 |  # node_Transpose_270\n",
       "                   %\"permute_8\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_11\") {perm=[0, 2, 1, 3]}\n",
       "            158 |  # node_Transpose_271\n",
       "                   %\"val_198\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.2.attention.attention.value.weight\"{...}) {perm=[1, 0]}\n",
       "            159 |  # node_MatMul_272\n",
       "                   %\"val_199\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_198\")\n",
       "            160 |  # node_Add_273\n",
       "                   %\"linear_13\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_199\", %\"backbone.encoder.layer.2.attention.attention.value.bias\"{...})\n",
       "            161 |  # node_Constant_1154\n",
       "                   %\"val_200\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_200')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_200')}\n",
       "            162 |  # node_Reshape_275\n",
       "                   %\"view_12\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_13\", %\"val_200\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            163 |  # node_Transpose_276\n",
       "                   %\"permute_9\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_12\") {perm=[0, 2, 1, 3]}\n",
       "            164 |  # node_Transpose_277\n",
       "                   %\"val_201\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.2.attention.attention.query.weight\"{...}) {perm=[1, 0]}\n",
       "            165 |  # node_MatMul_278\n",
       "                   %\"val_202\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_201\")\n",
       "            166 |  # node_Add_279\n",
       "                   %\"linear_14\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_202\", %\"backbone.encoder.layer.2.attention.attention.query.bias\"{...})\n",
       "            167 |  # node_Constant_1156\n",
       "                   %\"val_203\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_203')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_203')}\n",
       "            168 |  # node_Reshape_281\n",
       "                   %\"view_13\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_14\", %\"val_203\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            169 |  # node_Transpose_282\n",
       "                   %\"permute_10\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_13\") {perm=[0, 2, 1, 3]}\n",
       "            170 |  # node_Constant_1163\n",
       "                   %\"val_212\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_212')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_212')}\n",
       "            171 |  # node_Reshape_295\n",
       "                   %\"val_213\"<FLOAT,[12,261,64]> ⬅️ ::Reshape(%\"permute_8\", %\"val_212\"{[-1, 261, 64]}) {allowzero=0}\n",
       "            172 |  # node_Transpose_296\n",
       "                   %\"val_214\"<FLOAT,[12,64,261]> ⬅️ ::Transpose(%\"val_213\") {perm=[0, 2, 1]}\n",
       "            173 |  # node_Constant_1164\n",
       "                   %\"val_215\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_215')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_215')}\n",
       "            174 |  # node_Reshape_298\n",
       "                   %\"val_216\"<FLOAT,[1,12,64,261]> ⬅️ ::Reshape(%\"val_214\", %\"val_215\"{[1, 12, 64, 261]}) {allowzero=0}\n",
       "            175 |  # node_Constant_1165\n",
       "                   %\"val_217\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_217')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_217')}\n",
       "            176 |  # node_Mul_300\n",
       "                   %\"val_218\"<FLOAT,[1,12,261,64]> ⬅️ ::Mul(%\"permute_10\", %\"val_217\"{0.3535533845424652})\n",
       "            177 |  # node_Constant_1168\n",
       "                   %\"val_220\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_220')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_220')}\n",
       "            178 |  # node_Mul_303\n",
       "                   %\"val_221\"<FLOAT,[1,12,64,261]> ⬅️ ::Mul(%\"val_216\", %\"val_220\"{0.3535533845424652})\n",
       "            179 |  # node_MatMul_304\n",
       "                   %\"val_222\"<FLOAT,[1,12,261,261]> ⬅️ ::MatMul(%\"val_218\", %\"val_221\")\n",
       "            180 |  # node_Softmax_305\n",
       "                   %\"val_223\"<FLOAT,[1,12,261,261]> ⬅️ ::Softmax(%\"val_222\") {axis=-1}\n",
       "            181 |  # node_MatMul_307\n",
       "                   %\"scaled_dot_product_attention_2\"<FLOAT,[1,12,261,64]> ⬅️ ::MatMul(%\"val_223\", %\"permute_9\")\n",
       "            182 |  # node_Transpose_308\n",
       "                   %\"transpose_3\"<FLOAT,[1,261,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_2\") {perm=[0, 2, 1, 3]}\n",
       "            183 |  # node_Constant_1174\n",
       "                   %\"val_226\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_226')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_226')}\n",
       "            184 |  # node_Reshape_311\n",
       "                   %\"view_14\"<FLOAT,[1,261,768]> ⬅️ ::Reshape(%\"transpose_3\", %\"val_226\"{[1, 261, 768]}) {allowzero=True}\n",
       "            185 |  # node_Transpose_312\n",
       "                   %\"val_227\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.2.attention.output.dense.weight\"{...}) {perm=[1, 0]}\n",
       "            186 |  # node_MatMul_313\n",
       "                   %\"val_228\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"view_14\", %\"val_227\")\n",
       "            187 |  # node_Add_314\n",
       "                   %\"linear_15\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_228\", %\"backbone.encoder.layer.2.attention.output.dense.bias\"{...})\n",
       "            188 |  # node_Mul_316\n",
       "                   %\"mul_4\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_15\", %\"backbone.encoder.layer.2.layer_scale1.lambda1\"{...})\n",
       "            189 |  # node_Add_317\n",
       "                   %\"add_5\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_4\", %\"add_4\")\n",
       "            190 |  # node_LayerNormalization_318\n",
       "                   %\"layer_norm_5\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_5\", %\"backbone.encoder.layer.2.norm2.weight\"{...}, %\"backbone.encoder.layer.2.norm2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            191 |  # node_Transpose_319\n",
       "                   %\"val_231\"<FLOAT,[768,3072]> ⬅️ ::Transpose(%\"backbone.encoder.layer.2.mlp.fc1.weight\"{...}) {perm=[1, 0]}\n",
       "            192 |  # node_MatMul_320\n",
       "                   %\"val_232\"<FLOAT,[1,261,3072]> ⬅️ ::MatMul(%\"layer_norm_5\", %\"val_231\")\n",
       "            193 |  # node_Add_321\n",
       "                   %\"linear_16\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_232\", %\"backbone.encoder.layer.2.mlp.fc1.bias\"{...})\n",
       "            194 |  # node_Constant_322\n",
       "                   %\"val_233\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)}\n",
       "            195 |  # node_Div_323\n",
       "                   %\"val_234\"<FLOAT,[1,261,3072]> ⬅️ ::Div(%\"linear_16\", %\"val_233\"{1.4142135381698608})\n",
       "            196 |  # node_Erf_324\n",
       "                   %\"val_235\"<FLOAT,[1,261,3072]> ⬅️ ::Erf(%\"val_234\")\n",
       "            197 |  # node_Constant_325\n",
       "                   %\"val_236\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)}\n",
       "            198 |  # node_Add_326\n",
       "                   %\"val_237\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_235\", %\"val_236\"{1.0})\n",
       "            199 |  # node_Constant_327\n",
       "                   %\"val_238\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)}\n",
       "            200 |  # node_Mul_328\n",
       "                   %\"val_239\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"val_238\"{0.5}, %\"val_237\")\n",
       "            201 |  # node_Mul_329\n",
       "                   %\"gelu_2\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"linear_16\", %\"val_239\")\n",
       "            202 |  # node_Transpose_330\n",
       "                   %\"val_240\"<FLOAT,[3072,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.2.mlp.fc2.weight\"{...}) {perm=[1, 0]}\n",
       "            203 |  # node_MatMul_331\n",
       "                   %\"val_241\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"gelu_2\", %\"val_240\")\n",
       "            204 |  # node_Add_332\n",
       "                   %\"linear_17\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_241\", %\"backbone.encoder.layer.2.mlp.fc2.bias\"{...})\n",
       "            205 |  # node_Mul_333\n",
       "                   %\"mul_5\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_17\", %\"backbone.encoder.layer.2.layer_scale2.lambda1\"{...})\n",
       "            206 |  # node_Add_334\n",
       "                   %\"add_6\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_5\", %\"add_5\")\n",
       "            207 |  # node_LayerNormalization_335\n",
       "                   %\"layer_norm_6\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_6\", %\"backbone.encoder.layer.3.norm1.weight\"{...}, %\"backbone.encoder.layer.3.norm1.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            208 |  # node_Transpose_336\n",
       "                   %\"val_244\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.3.attention.attention.key.weight\"{...}) {perm=[1, 0]}\n",
       "            209 |  # node_MatMul_337\n",
       "                   %\"val_245\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_244\")\n",
       "            210 |  # node_Add_338\n",
       "                   %\"linear_18\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_245\", %\"backbone.encoder.layer.3.attention.attention.key.bias\"{...})\n",
       "            211 |  # node_Constant_1176\n",
       "                   %\"val_246\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_246')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_246')}\n",
       "            212 |  # node_Reshape_340\n",
       "                   %\"view_15\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_18\", %\"val_246\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            213 |  # node_Transpose_341\n",
       "                   %\"permute_11\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_15\") {perm=[0, 2, 1, 3]}\n",
       "            214 |  # node_Transpose_342\n",
       "                   %\"val_247\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.3.attention.attention.value.weight\"{...}) {perm=[1, 0]}\n",
       "            215 |  # node_MatMul_343\n",
       "                   %\"val_248\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_247\")\n",
       "            216 |  # node_Add_344\n",
       "                   %\"linear_19\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_248\", %\"backbone.encoder.layer.3.attention.attention.value.bias\"{...})\n",
       "            217 |  # node_Constant_1178\n",
       "                   %\"val_249\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_249')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_249')}\n",
       "            218 |  # node_Reshape_346\n",
       "                   %\"view_16\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_19\", %\"val_249\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            219 |  # node_Transpose_347\n",
       "                   %\"permute_12\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_16\") {perm=[0, 2, 1, 3]}\n",
       "            220 |  # node_Transpose_348\n",
       "                   %\"val_250\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.3.attention.attention.query.weight\"{...}) {perm=[1, 0]}\n",
       "            221 |  # node_MatMul_349\n",
       "                   %\"val_251\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_250\")\n",
       "            222 |  # node_Add_350\n",
       "                   %\"linear_20\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_251\", %\"backbone.encoder.layer.3.attention.attention.query.bias\"{...})\n",
       "            223 |  # node_Constant_1180\n",
       "                   %\"val_252\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_252')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_252')}\n",
       "            224 |  # node_Reshape_352\n",
       "                   %\"view_17\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_20\", %\"val_252\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            225 |  # node_Transpose_353\n",
       "                   %\"permute_13\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_17\") {perm=[0, 2, 1, 3]}\n",
       "            226 |  # node_Constant_1187\n",
       "                   %\"val_261\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_261')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_261')}\n",
       "            227 |  # node_Reshape_366\n",
       "                   %\"val_262\"<FLOAT,[12,261,64]> ⬅️ ::Reshape(%\"permute_11\", %\"val_261\"{[-1, 261, 64]}) {allowzero=0}\n",
       "            228 |  # node_Transpose_367\n",
       "                   %\"val_263\"<FLOAT,[12,64,261]> ⬅️ ::Transpose(%\"val_262\") {perm=[0, 2, 1]}\n",
       "            229 |  # node_Constant_1188\n",
       "                   %\"val_264\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_264')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_264')}\n",
       "            230 |  # node_Reshape_369\n",
       "                   %\"val_265\"<FLOAT,[1,12,64,261]> ⬅️ ::Reshape(%\"val_263\", %\"val_264\"{[1, 12, 64, 261]}) {allowzero=0}\n",
       "            231 |  # node_Constant_1189\n",
       "                   %\"val_266\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_266')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_266')}\n",
       "            232 |  # node_Mul_371\n",
       "                   %\"val_267\"<FLOAT,[1,12,261,64]> ⬅️ ::Mul(%\"permute_13\", %\"val_266\"{0.3535533845424652})\n",
       "            233 |  # node_Constant_1192\n",
       "                   %\"val_269\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_269')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_269')}\n",
       "            234 |  # node_Mul_374\n",
       "                   %\"val_270\"<FLOAT,[1,12,64,261]> ⬅️ ::Mul(%\"val_265\", %\"val_269\"{0.3535533845424652})\n",
       "            235 |  # node_MatMul_375\n",
       "                   %\"val_271\"<FLOAT,[1,12,261,261]> ⬅️ ::MatMul(%\"val_267\", %\"val_270\")\n",
       "            236 |  # node_Softmax_376\n",
       "                   %\"val_272\"<FLOAT,[1,12,261,261]> ⬅️ ::Softmax(%\"val_271\") {axis=-1}\n",
       "            237 |  # node_MatMul_378\n",
       "                   %\"scaled_dot_product_attention_3\"<FLOAT,[1,12,261,64]> ⬅️ ::MatMul(%\"val_272\", %\"permute_12\")\n",
       "            238 |  # node_Transpose_379\n",
       "                   %\"transpose_4\"<FLOAT,[1,261,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_3\") {perm=[0, 2, 1, 3]}\n",
       "            239 |  # node_Constant_1198\n",
       "                   %\"val_275\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_275')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_275')}\n",
       "            240 |  # node_Reshape_382\n",
       "                   %\"view_18\"<FLOAT,[1,261,768]> ⬅️ ::Reshape(%\"transpose_4\", %\"val_275\"{[1, 261, 768]}) {allowzero=True}\n",
       "            241 |  # node_Transpose_383\n",
       "                   %\"val_276\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.3.attention.output.dense.weight\"{...}) {perm=[1, 0]}\n",
       "            242 |  # node_MatMul_384\n",
       "                   %\"val_277\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"view_18\", %\"val_276\")\n",
       "            243 |  # node_Add_385\n",
       "                   %\"linear_21\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_277\", %\"backbone.encoder.layer.3.attention.output.dense.bias\"{...})\n",
       "            244 |  # node_Mul_387\n",
       "                   %\"mul_6\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_21\", %\"backbone.encoder.layer.3.layer_scale1.lambda1\"{...})\n",
       "            245 |  # node_Add_388\n",
       "                   %\"add_7\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_6\", %\"add_6\")\n",
       "            246 |  # node_LayerNormalization_389\n",
       "                   %\"layer_norm_7\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_7\", %\"backbone.encoder.layer.3.norm2.weight\"{...}, %\"backbone.encoder.layer.3.norm2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            247 |  # node_Transpose_390\n",
       "                   %\"val_280\"<FLOAT,[768,3072]> ⬅️ ::Transpose(%\"backbone.encoder.layer.3.mlp.fc1.weight\"{...}) {perm=[1, 0]}\n",
       "            248 |  # node_MatMul_391\n",
       "                   %\"val_281\"<FLOAT,[1,261,3072]> ⬅️ ::MatMul(%\"layer_norm_7\", %\"val_280\")\n",
       "            249 |  # node_Add_392\n",
       "                   %\"linear_22\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_281\", %\"backbone.encoder.layer.3.mlp.fc1.bias\"{...})\n",
       "            250 |  # node_Constant_393\n",
       "                   %\"val_282\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)}\n",
       "            251 |  # node_Div_394\n",
       "                   %\"val_283\"<FLOAT,[1,261,3072]> ⬅️ ::Div(%\"linear_22\", %\"val_282\"{1.4142135381698608})\n",
       "            252 |  # node_Erf_395\n",
       "                   %\"val_284\"<FLOAT,[1,261,3072]> ⬅️ ::Erf(%\"val_283\")\n",
       "            253 |  # node_Constant_396\n",
       "                   %\"val_285\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)}\n",
       "            254 |  # node_Add_397\n",
       "                   %\"val_286\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_284\", %\"val_285\"{1.0})\n",
       "            255 |  # node_Constant_398\n",
       "                   %\"val_287\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)}\n",
       "            256 |  # node_Mul_399\n",
       "                   %\"val_288\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"val_287\"{0.5}, %\"val_286\")\n",
       "            257 |  # node_Mul_400\n",
       "                   %\"gelu_3\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"linear_22\", %\"val_288\")\n",
       "            258 |  # node_Transpose_401\n",
       "                   %\"val_289\"<FLOAT,[3072,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.3.mlp.fc2.weight\"{...}) {perm=[1, 0]}\n",
       "            259 |  # node_MatMul_402\n",
       "                   %\"val_290\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"gelu_3\", %\"val_289\")\n",
       "            260 |  # node_Add_403\n",
       "                   %\"linear_23\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_290\", %\"backbone.encoder.layer.3.mlp.fc2.bias\"{...})\n",
       "            261 |  # node_Mul_404\n",
       "                   %\"mul_7\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_23\", %\"backbone.encoder.layer.3.layer_scale2.lambda1\"{...})\n",
       "            262 |  # node_Add_405\n",
       "                   %\"add_8\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_7\", %\"add_7\")\n",
       "            263 |  # node_LayerNormalization_406\n",
       "                   %\"layer_norm_8\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_8\", %\"backbone.encoder.layer.4.norm1.weight\"{...}, %\"backbone.encoder.layer.4.norm1.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            264 |  # node_Transpose_407\n",
       "                   %\"val_293\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.4.attention.attention.key.weight\"{...}) {perm=[1, 0]}\n",
       "            265 |  # node_MatMul_408\n",
       "                   %\"val_294\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_293\")\n",
       "            266 |  # node_Add_409\n",
       "                   %\"linear_24\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_294\", %\"backbone.encoder.layer.4.attention.attention.key.bias\"{...})\n",
       "            267 |  # node_Constant_1200\n",
       "                   %\"val_295\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_295')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_295')}\n",
       "            268 |  # node_Reshape_411\n",
       "                   %\"view_19\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_24\", %\"val_295\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            269 |  # node_Transpose_412\n",
       "                   %\"permute_14\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_19\") {perm=[0, 2, 1, 3]}\n",
       "            270 |  # node_Transpose_413\n",
       "                   %\"val_296\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.4.attention.attention.value.weight\"{...}) {perm=[1, 0]}\n",
       "            271 |  # node_MatMul_414\n",
       "                   %\"val_297\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_296\")\n",
       "            272 |  # node_Add_415\n",
       "                   %\"linear_25\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_297\", %\"backbone.encoder.layer.4.attention.attention.value.bias\"{...})\n",
       "            273 |  # node_Constant_1202\n",
       "                   %\"val_298\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_298')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_298')}\n",
       "            274 |  # node_Reshape_417\n",
       "                   %\"view_20\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_25\", %\"val_298\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            275 |  # node_Transpose_418\n",
       "                   %\"permute_15\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_20\") {perm=[0, 2, 1, 3]}\n",
       "            276 |  # node_Transpose_419\n",
       "                   %\"val_299\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.4.attention.attention.query.weight\"{...}) {perm=[1, 0]}\n",
       "            277 |  # node_MatMul_420\n",
       "                   %\"val_300\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_299\")\n",
       "            278 |  # node_Add_421\n",
       "                   %\"linear_26\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_300\", %\"backbone.encoder.layer.4.attention.attention.query.bias\"{...})\n",
       "            279 |  # node_Constant_1204\n",
       "                   %\"val_301\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_301')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_301')}\n",
       "            280 |  # node_Reshape_423\n",
       "                   %\"view_21\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_26\", %\"val_301\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            281 |  # node_Transpose_424\n",
       "                   %\"permute_16\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_21\") {perm=[0, 2, 1, 3]}\n",
       "            282 |  # node_Constant_1211\n",
       "                   %\"val_310\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_310')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_310')}\n",
       "            283 |  # node_Reshape_437\n",
       "                   %\"val_311\"<FLOAT,[12,261,64]> ⬅️ ::Reshape(%\"permute_14\", %\"val_310\"{[-1, 261, 64]}) {allowzero=0}\n",
       "            284 |  # node_Transpose_438\n",
       "                   %\"val_312\"<FLOAT,[12,64,261]> ⬅️ ::Transpose(%\"val_311\") {perm=[0, 2, 1]}\n",
       "            285 |  # node_Constant_1212\n",
       "                   %\"val_313\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_313')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_313')}\n",
       "            286 |  # node_Reshape_440\n",
       "                   %\"val_314\"<FLOAT,[1,12,64,261]> ⬅️ ::Reshape(%\"val_312\", %\"val_313\"{[1, 12, 64, 261]}) {allowzero=0}\n",
       "            287 |  # node_Constant_1213\n",
       "                   %\"val_315\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_315')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_315')}\n",
       "            288 |  # node_Mul_442\n",
       "                   %\"val_316\"<FLOAT,[1,12,261,64]> ⬅️ ::Mul(%\"permute_16\", %\"val_315\"{0.3535533845424652})\n",
       "            289 |  # node_Constant_1216\n",
       "                   %\"val_318\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_318')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_318')}\n",
       "            290 |  # node_Mul_445\n",
       "                   %\"val_319\"<FLOAT,[1,12,64,261]> ⬅️ ::Mul(%\"val_314\", %\"val_318\"{0.3535533845424652})\n",
       "            291 |  # node_MatMul_446\n",
       "                   %\"val_320\"<FLOAT,[1,12,261,261]> ⬅️ ::MatMul(%\"val_316\", %\"val_319\")\n",
       "            292 |  # node_Softmax_447\n",
       "                   %\"val_321\"<FLOAT,[1,12,261,261]> ⬅️ ::Softmax(%\"val_320\") {axis=-1}\n",
       "            293 |  # node_MatMul_449\n",
       "                   %\"scaled_dot_product_attention_4\"<FLOAT,[1,12,261,64]> ⬅️ ::MatMul(%\"val_321\", %\"permute_15\")\n",
       "            294 |  # node_Transpose_450\n",
       "                   %\"transpose_5\"<FLOAT,[1,261,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_4\") {perm=[0, 2, 1, 3]}\n",
       "            295 |  # node_Constant_1222\n",
       "                   %\"val_324\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_324')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_324')}\n",
       "            296 |  # node_Reshape_453\n",
       "                   %\"view_22\"<FLOAT,[1,261,768]> ⬅️ ::Reshape(%\"transpose_5\", %\"val_324\"{[1, 261, 768]}) {allowzero=True}\n",
       "            297 |  # node_Transpose_454\n",
       "                   %\"val_325\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.4.attention.output.dense.weight\"{...}) {perm=[1, 0]}\n",
       "            298 |  # node_MatMul_455\n",
       "                   %\"val_326\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"view_22\", %\"val_325\")\n",
       "            299 |  # node_Add_456\n",
       "                   %\"linear_27\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_326\", %\"backbone.encoder.layer.4.attention.output.dense.bias\"{...})\n",
       "            300 |  # node_Mul_458\n",
       "                   %\"mul_8\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_27\", %\"backbone.encoder.layer.4.layer_scale1.lambda1\"{...})\n",
       "            301 |  # node_Add_459\n",
       "                   %\"add_9\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_8\", %\"add_8\")\n",
       "            302 |  # node_LayerNormalization_460\n",
       "                   %\"layer_norm_9\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_9\", %\"backbone.encoder.layer.4.norm2.weight\"{...}, %\"backbone.encoder.layer.4.norm2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            303 |  # node_Transpose_461\n",
       "                   %\"val_329\"<FLOAT,[768,3072]> ⬅️ ::Transpose(%\"backbone.encoder.layer.4.mlp.fc1.weight\"{...}) {perm=[1, 0]}\n",
       "            304 |  # node_MatMul_462\n",
       "                   %\"val_330\"<FLOAT,[1,261,3072]> ⬅️ ::MatMul(%\"layer_norm_9\", %\"val_329\")\n",
       "            305 |  # node_Add_463\n",
       "                   %\"linear_28\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_330\", %\"backbone.encoder.layer.4.mlp.fc1.bias\"{...})\n",
       "            306 |  # node_Constant_464\n",
       "                   %\"val_331\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)}\n",
       "            307 |  # node_Div_465\n",
       "                   %\"val_332\"<FLOAT,[1,261,3072]> ⬅️ ::Div(%\"linear_28\", %\"val_331\"{1.4142135381698608})\n",
       "            308 |  # node_Erf_466\n",
       "                   %\"val_333\"<FLOAT,[1,261,3072]> ⬅️ ::Erf(%\"val_332\")\n",
       "            309 |  # node_Constant_467\n",
       "                   %\"val_334\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)}\n",
       "            310 |  # node_Add_468\n",
       "                   %\"val_335\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_333\", %\"val_334\"{1.0})\n",
       "            311 |  # node_Constant_469\n",
       "                   %\"val_336\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)}\n",
       "            312 |  # node_Mul_470\n",
       "                   %\"val_337\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"val_336\"{0.5}, %\"val_335\")\n",
       "            313 |  # node_Mul_471\n",
       "                   %\"gelu_4\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"linear_28\", %\"val_337\")\n",
       "            314 |  # node_Transpose_472\n",
       "                   %\"val_338\"<FLOAT,[3072,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.4.mlp.fc2.weight\"{...}) {perm=[1, 0]}\n",
       "            315 |  # node_MatMul_473\n",
       "                   %\"val_339\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"gelu_4\", %\"val_338\")\n",
       "            316 |  # node_Add_474\n",
       "                   %\"linear_29\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_339\", %\"backbone.encoder.layer.4.mlp.fc2.bias\"{...})\n",
       "            317 |  # node_Mul_475\n",
       "                   %\"mul_9\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_29\", %\"backbone.encoder.layer.4.layer_scale2.lambda1\"{...})\n",
       "            318 |  # node_Add_476\n",
       "                   %\"add_10\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_9\", %\"add_9\")\n",
       "            319 |  # node_LayerNormalization_477\n",
       "                   %\"layer_norm_10\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_10\", %\"backbone.encoder.layer.5.norm1.weight\"{...}, %\"backbone.encoder.layer.5.norm1.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            320 |  # node_Transpose_478\n",
       "                   %\"val_342\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.5.attention.attention.key.weight\"{...}) {perm=[1, 0]}\n",
       "            321 |  # node_MatMul_479\n",
       "                   %\"val_343\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_342\")\n",
       "            322 |  # node_Add_480\n",
       "                   %\"linear_30\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_343\", %\"backbone.encoder.layer.5.attention.attention.key.bias\"{...})\n",
       "            323 |  # node_Constant_1224\n",
       "                   %\"val_344\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_344')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_344')}\n",
       "            324 |  # node_Reshape_482\n",
       "                   %\"view_23\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_30\", %\"val_344\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            325 |  # node_Transpose_483\n",
       "                   %\"permute_17\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_23\") {perm=[0, 2, 1, 3]}\n",
       "            326 |  # node_Transpose_484\n",
       "                   %\"val_345\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.5.attention.attention.value.weight\"{...}) {perm=[1, 0]}\n",
       "            327 |  # node_MatMul_485\n",
       "                   %\"val_346\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_345\")\n",
       "            328 |  # node_Add_486\n",
       "                   %\"linear_31\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_346\", %\"backbone.encoder.layer.5.attention.attention.value.bias\"{...})\n",
       "            329 |  # node_Constant_1226\n",
       "                   %\"val_347\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_347')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_347')}\n",
       "            330 |  # node_Reshape_488\n",
       "                   %\"view_24\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_31\", %\"val_347\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            331 |  # node_Transpose_489\n",
       "                   %\"permute_18\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_24\") {perm=[0, 2, 1, 3]}\n",
       "            332 |  # node_Transpose_490\n",
       "                   %\"val_348\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.5.attention.attention.query.weight\"{...}) {perm=[1, 0]}\n",
       "            333 |  # node_MatMul_491\n",
       "                   %\"val_349\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_348\")\n",
       "            334 |  # node_Add_492\n",
       "                   %\"linear_32\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_349\", %\"backbone.encoder.layer.5.attention.attention.query.bias\"{...})\n",
       "            335 |  # node_Constant_1228\n",
       "                   %\"val_350\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_350')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_350')}\n",
       "            336 |  # node_Reshape_494\n",
       "                   %\"view_25\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_32\", %\"val_350\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            337 |  # node_Transpose_495\n",
       "                   %\"permute_19\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_25\") {perm=[0, 2, 1, 3]}\n",
       "            338 |  # node_Constant_1235\n",
       "                   %\"val_359\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_359')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_359')}\n",
       "            339 |  # node_Reshape_508\n",
       "                   %\"val_360\"<FLOAT,[12,261,64]> ⬅️ ::Reshape(%\"permute_17\", %\"val_359\"{[-1, 261, 64]}) {allowzero=0}\n",
       "            340 |  # node_Transpose_509\n",
       "                   %\"val_361\"<FLOAT,[12,64,261]> ⬅️ ::Transpose(%\"val_360\") {perm=[0, 2, 1]}\n",
       "            341 |  # node_Constant_1236\n",
       "                   %\"val_362\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_362')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_362')}\n",
       "            342 |  # node_Reshape_511\n",
       "                   %\"val_363\"<FLOAT,[1,12,64,261]> ⬅️ ::Reshape(%\"val_361\", %\"val_362\"{[1, 12, 64, 261]}) {allowzero=0}\n",
       "            343 |  # node_Constant_1237\n",
       "                   %\"val_364\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_364')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_364')}\n",
       "            344 |  # node_Mul_513\n",
       "                   %\"val_365\"<FLOAT,[1,12,261,64]> ⬅️ ::Mul(%\"permute_19\", %\"val_364\"{0.3535533845424652})\n",
       "            345 |  # node_Constant_1240\n",
       "                   %\"val_367\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_367')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_367')}\n",
       "            346 |  # node_Mul_516\n",
       "                   %\"val_368\"<FLOAT,[1,12,64,261]> ⬅️ ::Mul(%\"val_363\", %\"val_367\"{0.3535533845424652})\n",
       "            347 |  # node_MatMul_517\n",
       "                   %\"val_369\"<FLOAT,[1,12,261,261]> ⬅️ ::MatMul(%\"val_365\", %\"val_368\")\n",
       "            348 |  # node_Softmax_518\n",
       "                   %\"val_370\"<FLOAT,[1,12,261,261]> ⬅️ ::Softmax(%\"val_369\") {axis=-1}\n",
       "            349 |  # node_MatMul_520\n",
       "                   %\"scaled_dot_product_attention_5\"<FLOAT,[1,12,261,64]> ⬅️ ::MatMul(%\"val_370\", %\"permute_18\")\n",
       "            350 |  # node_Transpose_521\n",
       "                   %\"transpose_6\"<FLOAT,[1,261,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_5\") {perm=[0, 2, 1, 3]}\n",
       "            351 |  # node_Constant_1246\n",
       "                   %\"val_373\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_373')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_373')}\n",
       "            352 |  # node_Reshape_524\n",
       "                   %\"view_26\"<FLOAT,[1,261,768]> ⬅️ ::Reshape(%\"transpose_6\", %\"val_373\"{[1, 261, 768]}) {allowzero=True}\n",
       "            353 |  # node_Transpose_525\n",
       "                   %\"val_374\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.5.attention.output.dense.weight\"{...}) {perm=[1, 0]}\n",
       "            354 |  # node_MatMul_526\n",
       "                   %\"val_375\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"view_26\", %\"val_374\")\n",
       "            355 |  # node_Add_527\n",
       "                   %\"linear_33\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_375\", %\"backbone.encoder.layer.5.attention.output.dense.bias\"{...})\n",
       "            356 |  # node_Mul_529\n",
       "                   %\"mul_10\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_33\", %\"backbone.encoder.layer.5.layer_scale1.lambda1\"{...})\n",
       "            357 |  # node_Add_530\n",
       "                   %\"add_11\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_10\", %\"add_10\")\n",
       "            358 |  # node_LayerNormalization_531\n",
       "                   %\"layer_norm_11\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_11\", %\"backbone.encoder.layer.5.norm2.weight\"{...}, %\"backbone.encoder.layer.5.norm2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            359 |  # node_Transpose_532\n",
       "                   %\"val_378\"<FLOAT,[768,3072]> ⬅️ ::Transpose(%\"backbone.encoder.layer.5.mlp.fc1.weight\"{...}) {perm=[1, 0]}\n",
       "            360 |  # node_MatMul_533\n",
       "                   %\"val_379\"<FLOAT,[1,261,3072]> ⬅️ ::MatMul(%\"layer_norm_11\", %\"val_378\")\n",
       "            361 |  # node_Add_534\n",
       "                   %\"linear_34\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_379\", %\"backbone.encoder.layer.5.mlp.fc1.bias\"{...})\n",
       "            362 |  # node_Constant_535\n",
       "                   %\"val_380\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)}\n",
       "            363 |  # node_Div_536\n",
       "                   %\"val_381\"<FLOAT,[1,261,3072]> ⬅️ ::Div(%\"linear_34\", %\"val_380\"{1.4142135381698608})\n",
       "            364 |  # node_Erf_537\n",
       "                   %\"val_382\"<FLOAT,[1,261,3072]> ⬅️ ::Erf(%\"val_381\")\n",
       "            365 |  # node_Constant_538\n",
       "                   %\"val_383\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)}\n",
       "            366 |  # node_Add_539\n",
       "                   %\"val_384\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_382\", %\"val_383\"{1.0})\n",
       "            367 |  # node_Constant_540\n",
       "                   %\"val_385\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)}\n",
       "            368 |  # node_Mul_541\n",
       "                   %\"val_386\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"val_385\"{0.5}, %\"val_384\")\n",
       "            369 |  # node_Mul_542\n",
       "                   %\"gelu_5\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"linear_34\", %\"val_386\")\n",
       "            370 |  # node_Transpose_543\n",
       "                   %\"val_387\"<FLOAT,[3072,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.5.mlp.fc2.weight\"{...}) {perm=[1, 0]}\n",
       "            371 |  # node_MatMul_544\n",
       "                   %\"val_388\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"gelu_5\", %\"val_387\")\n",
       "            372 |  # node_Add_545\n",
       "                   %\"linear_35\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_388\", %\"backbone.encoder.layer.5.mlp.fc2.bias\"{...})\n",
       "            373 |  # node_Mul_546\n",
       "                   %\"mul_11\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_35\", %\"backbone.encoder.layer.5.layer_scale2.lambda1\"{...})\n",
       "            374 |  # node_Add_547\n",
       "                   %\"add_12\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_11\", %\"add_11\")\n",
       "            375 |  # node_LayerNormalization_548\n",
       "                   %\"layer_norm_12\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_12\", %\"backbone.encoder.layer.6.norm1.weight\"{...}, %\"backbone.encoder.layer.6.norm1.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            376 |  # node_Transpose_549\n",
       "                   %\"val_391\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.6.attention.attention.key.weight\"{...}) {perm=[1, 0]}\n",
       "            377 |  # node_MatMul_550\n",
       "                   %\"val_392\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_12\", %\"val_391\")\n",
       "            378 |  # node_Add_551\n",
       "                   %\"linear_36\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_392\", %\"backbone.encoder.layer.6.attention.attention.key.bias\"{...})\n",
       "            379 |  # node_Constant_1248\n",
       "                   %\"val_393\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_393')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_393')}\n",
       "            380 |  # node_Reshape_553\n",
       "                   %\"view_27\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_36\", %\"val_393\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            381 |  # node_Transpose_554\n",
       "                   %\"permute_20\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_27\") {perm=[0, 2, 1, 3]}\n",
       "            382 |  # node_Transpose_555\n",
       "                   %\"val_394\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.6.attention.attention.value.weight\"{...}) {perm=[1, 0]}\n",
       "            383 |  # node_MatMul_556\n",
       "                   %\"val_395\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_12\", %\"val_394\")\n",
       "            384 |  # node_Add_557\n",
       "                   %\"linear_37\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_395\", %\"backbone.encoder.layer.6.attention.attention.value.bias\"{...})\n",
       "            385 |  # node_Constant_1250\n",
       "                   %\"val_396\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_396')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_396')}\n",
       "            386 |  # node_Reshape_559\n",
       "                   %\"view_28\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_37\", %\"val_396\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            387 |  # node_Transpose_560\n",
       "                   %\"permute_21\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_28\") {perm=[0, 2, 1, 3]}\n",
       "            388 |  # node_Transpose_561\n",
       "                   %\"val_397\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.6.attention.attention.query.weight\"{...}) {perm=[1, 0]}\n",
       "            389 |  # node_MatMul_562\n",
       "                   %\"val_398\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_12\", %\"val_397\")\n",
       "            390 |  # node_Add_563\n",
       "                   %\"linear_38\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_398\", %\"backbone.encoder.layer.6.attention.attention.query.bias\"{...})\n",
       "            391 |  # node_Constant_1252\n",
       "                   %\"val_399\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_399')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_399')}\n",
       "            392 |  # node_Reshape_565\n",
       "                   %\"view_29\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_38\", %\"val_399\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            393 |  # node_Transpose_566\n",
       "                   %\"permute_22\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_29\") {perm=[0, 2, 1, 3]}\n",
       "            394 |  # node_Constant_1259\n",
       "                   %\"val_408\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_408')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_408')}\n",
       "            395 |  # node_Reshape_579\n",
       "                   %\"val_409\"<FLOAT,[12,261,64]> ⬅️ ::Reshape(%\"permute_20\", %\"val_408\"{[-1, 261, 64]}) {allowzero=0}\n",
       "            396 |  # node_Transpose_580\n",
       "                   %\"val_410\"<FLOAT,[12,64,261]> ⬅️ ::Transpose(%\"val_409\") {perm=[0, 2, 1]}\n",
       "            397 |  # node_Constant_1260\n",
       "                   %\"val_411\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_411')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_411')}\n",
       "            398 |  # node_Reshape_582\n",
       "                   %\"val_412\"<FLOAT,[1,12,64,261]> ⬅️ ::Reshape(%\"val_410\", %\"val_411\"{[1, 12, 64, 261]}) {allowzero=0}\n",
       "            399 |  # node_Constant_1261\n",
       "                   %\"val_413\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_413')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_413')}\n",
       "            400 |  # node_Mul_584\n",
       "                   %\"val_414\"<FLOAT,[1,12,261,64]> ⬅️ ::Mul(%\"permute_22\", %\"val_413\"{0.3535533845424652})\n",
       "            401 |  # node_Constant_1264\n",
       "                   %\"val_416\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_416')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_416')}\n",
       "            402 |  # node_Mul_587\n",
       "                   %\"val_417\"<FLOAT,[1,12,64,261]> ⬅️ ::Mul(%\"val_412\", %\"val_416\"{0.3535533845424652})\n",
       "            403 |  # node_MatMul_588\n",
       "                   %\"val_418\"<FLOAT,[1,12,261,261]> ⬅️ ::MatMul(%\"val_414\", %\"val_417\")\n",
       "            404 |  # node_Softmax_589\n",
       "                   %\"val_419\"<FLOAT,[1,12,261,261]> ⬅️ ::Softmax(%\"val_418\") {axis=-1}\n",
       "            405 |  # node_MatMul_591\n",
       "                   %\"scaled_dot_product_attention_6\"<FLOAT,[1,12,261,64]> ⬅️ ::MatMul(%\"val_419\", %\"permute_21\")\n",
       "            406 |  # node_Transpose_592\n",
       "                   %\"transpose_7\"<FLOAT,[1,261,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_6\") {perm=[0, 2, 1, 3]}\n",
       "            407 |  # node_Constant_1270\n",
       "                   %\"val_422\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_422')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_422')}\n",
       "            408 |  # node_Reshape_595\n",
       "                   %\"view_30\"<FLOAT,[1,261,768]> ⬅️ ::Reshape(%\"transpose_7\", %\"val_422\"{[1, 261, 768]}) {allowzero=True}\n",
       "            409 |  # node_Transpose_596\n",
       "                   %\"val_423\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.6.attention.output.dense.weight\"{...}) {perm=[1, 0]}\n",
       "            410 |  # node_MatMul_597\n",
       "                   %\"val_424\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"view_30\", %\"val_423\")\n",
       "            411 |  # node_Add_598\n",
       "                   %\"linear_39\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_424\", %\"backbone.encoder.layer.6.attention.output.dense.bias\"{...})\n",
       "            412 |  # node_Mul_600\n",
       "                   %\"mul_12\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_39\", %\"backbone.encoder.layer.6.layer_scale1.lambda1\"{...})\n",
       "            413 |  # node_Add_601\n",
       "                   %\"add_13\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_12\", %\"add_12\")\n",
       "            414 |  # node_LayerNormalization_602\n",
       "                   %\"layer_norm_13\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_13\", %\"backbone.encoder.layer.6.norm2.weight\"{...}, %\"backbone.encoder.layer.6.norm2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            415 |  # node_Transpose_603\n",
       "                   %\"val_427\"<FLOAT,[768,3072]> ⬅️ ::Transpose(%\"backbone.encoder.layer.6.mlp.fc1.weight\"{...}) {perm=[1, 0]}\n",
       "            416 |  # node_MatMul_604\n",
       "                   %\"val_428\"<FLOAT,[1,261,3072]> ⬅️ ::MatMul(%\"layer_norm_13\", %\"val_427\")\n",
       "            417 |  # node_Add_605\n",
       "                   %\"linear_40\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_428\", %\"backbone.encoder.layer.6.mlp.fc1.bias\"{...})\n",
       "            418 |  # node_Constant_606\n",
       "                   %\"val_429\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)}\n",
       "            419 |  # node_Div_607\n",
       "                   %\"val_430\"<FLOAT,[1,261,3072]> ⬅️ ::Div(%\"linear_40\", %\"val_429\"{1.4142135381698608})\n",
       "            420 |  # node_Erf_608\n",
       "                   %\"val_431\"<FLOAT,[1,261,3072]> ⬅️ ::Erf(%\"val_430\")\n",
       "            421 |  # node_Constant_609\n",
       "                   %\"val_432\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)}\n",
       "            422 |  # node_Add_610\n",
       "                   %\"val_433\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_431\", %\"val_432\"{1.0})\n",
       "            423 |  # node_Constant_611\n",
       "                   %\"val_434\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)}\n",
       "            424 |  # node_Mul_612\n",
       "                   %\"val_435\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"val_434\"{0.5}, %\"val_433\")\n",
       "            425 |  # node_Mul_613\n",
       "                   %\"gelu_6\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"linear_40\", %\"val_435\")\n",
       "            426 |  # node_Transpose_614\n",
       "                   %\"val_436\"<FLOAT,[3072,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.6.mlp.fc2.weight\"{...}) {perm=[1, 0]}\n",
       "            427 |  # node_MatMul_615\n",
       "                   %\"val_437\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"gelu_6\", %\"val_436\")\n",
       "            428 |  # node_Add_616\n",
       "                   %\"linear_41\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_437\", %\"backbone.encoder.layer.6.mlp.fc2.bias\"{...})\n",
       "            429 |  # node_Mul_617\n",
       "                   %\"mul_13\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_41\", %\"backbone.encoder.layer.6.layer_scale2.lambda1\"{...})\n",
       "            430 |  # node_Add_618\n",
       "                   %\"add_14\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_13\", %\"add_13\")\n",
       "            431 |  # node_LayerNormalization_619\n",
       "                   %\"layer_norm_14\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_14\", %\"backbone.encoder.layer.7.norm1.weight\"{...}, %\"backbone.encoder.layer.7.norm1.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            432 |  # node_Transpose_620\n",
       "                   %\"val_440\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.7.attention.attention.key.weight\"{...}) {perm=[1, 0]}\n",
       "            433 |  # node_MatMul_621\n",
       "                   %\"val_441\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_14\", %\"val_440\")\n",
       "            434 |  # node_Add_622\n",
       "                   %\"linear_42\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_441\", %\"backbone.encoder.layer.7.attention.attention.key.bias\"{...})\n",
       "            435 |  # node_Constant_1272\n",
       "                   %\"val_442\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_442')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_442')}\n",
       "            436 |  # node_Reshape_624\n",
       "                   %\"view_31\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_42\", %\"val_442\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            437 |  # node_Transpose_625\n",
       "                   %\"permute_23\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_31\") {perm=[0, 2, 1, 3]}\n",
       "            438 |  # node_Transpose_626\n",
       "                   %\"val_443\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.7.attention.attention.value.weight\"{...}) {perm=[1, 0]}\n",
       "            439 |  # node_MatMul_627\n",
       "                   %\"val_444\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_14\", %\"val_443\")\n",
       "            440 |  # node_Add_628\n",
       "                   %\"linear_43\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_444\", %\"backbone.encoder.layer.7.attention.attention.value.bias\"{...})\n",
       "            441 |  # node_Constant_1274\n",
       "                   %\"val_445\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_445')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_445')}\n",
       "            442 |  # node_Reshape_630\n",
       "                   %\"view_32\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_43\", %\"val_445\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            443 |  # node_Transpose_631\n",
       "                   %\"permute_24\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_32\") {perm=[0, 2, 1, 3]}\n",
       "            444 |  # node_Transpose_632\n",
       "                   %\"val_446\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.7.attention.attention.query.weight\"{...}) {perm=[1, 0]}\n",
       "            445 |  # node_MatMul_633\n",
       "                   %\"val_447\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_14\", %\"val_446\")\n",
       "            446 |  # node_Add_634\n",
       "                   %\"linear_44\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_447\", %\"backbone.encoder.layer.7.attention.attention.query.bias\"{...})\n",
       "            447 |  # node_Constant_1276\n",
       "                   %\"val_448\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_448')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_448')}\n",
       "            448 |  # node_Reshape_636\n",
       "                   %\"view_33\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_44\", %\"val_448\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            449 |  # node_Transpose_637\n",
       "                   %\"permute_25\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_33\") {perm=[0, 2, 1, 3]}\n",
       "            450 |  # node_Constant_1283\n",
       "                   %\"val_457\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_457')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_457')}\n",
       "            451 |  # node_Reshape_650\n",
       "                   %\"val_458\"<FLOAT,[12,261,64]> ⬅️ ::Reshape(%\"permute_23\", %\"val_457\"{[-1, 261, 64]}) {allowzero=0}\n",
       "            452 |  # node_Transpose_651\n",
       "                   %\"val_459\"<FLOAT,[12,64,261]> ⬅️ ::Transpose(%\"val_458\") {perm=[0, 2, 1]}\n",
       "            453 |  # node_Constant_1284\n",
       "                   %\"val_460\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_460')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_460')}\n",
       "            454 |  # node_Reshape_653\n",
       "                   %\"val_461\"<FLOAT,[1,12,64,261]> ⬅️ ::Reshape(%\"val_459\", %\"val_460\"{[1, 12, 64, 261]}) {allowzero=0}\n",
       "            455 |  # node_Constant_1285\n",
       "                   %\"val_462\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_462')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_462')}\n",
       "            456 |  # node_Mul_655\n",
       "                   %\"val_463\"<FLOAT,[1,12,261,64]> ⬅️ ::Mul(%\"permute_25\", %\"val_462\"{0.3535533845424652})\n",
       "            457 |  # node_Constant_1288\n",
       "                   %\"val_465\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_465')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_465')}\n",
       "            458 |  # node_Mul_658\n",
       "                   %\"val_466\"<FLOAT,[1,12,64,261]> ⬅️ ::Mul(%\"val_461\", %\"val_465\"{0.3535533845424652})\n",
       "            459 |  # node_MatMul_659\n",
       "                   %\"val_467\"<FLOAT,[1,12,261,261]> ⬅️ ::MatMul(%\"val_463\", %\"val_466\")\n",
       "            460 |  # node_Softmax_660\n",
       "                   %\"val_468\"<FLOAT,[1,12,261,261]> ⬅️ ::Softmax(%\"val_467\") {axis=-1}\n",
       "            461 |  # node_MatMul_662\n",
       "                   %\"scaled_dot_product_attention_7\"<FLOAT,[1,12,261,64]> ⬅️ ::MatMul(%\"val_468\", %\"permute_24\")\n",
       "            462 |  # node_Transpose_663\n",
       "                   %\"transpose_8\"<FLOAT,[1,261,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_7\") {perm=[0, 2, 1, 3]}\n",
       "            463 |  # node_Constant_1294\n",
       "                   %\"val_471\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_471')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_471')}\n",
       "            464 |  # node_Reshape_666\n",
       "                   %\"view_34\"<FLOAT,[1,261,768]> ⬅️ ::Reshape(%\"transpose_8\", %\"val_471\"{[1, 261, 768]}) {allowzero=True}\n",
       "            465 |  # node_Transpose_667\n",
       "                   %\"val_472\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.7.attention.output.dense.weight\"{...}) {perm=[1, 0]}\n",
       "            466 |  # node_MatMul_668\n",
       "                   %\"val_473\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"view_34\", %\"val_472\")\n",
       "            467 |  # node_Add_669\n",
       "                   %\"linear_45\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_473\", %\"backbone.encoder.layer.7.attention.output.dense.bias\"{...})\n",
       "            468 |  # node_Mul_671\n",
       "                   %\"mul_14\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_45\", %\"backbone.encoder.layer.7.layer_scale1.lambda1\"{...})\n",
       "            469 |  # node_Add_672\n",
       "                   %\"add_15\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_14\", %\"add_14\")\n",
       "            470 |  # node_LayerNormalization_673\n",
       "                   %\"layer_norm_15\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_15\", %\"backbone.encoder.layer.7.norm2.weight\"{...}, %\"backbone.encoder.layer.7.norm2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            471 |  # node_Transpose_674\n",
       "                   %\"val_476\"<FLOAT,[768,3072]> ⬅️ ::Transpose(%\"backbone.encoder.layer.7.mlp.fc1.weight\"{...}) {perm=[1, 0]}\n",
       "            472 |  # node_MatMul_675\n",
       "                   %\"val_477\"<FLOAT,[1,261,3072]> ⬅️ ::MatMul(%\"layer_norm_15\", %\"val_476\")\n",
       "            473 |  # node_Add_676\n",
       "                   %\"linear_46\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_477\", %\"backbone.encoder.layer.7.mlp.fc1.bias\"{...})\n",
       "            474 |  # node_Constant_677\n",
       "                   %\"val_478\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)}\n",
       "            475 |  # node_Div_678\n",
       "                   %\"val_479\"<FLOAT,[1,261,3072]> ⬅️ ::Div(%\"linear_46\", %\"val_478\"{1.4142135381698608})\n",
       "            476 |  # node_Erf_679\n",
       "                   %\"val_480\"<FLOAT,[1,261,3072]> ⬅️ ::Erf(%\"val_479\")\n",
       "            477 |  # node_Constant_680\n",
       "                   %\"val_481\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)}\n",
       "            478 |  # node_Add_681\n",
       "                   %\"val_482\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_480\", %\"val_481\"{1.0})\n",
       "            479 |  # node_Constant_682\n",
       "                   %\"val_483\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)}\n",
       "            480 |  # node_Mul_683\n",
       "                   %\"val_484\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"val_483\"{0.5}, %\"val_482\")\n",
       "            481 |  # node_Mul_684\n",
       "                   %\"gelu_7\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"linear_46\", %\"val_484\")\n",
       "            482 |  # node_Transpose_685\n",
       "                   %\"val_485\"<FLOAT,[3072,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.7.mlp.fc2.weight\"{...}) {perm=[1, 0]}\n",
       "            483 |  # node_MatMul_686\n",
       "                   %\"val_486\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"gelu_7\", %\"val_485\")\n",
       "            484 |  # node_Add_687\n",
       "                   %\"linear_47\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_486\", %\"backbone.encoder.layer.7.mlp.fc2.bias\"{...})\n",
       "            485 |  # node_Mul_688\n",
       "                   %\"mul_15\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_47\", %\"backbone.encoder.layer.7.layer_scale2.lambda1\"{...})\n",
       "            486 |  # node_Add_689\n",
       "                   %\"add_16\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_15\", %\"add_15\")\n",
       "            487 |  # node_LayerNormalization_690\n",
       "                   %\"layer_norm_16\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_16\", %\"backbone.encoder.layer.8.norm1.weight\"{...}, %\"backbone.encoder.layer.8.norm1.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            488 |  # node_Transpose_691\n",
       "                   %\"val_489\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.8.attention.attention.key.weight\"{...}) {perm=[1, 0]}\n",
       "            489 |  # node_MatMul_692\n",
       "                   %\"val_490\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_16\", %\"val_489\")\n",
       "            490 |  # node_Add_693\n",
       "                   %\"linear_48\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_490\", %\"backbone.encoder.layer.8.attention.attention.key.bias\"{...})\n",
       "            491 |  # node_Constant_1296\n",
       "                   %\"val_491\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_491')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_491')}\n",
       "            492 |  # node_Reshape_695\n",
       "                   %\"view_35\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_48\", %\"val_491\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            493 |  # node_Transpose_696\n",
       "                   %\"permute_26\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_35\") {perm=[0, 2, 1, 3]}\n",
       "            494 |  # node_Transpose_697\n",
       "                   %\"val_492\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.8.attention.attention.value.weight\"{...}) {perm=[1, 0]}\n",
       "            495 |  # node_MatMul_698\n",
       "                   %\"val_493\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_16\", %\"val_492\")\n",
       "            496 |  # node_Add_699\n",
       "                   %\"linear_49\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_493\", %\"backbone.encoder.layer.8.attention.attention.value.bias\"{...})\n",
       "            497 |  # node_Constant_1298\n",
       "                   %\"val_494\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_494')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_494')}\n",
       "            498 |  # node_Reshape_701\n",
       "                   %\"view_36\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_49\", %\"val_494\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            499 |  # node_Transpose_702\n",
       "                   %\"permute_27\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_36\") {perm=[0, 2, 1, 3]}\n",
       "            500 |  # node_Transpose_703\n",
       "                   %\"val_495\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.8.attention.attention.query.weight\"{...}) {perm=[1, 0]}\n",
       "            501 |  # node_MatMul_704\n",
       "                   %\"val_496\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_16\", %\"val_495\")\n",
       "            502 |  # node_Add_705\n",
       "                   %\"linear_50\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_496\", %\"backbone.encoder.layer.8.attention.attention.query.bias\"{...})\n",
       "            503 |  # node_Constant_1300\n",
       "                   %\"val_497\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_497')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_497')}\n",
       "            504 |  # node_Reshape_707\n",
       "                   %\"view_37\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_50\", %\"val_497\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            505 |  # node_Transpose_708\n",
       "                   %\"permute_28\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_37\") {perm=[0, 2, 1, 3]}\n",
       "            506 |  # node_Constant_1307\n",
       "                   %\"val_506\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_506')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_506')}\n",
       "            507 |  # node_Reshape_721\n",
       "                   %\"val_507\"<FLOAT,[12,261,64]> ⬅️ ::Reshape(%\"permute_26\", %\"val_506\"{[-1, 261, 64]}) {allowzero=0}\n",
       "            508 |  # node_Transpose_722\n",
       "                   %\"val_508\"<FLOAT,[12,64,261]> ⬅️ ::Transpose(%\"val_507\") {perm=[0, 2, 1]}\n",
       "            509 |  # node_Constant_1308\n",
       "                   %\"val_509\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_509')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_509')}\n",
       "            510 |  # node_Reshape_724\n",
       "                   %\"val_510\"<FLOAT,[1,12,64,261]> ⬅️ ::Reshape(%\"val_508\", %\"val_509\"{[1, 12, 64, 261]}) {allowzero=0}\n",
       "            511 |  # node_Constant_1309\n",
       "                   %\"val_511\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_511')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_511')}\n",
       "            512 |  # node_Mul_726\n",
       "                   %\"val_512\"<FLOAT,[1,12,261,64]> ⬅️ ::Mul(%\"permute_28\", %\"val_511\"{0.3535533845424652})\n",
       "            513 |  # node_Constant_1312\n",
       "                   %\"val_514\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_514')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_514')}\n",
       "            514 |  # node_Mul_729\n",
       "                   %\"val_515\"<FLOAT,[1,12,64,261]> ⬅️ ::Mul(%\"val_510\", %\"val_514\"{0.3535533845424652})\n",
       "            515 |  # node_MatMul_730\n",
       "                   %\"val_516\"<FLOAT,[1,12,261,261]> ⬅️ ::MatMul(%\"val_512\", %\"val_515\")\n",
       "            516 |  # node_Softmax_731\n",
       "                   %\"val_517\"<FLOAT,[1,12,261,261]> ⬅️ ::Softmax(%\"val_516\") {axis=-1}\n",
       "            517 |  # node_MatMul_733\n",
       "                   %\"scaled_dot_product_attention_8\"<FLOAT,[1,12,261,64]> ⬅️ ::MatMul(%\"val_517\", %\"permute_27\")\n",
       "            518 |  # node_Transpose_734\n",
       "                   %\"transpose_9\"<FLOAT,[1,261,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_8\") {perm=[0, 2, 1, 3]}\n",
       "            519 |  # node_Constant_1318\n",
       "                   %\"val_520\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_520')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_520')}\n",
       "            520 |  # node_Reshape_737\n",
       "                   %\"view_38\"<FLOAT,[1,261,768]> ⬅️ ::Reshape(%\"transpose_9\", %\"val_520\"{[1, 261, 768]}) {allowzero=True}\n",
       "            521 |  # node_Transpose_738\n",
       "                   %\"val_521\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.8.attention.output.dense.weight\"{...}) {perm=[1, 0]}\n",
       "            522 |  # node_MatMul_739\n",
       "                   %\"val_522\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"view_38\", %\"val_521\")\n",
       "            523 |  # node_Add_740\n",
       "                   %\"linear_51\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_522\", %\"backbone.encoder.layer.8.attention.output.dense.bias\"{...})\n",
       "            524 |  # node_Mul_742\n",
       "                   %\"mul_16\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_51\", %\"backbone.encoder.layer.8.layer_scale1.lambda1\"{...})\n",
       "            525 |  # node_Add_743\n",
       "                   %\"add_17\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_16\", %\"add_16\")\n",
       "            526 |  # node_LayerNormalization_744\n",
       "                   %\"layer_norm_17\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_17\", %\"backbone.encoder.layer.8.norm2.weight\"{...}, %\"backbone.encoder.layer.8.norm2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            527 |  # node_Transpose_745\n",
       "                   %\"val_525\"<FLOAT,[768,3072]> ⬅️ ::Transpose(%\"backbone.encoder.layer.8.mlp.fc1.weight\"{...}) {perm=[1, 0]}\n",
       "            528 |  # node_MatMul_746\n",
       "                   %\"val_526\"<FLOAT,[1,261,3072]> ⬅️ ::MatMul(%\"layer_norm_17\", %\"val_525\")\n",
       "            529 |  # node_Add_747\n",
       "                   %\"linear_52\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_526\", %\"backbone.encoder.layer.8.mlp.fc1.bias\"{...})\n",
       "            530 |  # node_Constant_748\n",
       "                   %\"val_527\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)}\n",
       "            531 |  # node_Div_749\n",
       "                   %\"val_528\"<FLOAT,[1,261,3072]> ⬅️ ::Div(%\"linear_52\", %\"val_527\"{1.4142135381698608})\n",
       "            532 |  # node_Erf_750\n",
       "                   %\"val_529\"<FLOAT,[1,261,3072]> ⬅️ ::Erf(%\"val_528\")\n",
       "            533 |  # node_Constant_751\n",
       "                   %\"val_530\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)}\n",
       "            534 |  # node_Add_752\n",
       "                   %\"val_531\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_529\", %\"val_530\"{1.0})\n",
       "            535 |  # node_Constant_753\n",
       "                   %\"val_532\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)}\n",
       "            536 |  # node_Mul_754\n",
       "                   %\"val_533\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"val_532\"{0.5}, %\"val_531\")\n",
       "            537 |  # node_Mul_755\n",
       "                   %\"gelu_8\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"linear_52\", %\"val_533\")\n",
       "            538 |  # node_Transpose_756\n",
       "                   %\"val_534\"<FLOAT,[3072,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.8.mlp.fc2.weight\"{...}) {perm=[1, 0]}\n",
       "            539 |  # node_MatMul_757\n",
       "                   %\"val_535\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"gelu_8\", %\"val_534\")\n",
       "            540 |  # node_Add_758\n",
       "                   %\"linear_53\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_535\", %\"backbone.encoder.layer.8.mlp.fc2.bias\"{...})\n",
       "            541 |  # node_Mul_759\n",
       "                   %\"mul_17\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_53\", %\"backbone.encoder.layer.8.layer_scale2.lambda1\"{...})\n",
       "            542 |  # node_Add_760\n",
       "                   %\"add_18\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_17\", %\"add_17\")\n",
       "            543 |  # node_LayerNormalization_761\n",
       "                   %\"layer_norm_18\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_18\", %\"backbone.encoder.layer.9.norm1.weight\"{...}, %\"backbone.encoder.layer.9.norm1.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            544 |  # node_Transpose_762\n",
       "                   %\"val_538\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.9.attention.attention.key.weight\"{...}) {perm=[1, 0]}\n",
       "            545 |  # node_MatMul_763\n",
       "                   %\"val_539\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_18\", %\"val_538\")\n",
       "            546 |  # node_Add_764\n",
       "                   %\"linear_54\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_539\", %\"backbone.encoder.layer.9.attention.attention.key.bias\"{...})\n",
       "            547 |  # node_Constant_1320\n",
       "                   %\"val_540\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_540')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_540')}\n",
       "            548 |  # node_Reshape_766\n",
       "                   %\"view_39\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_54\", %\"val_540\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            549 |  # node_Transpose_767\n",
       "                   %\"permute_29\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_39\") {perm=[0, 2, 1, 3]}\n",
       "            550 |  # node_Transpose_768\n",
       "                   %\"val_541\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.9.attention.attention.value.weight\"{...}) {perm=[1, 0]}\n",
       "            551 |  # node_MatMul_769\n",
       "                   %\"val_542\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_18\", %\"val_541\")\n",
       "            552 |  # node_Add_770\n",
       "                   %\"linear_55\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_542\", %\"backbone.encoder.layer.9.attention.attention.value.bias\"{...})\n",
       "            553 |  # node_Constant_1322\n",
       "                   %\"val_543\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_543')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_543')}\n",
       "            554 |  # node_Reshape_772\n",
       "                   %\"view_40\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_55\", %\"val_543\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            555 |  # node_Transpose_773\n",
       "                   %\"permute_30\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_40\") {perm=[0, 2, 1, 3]}\n",
       "            556 |  # node_Transpose_774\n",
       "                   %\"val_544\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.9.attention.attention.query.weight\"{...}) {perm=[1, 0]}\n",
       "            557 |  # node_MatMul_775\n",
       "                   %\"val_545\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_18\", %\"val_544\")\n",
       "            558 |  # node_Add_776\n",
       "                   %\"linear_56\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_545\", %\"backbone.encoder.layer.9.attention.attention.query.bias\"{...})\n",
       "            559 |  # node_Constant_1324\n",
       "                   %\"val_546\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_546')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_546')}\n",
       "            560 |  # node_Reshape_778\n",
       "                   %\"view_41\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_56\", %\"val_546\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            561 |  # node_Transpose_779\n",
       "                   %\"permute_31\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_41\") {perm=[0, 2, 1, 3]}\n",
       "            562 |  # node_Constant_1331\n",
       "                   %\"val_555\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_555')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_555')}\n",
       "            563 |  # node_Reshape_792\n",
       "                   %\"val_556\"<FLOAT,[12,261,64]> ⬅️ ::Reshape(%\"permute_29\", %\"val_555\"{[-1, 261, 64]}) {allowzero=0}\n",
       "            564 |  # node_Transpose_793\n",
       "                   %\"val_557\"<FLOAT,[12,64,261]> ⬅️ ::Transpose(%\"val_556\") {perm=[0, 2, 1]}\n",
       "            565 |  # node_Constant_1332\n",
       "                   %\"val_558\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_558')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_558')}\n",
       "            566 |  # node_Reshape_795\n",
       "                   %\"val_559\"<FLOAT,[1,12,64,261]> ⬅️ ::Reshape(%\"val_557\", %\"val_558\"{[1, 12, 64, 261]}) {allowzero=0}\n",
       "            567 |  # node_Constant_1333\n",
       "                   %\"val_560\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_560')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_560')}\n",
       "            568 |  # node_Mul_797\n",
       "                   %\"val_561\"<FLOAT,[1,12,261,64]> ⬅️ ::Mul(%\"permute_31\", %\"val_560\"{0.3535533845424652})\n",
       "            569 |  # node_Constant_1336\n",
       "                   %\"val_563\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_563')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_563')}\n",
       "            570 |  # node_Mul_800\n",
       "                   %\"val_564\"<FLOAT,[1,12,64,261]> ⬅️ ::Mul(%\"val_559\", %\"val_563\"{0.3535533845424652})\n",
       "            571 |  # node_MatMul_801\n",
       "                   %\"val_565\"<FLOAT,[1,12,261,261]> ⬅️ ::MatMul(%\"val_561\", %\"val_564\")\n",
       "            572 |  # node_Softmax_802\n",
       "                   %\"val_566\"<FLOAT,[1,12,261,261]> ⬅️ ::Softmax(%\"val_565\") {axis=-1}\n",
       "            573 |  # node_MatMul_804\n",
       "                   %\"scaled_dot_product_attention_9\"<FLOAT,[1,12,261,64]> ⬅️ ::MatMul(%\"val_566\", %\"permute_30\")\n",
       "            574 |  # node_Transpose_805\n",
       "                   %\"transpose_10\"<FLOAT,[1,261,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_9\") {perm=[0, 2, 1, 3]}\n",
       "            575 |  # node_Constant_1342\n",
       "                   %\"val_569\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_569')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_569')}\n",
       "            576 |  # node_Reshape_808\n",
       "                   %\"view_42\"<FLOAT,[1,261,768]> ⬅️ ::Reshape(%\"transpose_10\", %\"val_569\"{[1, 261, 768]}) {allowzero=True}\n",
       "            577 |  # node_Transpose_809\n",
       "                   %\"val_570\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.9.attention.output.dense.weight\"{...}) {perm=[1, 0]}\n",
       "            578 |  # node_MatMul_810\n",
       "                   %\"val_571\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"view_42\", %\"val_570\")\n",
       "            579 |  # node_Add_811\n",
       "                   %\"linear_57\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_571\", %\"backbone.encoder.layer.9.attention.output.dense.bias\"{...})\n",
       "            580 |  # node_Mul_813\n",
       "                   %\"mul_18\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_57\", %\"backbone.encoder.layer.9.layer_scale1.lambda1\"{...})\n",
       "            581 |  # node_Add_814\n",
       "                   %\"add_19\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_18\", %\"add_18\")\n",
       "            582 |  # node_LayerNormalization_815\n",
       "                   %\"layer_norm_19\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_19\", %\"backbone.encoder.layer.9.norm2.weight\"{...}, %\"backbone.encoder.layer.9.norm2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            583 |  # node_Transpose_816\n",
       "                   %\"val_574\"<FLOAT,[768,3072]> ⬅️ ::Transpose(%\"backbone.encoder.layer.9.mlp.fc1.weight\"{...}) {perm=[1, 0]}\n",
       "            584 |  # node_MatMul_817\n",
       "                   %\"val_575\"<FLOAT,[1,261,3072]> ⬅️ ::MatMul(%\"layer_norm_19\", %\"val_574\")\n",
       "            585 |  # node_Add_818\n",
       "                   %\"linear_58\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_575\", %\"backbone.encoder.layer.9.mlp.fc1.bias\"{...})\n",
       "            586 |  # node_Constant_819\n",
       "                   %\"val_576\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)}\n",
       "            587 |  # node_Div_820\n",
       "                   %\"val_577\"<FLOAT,[1,261,3072]> ⬅️ ::Div(%\"linear_58\", %\"val_576\"{1.4142135381698608})\n",
       "            588 |  # node_Erf_821\n",
       "                   %\"val_578\"<FLOAT,[1,261,3072]> ⬅️ ::Erf(%\"val_577\")\n",
       "            589 |  # node_Constant_822\n",
       "                   %\"val_579\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)}\n",
       "            590 |  # node_Add_823\n",
       "                   %\"val_580\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_578\", %\"val_579\"{1.0})\n",
       "            591 |  # node_Constant_824\n",
       "                   %\"val_581\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)}\n",
       "            592 |  # node_Mul_825\n",
       "                   %\"val_582\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"val_581\"{0.5}, %\"val_580\")\n",
       "            593 |  # node_Mul_826\n",
       "                   %\"gelu_9\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"linear_58\", %\"val_582\")\n",
       "            594 |  # node_Transpose_827\n",
       "                   %\"val_583\"<FLOAT,[3072,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.9.mlp.fc2.weight\"{...}) {perm=[1, 0]}\n",
       "            595 |  # node_MatMul_828\n",
       "                   %\"val_584\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"gelu_9\", %\"val_583\")\n",
       "            596 |  # node_Add_829\n",
       "                   %\"linear_59\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_584\", %\"backbone.encoder.layer.9.mlp.fc2.bias\"{...})\n",
       "            597 |  # node_Mul_830\n",
       "                   %\"mul_19\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_59\", %\"backbone.encoder.layer.9.layer_scale2.lambda1\"{...})\n",
       "            598 |  # node_Add_831\n",
       "                   %\"add_20\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_19\", %\"add_19\")\n",
       "            599 |  # node_LayerNormalization_832\n",
       "                   %\"layer_norm_20\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_20\", %\"backbone.encoder.layer.10.norm1.weight\"{...}, %\"backbone.encoder.layer.10.norm1.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            600 |  # node_Transpose_833\n",
       "                   %\"val_587\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.10.attention.attention.key.weight\"{...}) {perm=[1, 0]}\n",
       "            601 |  # node_MatMul_834\n",
       "                   %\"val_588\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_20\", %\"val_587\")\n",
       "            602 |  # node_Add_835\n",
       "                   %\"linear_60\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_588\", %\"backbone.encoder.layer.10.attention.attention.key.bias\"{...})\n",
       "            603 |  # node_Constant_1344\n",
       "                   %\"val_589\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_589')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_589')}\n",
       "            604 |  # node_Reshape_837\n",
       "                   %\"view_43\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_60\", %\"val_589\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            605 |  # node_Transpose_838\n",
       "                   %\"permute_32\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_43\") {perm=[0, 2, 1, 3]}\n",
       "            606 |  # node_Transpose_839\n",
       "                   %\"val_590\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.10.attention.attention.value.weight\"{...}) {perm=[1, 0]}\n",
       "            607 |  # node_MatMul_840\n",
       "                   %\"val_591\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_20\", %\"val_590\")\n",
       "            608 |  # node_Add_841\n",
       "                   %\"linear_61\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_591\", %\"backbone.encoder.layer.10.attention.attention.value.bias\"{...})\n",
       "            609 |  # node_Constant_1346\n",
       "                   %\"val_592\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_592')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_592')}\n",
       "            610 |  # node_Reshape_843\n",
       "                   %\"view_44\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_61\", %\"val_592\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            611 |  # node_Transpose_844\n",
       "                   %\"permute_33\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_44\") {perm=[0, 2, 1, 3]}\n",
       "            612 |  # node_Transpose_845\n",
       "                   %\"val_593\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.10.attention.attention.query.weight\"{...}) {perm=[1, 0]}\n",
       "            613 |  # node_MatMul_846\n",
       "                   %\"val_594\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_20\", %\"val_593\")\n",
       "            614 |  # node_Add_847\n",
       "                   %\"linear_62\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_594\", %\"backbone.encoder.layer.10.attention.attention.query.bias\"{...})\n",
       "            615 |  # node_Constant_1348\n",
       "                   %\"val_595\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_595')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_595')}\n",
       "            616 |  # node_Reshape_849\n",
       "                   %\"view_45\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_62\", %\"val_595\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            617 |  # node_Transpose_850\n",
       "                   %\"permute_34\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_45\") {perm=[0, 2, 1, 3]}\n",
       "            618 |  # node_Constant_1355\n",
       "                   %\"val_604\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_604')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_604')}\n",
       "            619 |  # node_Reshape_863\n",
       "                   %\"val_605\"<FLOAT,[12,261,64]> ⬅️ ::Reshape(%\"permute_32\", %\"val_604\"{[-1, 261, 64]}) {allowzero=0}\n",
       "            620 |  # node_Transpose_864\n",
       "                   %\"val_606\"<FLOAT,[12,64,261]> ⬅️ ::Transpose(%\"val_605\") {perm=[0, 2, 1]}\n",
       "            621 |  # node_Constant_1356\n",
       "                   %\"val_607\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_607')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_607')}\n",
       "            622 |  # node_Reshape_866\n",
       "                   %\"val_608\"<FLOAT,[1,12,64,261]> ⬅️ ::Reshape(%\"val_606\", %\"val_607\"{[1, 12, 64, 261]}) {allowzero=0}\n",
       "            623 |  # node_Constant_1357\n",
       "                   %\"val_609\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_609')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_609')}\n",
       "            624 |  # node_Mul_868\n",
       "                   %\"val_610\"<FLOAT,[1,12,261,64]> ⬅️ ::Mul(%\"permute_34\", %\"val_609\"{0.3535533845424652})\n",
       "            625 |  # node_Constant_1360\n",
       "                   %\"val_612\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_612')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_612')}\n",
       "            626 |  # node_Mul_871\n",
       "                   %\"val_613\"<FLOAT,[1,12,64,261]> ⬅️ ::Mul(%\"val_608\", %\"val_612\"{0.3535533845424652})\n",
       "            627 |  # node_MatMul_872\n",
       "                   %\"val_614\"<FLOAT,[1,12,261,261]> ⬅️ ::MatMul(%\"val_610\", %\"val_613\")\n",
       "            628 |  # node_Softmax_873\n",
       "                   %\"val_615\"<FLOAT,[1,12,261,261]> ⬅️ ::Softmax(%\"val_614\") {axis=-1}\n",
       "            629 |  # node_MatMul_875\n",
       "                   %\"scaled_dot_product_attention_10\"<FLOAT,[1,12,261,64]> ⬅️ ::MatMul(%\"val_615\", %\"permute_33\")\n",
       "            630 |  # node_Transpose_876\n",
       "                   %\"transpose_11\"<FLOAT,[1,261,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_10\") {perm=[0, 2, 1, 3]}\n",
       "            631 |  # node_Constant_1366\n",
       "                   %\"val_618\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_618')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_618')}\n",
       "            632 |  # node_Reshape_879\n",
       "                   %\"view_46\"<FLOAT,[1,261,768]> ⬅️ ::Reshape(%\"transpose_11\", %\"val_618\"{[1, 261, 768]}) {allowzero=True}\n",
       "            633 |  # node_Transpose_880\n",
       "                   %\"val_619\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.10.attention.output.dense.weight\"{...}) {perm=[1, 0]}\n",
       "            634 |  # node_MatMul_881\n",
       "                   %\"val_620\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"view_46\", %\"val_619\")\n",
       "            635 |  # node_Add_882\n",
       "                   %\"linear_63\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_620\", %\"backbone.encoder.layer.10.attention.output.dense.bias\"{...})\n",
       "            636 |  # node_Mul_884\n",
       "                   %\"mul_20\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_63\", %\"backbone.encoder.layer.10.layer_scale1.lambda1\"{...})\n",
       "            637 |  # node_Add_885\n",
       "                   %\"add_21\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_20\", %\"add_20\")\n",
       "            638 |  # node_LayerNormalization_886\n",
       "                   %\"layer_norm_21\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_21\", %\"backbone.encoder.layer.10.norm2.weight\"{...}, %\"backbone.encoder.layer.10.norm2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            639 |  # node_Transpose_887\n",
       "                   %\"val_623\"<FLOAT,[768,3072]> ⬅️ ::Transpose(%\"backbone.encoder.layer.10.mlp.fc1.weight\"{...}) {perm=[1, 0]}\n",
       "            640 |  # node_MatMul_888\n",
       "                   %\"val_624\"<FLOAT,[1,261,3072]> ⬅️ ::MatMul(%\"layer_norm_21\", %\"val_623\")\n",
       "            641 |  # node_Add_889\n",
       "                   %\"linear_64\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_624\", %\"backbone.encoder.layer.10.mlp.fc1.bias\"{...})\n",
       "            642 |  # node_Constant_890\n",
       "                   %\"val_625\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)}\n",
       "            643 |  # node_Div_891\n",
       "                   %\"val_626\"<FLOAT,[1,261,3072]> ⬅️ ::Div(%\"linear_64\", %\"val_625\"{1.4142135381698608})\n",
       "            644 |  # node_Erf_892\n",
       "                   %\"val_627\"<FLOAT,[1,261,3072]> ⬅️ ::Erf(%\"val_626\")\n",
       "            645 |  # node_Constant_893\n",
       "                   %\"val_628\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)}\n",
       "            646 |  # node_Add_894\n",
       "                   %\"val_629\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_627\", %\"val_628\"{1.0})\n",
       "            647 |  # node_Constant_895\n",
       "                   %\"val_630\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)}\n",
       "            648 |  # node_Mul_896\n",
       "                   %\"val_631\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"val_630\"{0.5}, %\"val_629\")\n",
       "            649 |  # node_Mul_897\n",
       "                   %\"gelu_10\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"linear_64\", %\"val_631\")\n",
       "            650 |  # node_Transpose_898\n",
       "                   %\"val_632\"<FLOAT,[3072,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.10.mlp.fc2.weight\"{...}) {perm=[1, 0]}\n",
       "            651 |  # node_MatMul_899\n",
       "                   %\"val_633\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"gelu_10\", %\"val_632\")\n",
       "            652 |  # node_Add_900\n",
       "                   %\"linear_65\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_633\", %\"backbone.encoder.layer.10.mlp.fc2.bias\"{...})\n",
       "            653 |  # node_Mul_901\n",
       "                   %\"mul_21\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_65\", %\"backbone.encoder.layer.10.layer_scale2.lambda1\"{...})\n",
       "            654 |  # node_Add_902\n",
       "                   %\"add_22\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_21\", %\"add_21\")\n",
       "            655 |  # node_LayerNormalization_903\n",
       "                   %\"layer_norm_22\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_22\", %\"backbone.encoder.layer.11.norm1.weight\"{...}, %\"backbone.encoder.layer.11.norm1.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            656 |  # node_Transpose_904\n",
       "                   %\"val_636\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.11.attention.attention.key.weight\"{...}) {perm=[1, 0]}\n",
       "            657 |  # node_MatMul_905\n",
       "                   %\"val_637\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_22\", %\"val_636\")\n",
       "            658 |  # node_Add_906\n",
       "                   %\"linear_66\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_637\", %\"backbone.encoder.layer.11.attention.attention.key.bias\"{...})\n",
       "            659 |  # node_Constant_1368\n",
       "                   %\"val_638\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_638')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_638')}\n",
       "            660 |  # node_Reshape_908\n",
       "                   %\"view_47\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_66\", %\"val_638\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            661 |  # node_Transpose_909\n",
       "                   %\"permute_35\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_47\") {perm=[0, 2, 1, 3]}\n",
       "            662 |  # node_Transpose_910\n",
       "                   %\"val_639\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.11.attention.attention.value.weight\"{...}) {perm=[1, 0]}\n",
       "            663 |  # node_MatMul_911\n",
       "                   %\"val_640\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_22\", %\"val_639\")\n",
       "            664 |  # node_Add_912\n",
       "                   %\"linear_67\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_640\", %\"backbone.encoder.layer.11.attention.attention.value.bias\"{...})\n",
       "            665 |  # node_Constant_1370\n",
       "                   %\"val_641\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_641')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_641')}\n",
       "            666 |  # node_Reshape_914\n",
       "                   %\"view_48\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_67\", %\"val_641\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            667 |  # node_Transpose_915\n",
       "                   %\"permute_36\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_48\") {perm=[0, 2, 1, 3]}\n",
       "            668 |  # node_Transpose_916\n",
       "                   %\"val_642\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.11.attention.attention.query.weight\"{...}) {perm=[1, 0]}\n",
       "            669 |  # node_MatMul_917\n",
       "                   %\"val_643\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"layer_norm_22\", %\"val_642\")\n",
       "            670 |  # node_Add_918\n",
       "                   %\"linear_68\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_643\", %\"backbone.encoder.layer.11.attention.attention.query.bias\"{...})\n",
       "            671 |  # node_Constant_1372\n",
       "                   %\"val_644\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_644')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 261,  12,  64]), name='val_644')}\n",
       "            672 |  # node_Reshape_920\n",
       "                   %\"view_49\"<FLOAT,[1,261,12,64]> ⬅️ ::Reshape(%\"linear_68\", %\"val_644\"{[1, 261, 12, 64]}) {allowzero=True}\n",
       "            673 |  # node_Transpose_921\n",
       "                   %\"permute_37\"<FLOAT,[1,12,261,64]> ⬅️ ::Transpose(%\"view_49\") {perm=[0, 2, 1, 3]}\n",
       "            674 |  # node_Constant_1379\n",
       "                   %\"val_653\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_653')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([ -1, 261,  64]), name='val_653')}\n",
       "            675 |  # node_Reshape_934\n",
       "                   %\"val_654\"<FLOAT,[12,261,64]> ⬅️ ::Reshape(%\"permute_35\", %\"val_653\"{[-1, 261, 64]}) {allowzero=0}\n",
       "            676 |  # node_Transpose_935\n",
       "                   %\"val_655\"<FLOAT,[12,64,261]> ⬅️ ::Transpose(%\"val_654\") {perm=[0, 2, 1]}\n",
       "            677 |  # node_Constant_1380\n",
       "                   %\"val_656\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_656')} ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,  12,  64, 261]), name='val_656')}\n",
       "            678 |  # node_Reshape_937\n",
       "                   %\"val_657\"<FLOAT,[1,12,64,261]> ⬅️ ::Reshape(%\"val_655\", %\"val_656\"{[1, 12, 64, 261]}) {allowzero=0}\n",
       "            679 |  # node_Constant_1381\n",
       "                   %\"val_658\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_658')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_658')}\n",
       "            680 |  # node_Mul_939\n",
       "                   %\"val_659\"<FLOAT,[1,12,261,64]> ⬅️ ::Mul(%\"permute_37\", %\"val_658\"{0.3535533845424652})\n",
       "            681 |  # node_Constant_1384\n",
       "                   %\"val_661\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_661')} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.35355338, dtype=float32), name='val_661')}\n",
       "            682 |  # node_Mul_942\n",
       "                   %\"val_662\"<FLOAT,[1,12,64,261]> ⬅️ ::Mul(%\"val_657\", %\"val_661\"{0.3535533845424652})\n",
       "            683 |  # node_MatMul_943\n",
       "                   %\"val_663\"<FLOAT,[1,12,261,261]> ⬅️ ::MatMul(%\"val_659\", %\"val_662\")\n",
       "            684 |  # node_Softmax_944\n",
       "                   %\"val_664\"<FLOAT,[1,12,261,261]> ⬅️ ::Softmax(%\"val_663\") {axis=-1}\n",
       "            685 |  # node_MatMul_946\n",
       "                   %\"scaled_dot_product_attention_11\"<FLOAT,[1,12,261,64]> ⬅️ ::MatMul(%\"val_664\", %\"permute_36\")\n",
       "            686 |  # node_Transpose_947\n",
       "                   %\"transpose_12\"<FLOAT,[1,261,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_11\") {perm=[0, 2, 1, 3]}\n",
       "            687 |  # node_Constant_1390\n",
       "                   %\"val_667\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_667')} ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  1, 261, 768]), name='val_667')}\n",
       "            688 |  # node_Reshape_950\n",
       "                   %\"view_50\"<FLOAT,[1,261,768]> ⬅️ ::Reshape(%\"transpose_12\", %\"val_667\"{[1, 261, 768]}) {allowzero=True}\n",
       "            689 |  # node_Transpose_951\n",
       "                   %\"val_668\"<FLOAT,[768,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.11.attention.output.dense.weight\"{...}) {perm=[1, 0]}\n",
       "            690 |  # node_MatMul_952\n",
       "                   %\"val_669\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"view_50\", %\"val_668\")\n",
       "            691 |  # node_Add_953\n",
       "                   %\"linear_69\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_669\", %\"backbone.encoder.layer.11.attention.output.dense.bias\"{...})\n",
       "            692 |  # node_Mul_955\n",
       "                   %\"mul_22\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_69\", %\"backbone.encoder.layer.11.layer_scale1.lambda1\"{...})\n",
       "            693 |  # node_Add_956\n",
       "                   %\"add_23\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_22\", %\"add_22\")\n",
       "            694 |  # node_LayerNormalization_957\n",
       "                   %\"layer_norm_23\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_23\", %\"backbone.encoder.layer.11.norm2.weight\"{...}, %\"backbone.encoder.layer.11.norm2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            695 |  # node_Transpose_958\n",
       "                   %\"val_672\"<FLOAT,[768,3072]> ⬅️ ::Transpose(%\"backbone.encoder.layer.11.mlp.fc1.weight\"{...}) {perm=[1, 0]}\n",
       "            696 |  # node_MatMul_959\n",
       "                   %\"val_673\"<FLOAT,[1,261,3072]> ⬅️ ::MatMul(%\"layer_norm_23\", %\"val_672\")\n",
       "            697 |  # node_Add_960\n",
       "                   %\"linear_70\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_673\", %\"backbone.encoder.layer.11.mlp.fc1.bias\"{...})\n",
       "            698 |  # node_Constant_961\n",
       "                   %\"val_674\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name=None)}\n",
       "            699 |  # node_Div_962\n",
       "                   %\"val_675\"<FLOAT,[1,261,3072]> ⬅️ ::Div(%\"linear_70\", %\"val_674\"{1.4142135381698608})\n",
       "            700 |  # node_Erf_963\n",
       "                   %\"val_676\"<FLOAT,[1,261,3072]> ⬅️ ::Erf(%\"val_675\")\n",
       "            701 |  # node_Constant_964\n",
       "                   %\"val_677\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)}\n",
       "            702 |  # node_Add_965\n",
       "                   %\"val_678\"<FLOAT,[1,261,3072]> ⬅️ ::Add(%\"val_676\", %\"val_677\"{1.0})\n",
       "            703 |  # node_Constant_966\n",
       "                   %\"val_679\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.5, dtype=float32), name=None)}\n",
       "            704 |  # node_Mul_967\n",
       "                   %\"val_680\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"val_679\"{0.5}, %\"val_678\")\n",
       "            705 |  # node_Mul_968\n",
       "                   %\"gelu_11\"<FLOAT,[1,261,3072]> ⬅️ ::Mul(%\"linear_70\", %\"val_680\")\n",
       "            706 |  # node_Transpose_969\n",
       "                   %\"val_681\"<FLOAT,[3072,768]> ⬅️ ::Transpose(%\"backbone.encoder.layer.11.mlp.fc2.weight\"{...}) {perm=[1, 0]}\n",
       "            707 |  # node_MatMul_970\n",
       "                   %\"val_682\"<FLOAT,[1,261,768]> ⬅️ ::MatMul(%\"gelu_11\", %\"val_681\")\n",
       "            708 |  # node_Add_971\n",
       "                   %\"linear_71\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"val_682\", %\"backbone.encoder.layer.11.mlp.fc2.bias\"{...})\n",
       "            709 |  # node_Mul_972\n",
       "                   %\"mul_23\"<FLOAT,[1,261,768]> ⬅️ ::Mul(%\"linear_71\", %\"backbone.encoder.layer.11.layer_scale2.lambda1\"{...})\n",
       "            710 |  # node_Add_973\n",
       "                   %\"add_24\"<FLOAT,[1,261,768]> ⬅️ ::Add(%\"mul_23\", %\"add_23\")\n",
       "            711 |  # node_LayerNormalization_974\n",
       "                   %\"layer_norm_24\"<FLOAT,[1,261,768]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_24\", %\"backbone.layernorm.weight\"{...}, %\"backbone.layernorm.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            712 |  # node_Gather_986\n",
       "                   %\"select_2\"<FLOAT,[1,768]> ⬅️ ::Gather(%\"layer_norm_24\", %\"val_5\"{0}) {axis=1}\n",
       "            713 |  # node_Constant_1411\n",
       "                   %\"val_708\"<INT64,[1]>{Tensor<INT64,[1]>(array([5]), name='val_708')} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([5]), name='val_708')}\n",
       "            714 |  # node_Constant_1414\n",
       "                   %\"val_711\"<INT64,[1]>{Tensor<INT64,[1]>(array([9223372036854775807]), name='val_711')} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([9223372036854775807]), name='val_711')}\n",
       "            715 |  # node_Constant_1417\n",
       "                   %\"val_714\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_714')} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([1]), name='val_714')}\n",
       "            716 |  # node_Constant_1008\n",
       "                   %\"val_715\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_715')} ⬅️ ::Constant() {value_ints=[1]}\n",
       "            717 |  # node_Slice_1009\n",
       "                   %\"slice_12\"<FLOAT,[1,256,768]> ⬅️ ::Slice(%\"layer_norm_24\", %\"val_708\"{[5]}, %\"val_711\"{[9223372036854775807]}, %\"val_714\"{[1]}, %\"val_715\"{[1]})\n",
       "            718 |  # node_Constant_1418\n",
       "                   %\"val_718\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_718')} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([1]), name='val_718')}\n",
       "            719 |  # node_ReduceMean_1013\n",
       "                   %\"mean\"<FLOAT,[1,768]> ⬅️ ::ReduceMean(%\"slice_12\", %\"val_718\"{[1]}) {keepdims=False, noop_with_empty_axes=0}\n",
       "            720 |  # node_Concat_1014\n",
       "                   %\"cat_3\"<FLOAT,[1,1536]> ⬅️ ::Concat(%\"select_2\", %\"mean\") {axis=1}\n",
       "            721 |  # node_Gemm_1015\n",
       "                   %\"linear_72\"<FLOAT,[1,256]> ⬅️ ::Gemm(%\"cat_3\", %\"classifier.0.weight\"{...}, %\"classifier.0.bias\"{...}) {transA=0, transB=True, alpha=1.0, beta=1.0}\n",
       "            722 |  # node_LayerNormalization_1016\n",
       "                   %\"layer_norm_25\"<FLOAT,[1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"linear_72\", %\"classifier.1.weight\"{...}, %\"classifier.1.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            723 |  # node_Sigmoid_1017\n",
       "                   %\"val_721\"<FLOAT,[1,256]> ⬅️ ::Sigmoid(%\"layer_norm_25\")\n",
       "            724 |  # node_Mul_1018\n",
       "                   %\"silu\"<FLOAT,[1,256]> ⬅️ ::Mul(%\"layer_norm_25\", %\"val_721\")\n",
       "            725 |  # node_Gemm_1020\n",
       "                   %\"logits\"<FLOAT,[1,131]> ⬅️ ::Gemm(%\"silu\", %\"classifier.4.weight\"{...}, %\"classifier.4.bias\"{...}) {transA=0, transB=True, alpha=1.0, beta=1.0}\n",
       "            return %\"logits\"<FLOAT,[1,131]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_backbone_embeddings_cls_token: \"f32[1, 1, 768]\", p_backbone_embeddings_mask_token: \"f32[1, 768]\", p_backbone_embeddings_register_tokens: \"f32[1, 4, 768]\", p_backbone_embeddings_position_embeddings: \"f32[1, 1370, 768]\", p_backbone_embeddings_patch_embeddings_projection_weight: \"f32[768, 3, 14, 14]\", p_backbone_embeddings_patch_embeddings_projection_bias: \"f32[768]\", p_backbone_encoder_layer_0_norm1_weight: \"f32[768]\", p_backbone_encoder_layer_0_norm1_bias: \"f32[768]\", p_backbone_encoder_layer_0_attention_attention_query_weight: \"f32[768, 768]\", p_backbone_encoder_layer_0_attention_attention_query_bias: \"f32[768]\", p_backbone_encoder_layer_0_attention_attention_key_weight: \"f32[768, 768]\", p_backbone_encoder_layer_0_attention_attention_key_bias: \"f32[768]\", p_backbone_encoder_layer_0_attention_attention_value_weight: \"f32[768, 768]\", p_backbone_encoder_layer_0_attention_attention_value_bias: \"f32[768]\", p_backbone_encoder_layer_0_attention_output_dense_weight: \"f32[768, 768]\", p_backbone_encoder_layer_0_attention_output_dense_bias: \"f32[768]\", p_backbone_encoder_layer_0_layer_scale1_lambda1: \"f32[768]\", p_backbone_encoder_layer_0_norm2_weight: \"f32[768]\", p_backbone_encoder_layer_0_norm2_bias: \"f32[768]\", p_backbone_encoder_layer_0_mlp_fc1_weight: \"f32[3072, 768]\", p_backbone_encoder_layer_0_mlp_fc1_bias: \"f32[3072]\", p_backbone_encoder_layer_0_mlp_fc2_weight: \"f32[768, 3072]\", p_backbone_encoder_layer_0_mlp_fc2_bias: \"f32[768]\", p_backbone_encoder_layer_0_layer_scale2_lambda1: \"f32[768]\", p_backbone_encoder_layer_1_norm1_weight: \"f32[768]\", p_backbone_encoder_layer_1_norm1_bias: \"f32[768]\", p_backbone_encoder_layer_1_attention_attention_query_weight: \"f32[768, 768]\", p_backbone_encoder_layer_1_attention_attention_query_bias: \"f32[768]\", p_backbone_encoder_layer_1_attention_attention_key_weight: \"f32[768, 768]\", p_backbone_encoder_layer_1_attention_attention_key_bias: \"f32[768]\", p_backbone_encoder_layer_1_attention_attention_value_weight: \"f32[768, 768]\", p_backbone_encoder_layer_1_attention_attention_value_bias: \"f32[768]\", p_backbone_encoder_layer_1_attention_output_dense_weight: \"f32[768, 768]\", p_backbone_encoder_layer_1_attention_output_dense_bias: \"f32[768]\", p_backbone_encoder_layer_1_layer_scale1_lambda1: \"f32[768]\", p_backbone_encoder_layer_1_norm2_weight: \"f32[768]\", p_backbone_encoder_layer_1_norm2_bias: \"f32[768]\", p_backbone_encoder_layer_1_mlp_fc1_weight: \"f32[3072, 768]\", p_backbone_encoder_layer_1_mlp_fc1_bias: \"f32[3072]\", p_backbone_encoder_layer_1_mlp_fc2_weight: \"f32[768, 3072]\", p_backbone_encoder_layer_1_mlp_fc2_bias: \"f32[768]\", p_backbone_encoder_layer_1_layer_scale2_lambda1: \"f32[768]\", p_backbone_encoder_layer_2_norm1_weight: \"f32[768]\", p_backbone_encoder_layer_2_norm1_bias: \"f32[768]\", p_backbone_encoder_layer_2_attention_attention_query_weight: \"f32[768, 768]\", p_backbone_encoder_layer_2_attention_attention_query_bias: \"f32[768]\", p_backbone_encoder_layer_2_attention_attention_key_weight: \"f32[768, 768]\", p_backbone_encoder_layer_2_attention_attention_key_bias: \"f32[768]\", p_backbone_encoder_layer_2_attention_attention_value_weight: \"f32[768, 768]\", p_backbone_encoder_layer_2_attention_attention_value_bias: \"f32[768]\", p_backbone_encoder_layer_2_attention_output_dense_weight: \"f32[768, 768]\", p_backbone_encoder_layer_2_attention_output_dense_bias: \"f32[768]\", p_backbone_encoder_layer_2_layer_scale1_lambda1: \"f32[768]\", p_backbone_encoder_layer_2_norm2_weight: \"f32[768]\", p_backbone_encoder_layer_2_norm2_bias: \"f32[768]\", p_backbone_encoder_layer_2_mlp_fc1_weight: \"f32[3072, 768]\", p_backbone_encoder_layer_2_mlp_fc1_bias: \"f32[3072]\", p_backbone_encoder_layer_2_mlp_fc2_weight: \"f32[768, 3072]\", p_backbone_encoder_layer_2_mlp_fc2_bias: \"f32[768]\", p_backbone_encoder_layer_2_layer_scale2_lambda1: \"f32[768]\", p_backbone_encoder_layer_3_norm1_weight: \"f32[768]\", p_backbone_encoder_layer_3_norm1_bias: \"f32[768]\", p_backbone_encoder_layer_3_attention_attention_query_weight: \"f32[768, 768]\", p_backbone_encoder_layer_3_attention_attention_query_bias: \"f32[768]\", p_backbone_encoder_layer_3_attention_attention_key_weight: \"f32[768, 768]\", p_backbone_encoder_layer_3_attention_attention_key_bias: \"f32[768]\", p_backbone_encoder_layer_3_attention_attention_value_weight: \"f32[768, 768]\", p_backbone_encoder_layer_3_attention_attention_value_bias: \"f32[768]\", p_backbone_encoder_layer_3_attention_output_dense_weight: \"f32[768, 768]\", p_backbone_encoder_layer_3_attention_output_dense_bias: \"f32[768]\", p_backbone_encoder_layer_3_layer_scale1_lambda1: \"f32[768]\", p_backbone_encoder_layer_3_norm2_weight: \"f32[768]\", p_backbone_encoder_layer_3_norm2_bias: \"f32[768]\", p_backbone_encoder_layer_3_mlp_fc1_weight: \"f32[3072, 768]\", p_backbone_encoder_layer_3_mlp_fc1_bias: \"f32[3072]\", p_backbone_encoder_layer_3_mlp_fc2_weight: \"f32[768, 3072]\", p_backbone_encoder_layer_3_mlp_fc2_bias: \"f32[768]\", p_backbone_encoder_layer_3_layer_scale2_lambda1: \"f32[768]\", p_backbone_encoder_layer_4_norm1_weight: \"f32[768]\", p_backbone_encoder_layer_4_norm1_bias: \"f32[768]\", p_backbone_encoder_layer_4_attention_attention_query_weight: \"f32[768, 768]\", p_backbone_encoder_layer_4_attention_attention_query_bias: \"f32[768]\", p_backbone_encoder_layer_4_attention_attention_key_weight: \"f32[768, 768]\", p_backbone_encoder_layer_4_attention_attention_key_bias: \"f32[768]\", p_backbone_encoder_layer_4_attention_attention_value_weight: \"f32[768, 768]\", p_backbone_encoder_layer_4_attention_attention_value_bias: \"f32[768]\", p_backbone_encoder_layer_4_attention_output_dense_weight: \"f32[768, 768]\", p_backbone_encoder_layer_4_attention_output_dense_bias: \"f32[768]\", p_backbone_encoder_layer_4_layer_scale1_lambda1: \"f32[768]\", p_backbone_encoder_layer_4_norm2_weight: \"f32[768]\", p_backbone_encoder_layer_4_norm2_bias: \"f32[768]\", p_backbone_encoder_layer_4_mlp_fc1_weight: \"f32[3072, 768]\", p_backbone_encoder_layer_4_mlp_fc1_bias: \"f32[3072]\", p_backbone_encoder_layer_4_mlp_fc2_weight: \"f32[768, 3072]\", p_backbone_encoder_layer_4_mlp_fc2_bias: \"f32[768]\", p_backbone_encoder_layer_4_layer_scale2_lambda1: \"f32[768]\", p_backbone_encoder_layer_5_norm1_weight: \"f32[768]\", p_backbone_encoder_layer_5_norm1_bias: \"f32[768]\", p_backbone_encoder_layer_5_attention_attention_query_weight: \"f32[768, 768]\", p_backbone_encoder_layer_5_attention_attention_query_bias: \"f32[768]\", p_backbone_encoder_layer_5_attention_attention_key_weight: \"f32[768, 768]\", p_backbone_encoder_layer_5_attention_attention_key_bias: \"f32[768]\", p_backbone_encoder_layer_5_attention_attention_value_weight: \"f32[768, 768]\", p_backbone_encoder_layer_5_attention_attention_value_bias: \"f32[768]\", p_backbone_encoder_layer_5_attention_output_dense_weight: \"f32[768, 768]\", p_backbone_encoder_layer_5_attention_output_dense_bias: \"f32[768]\", p_backbone_encoder_layer_5_layer_scale1_lambda1: \"f32[768]\", p_backbone_encoder_layer_5_norm2_weight: \"f32[768]\", p_backbone_encoder_layer_5_norm2_bias: \"f32[768]\", p_backbone_encoder_layer_5_mlp_fc1_weight: \"f32[3072, 768]\", p_backbone_encoder_layer_5_mlp_fc1_bias: \"f32[3072]\", p_backbone_encoder_layer_5_mlp_fc2_weight: \"f32[768, 3072]\", p_backbone_encoder_layer_5_mlp_fc2_bias: \"f32[768]\", p_backbone_encoder_layer_5_layer_scale2_lambda1: \"f32[768]\", p_backbone_encoder_layer_6_norm1_weight: \"f32[768]\", p_backbone_encoder_layer_6_norm1_bias: \"f32[768]\", p_backbone_encoder_layer_6_attention_attention_query_weight: \"f32[768, 768]\", p_backbone_encoder_layer_6_attention_attention_query_bias: \"f32[768]\", p_backbone_encoder_layer_6_attention_attention_key_weight: \"f32[768, 768]\", p_backbone_encoder_layer_6_attention_attention_key_bias: \"f32[768]\", p_backbone_encoder_layer_6_attention_attention_value_weight: \"f32[768, 768]\", p_backbone_encoder_layer_6_attention_attention_value_bias: \"f32[768]\", p_backbone_encoder_layer_6_attention_output_dense_weight: \"f32[768, 768]\", p_backbone_encoder_layer_6_attention_output_dense_bias: \"f32[768]\", p_backbone_encoder_layer_6_layer_scale1_lambda1: \"f32[768]\", p_backbone_encoder_layer_6_norm2_weight: \"f32[768]\", p_backbone_encoder_layer_6_norm2_bias: \"f32[768]\", p_backbone_encoder_layer_6_mlp_fc1_weight: \"f32[3072, 768]\", p_backbone_encoder_layer_6_mlp_fc1_bias: \"f32[3072]\", p_backbone_encoder_layer_6_mlp_fc2_weight: \"f32[768, 3072]\", p_backbone_encoder_layer_6_mlp_fc2_bias: \"f32[768]\", p_backbone_encoder_layer_6_layer_scale2_lambda1: \"f32[768]\", p_backbone_encoder_layer_7_norm1_weight: \"f32[768]\", p_backbone_encoder_layer_7_norm1_bias: \"f32[768]\", p_backbone_encoder_layer_7_attention_attention_query_weight: \"f32[768, 768]\", p_backbone_encoder_layer_7_attention_attention_query_bias: \"f32[768]\", p_backbone_encoder_layer_7_attention_attention_key_weight: \"f32[768, 768]\", p_backbone_encoder_layer_7_attention_attention_key_bias: \"f32[768]\", p_backbone_encoder_layer_7_attention_attention_value_weight: \"f32[768, 768]\", p_backbone_encoder_layer_7_attention_attention_value_bias: \"f32[768]\", p_backbone_encoder_layer_7_attention_output_dense_weight: \"f32[768, 768]\", p_backbone_encoder_layer_7_attention_output_dense_bias: \"f32[768]\", p_backbone_encoder_layer_7_layer_scale1_lambda1: \"f32[768]\", p_backbone_encoder_layer_7_norm2_weight: \"f32[768]\", p_backbone_encoder_layer_7_norm2_bias: \"f32[768]\", p_backbone_encoder_layer_7_mlp_fc1_weight: \"f32[3072, 768]\", p_backbone_encoder_layer_7_mlp_fc1_bias: \"f32[3072]\", p_backbone_encoder_layer_7_mlp_fc2_weight: \"f32[768, 3072]\", p_backbone_encoder_layer_7_mlp_fc2_bias: \"f32[768]\", p_backbone_encoder_layer_7_layer_scale2_lambda1: \"f32[768]\", p_backbone_encoder_layer_8_norm1_weight: \"f32[768]\", p_backbone_encoder_layer_8_norm1_bias: \"f32[768]\", p_backbone_encoder_layer_8_attention_attention_query_weight: \"f32[768, 768]\", p_backbone_encoder_layer_8_attention_attention_query_bias: \"f32[768]\", p_backbone_encoder_layer_8_attention_attention_key_weight: \"f32[768, 768]\", p_backbone_encoder_layer_8_attention_attention_key_bias: \"f32[768]\", p_backbone_encoder_layer_8_attention_attention_value_weight: \"f32[768, 768]\", p_backbone_encoder_layer_8_attention_attention_value_bias: \"f32[768]\", p_backbone_encoder_layer_8_attention_output_dense_weight: \"f32[768, 768]\", p_backbone_encoder_layer_8_attention_output_dense_bias: \"f32[768]\", p_backbone_encoder_layer_8_layer_scale1_lambda1: \"f32[768]\", p_backbone_encoder_layer_8_norm2_weight: \"f32[768]\", p_backbone_encoder_layer_8_norm2_bias: \"f32[768]\", p_backbone_encoder_layer_8_mlp_fc1_weight: \"f32[3072, 768]\", p_backbone_encoder_layer_8_mlp_fc1_bias: \"f32[3072]\", p_backbone_encoder_layer_8_mlp_fc2_weight: \"f32[768, 3072]\", p_backbone_encoder_layer_8_mlp_fc2_bias: \"f32[768]\", p_backbone_encoder_layer_8_layer_scale2_lambda1: \"f32[768]\", p_backbone_encoder_layer_9_norm1_weight: \"f32[768]\", p_backbone_encoder_layer_9_norm1_bias: \"f32[768]\", p_backbone_encoder_layer_9_attention_attention_query_weight: \"f32[768, 768]\", p_backbone_encoder_layer_9_attention_attention_query_bias: \"f32[768]\", p_backbone_encoder_layer_9_attention_attention_key_weight: \"f32[768, 768]\", p_backbone_encoder_layer_9_attention_attention_key_bias: \"f32[768]\", p_backbone_encoder_layer_9_attention_attention_value_weight: \"f32[768, 768]\", p_backbone_encoder_layer_9_attention_attention_value_bias: \"f32[768]\", p_backbone_encoder_layer_9_attention_output_dense_weight: \"f32[768, 768]\", p_backbone_encoder_layer_9_attention_output_dense_bias: \"f32[768]\", p_backbone_encoder_layer_9_layer_scale1_lambda1: \"f32[768]\", p_backbone_encoder_layer_9_norm2_weight: \"f32[768]\", p_backbone_encoder_layer_9_norm2_bias: \"f32[768]\", p_backbone_encoder_layer_9_mlp_fc1_weight: \"f32[3072, 768]\", p_backbone_encoder_layer_9_mlp_fc1_bias: \"f32[3072]\", p_backbone_encoder_layer_9_mlp_fc2_weight: \"f32[768, 3072]\", p_backbone_encoder_layer_9_mlp_fc2_bias: \"f32[768]\", p_backbone_encoder_layer_9_layer_scale2_lambda1: \"f32[768]\", p_backbone_encoder_layer_10_norm1_weight: \"f32[768]\", p_backbone_encoder_layer_10_norm1_bias: \"f32[768]\", p_backbone_encoder_layer_10_attention_attention_query_weight: \"f32[768, 768]\", p_backbone_encoder_layer_10_attention_attention_query_bias: \"f32[768]\", p_backbone_encoder_layer_10_attention_attention_key_weight: \"f32[768, 768]\", p_backbone_encoder_layer_10_attention_attention_key_bias: \"f32[768]\", p_backbone_encoder_layer_10_attention_attention_value_weight: \"f32[768, 768]\", p_backbone_encoder_layer_10_attention_attention_value_bias: \"f32[768]\", p_backbone_encoder_layer_10_attention_output_dense_weight: \"f32[768, 768]\", p_backbone_encoder_layer_10_attention_output_dense_bias: \"f32[768]\", p_backbone_encoder_layer_10_layer_scale1_lambda1: \"f32[768]\", p_backbone_encoder_layer_10_norm2_weight: \"f32[768]\", p_backbone_encoder_layer_10_norm2_bias: \"f32[768]\", p_backbone_encoder_layer_10_mlp_fc1_weight: \"f32[3072, 768]\", p_backbone_encoder_layer_10_mlp_fc1_bias: \"f32[3072]\", p_backbone_encoder_layer_10_mlp_fc2_weight: \"f32[768, 3072]\", p_backbone_encoder_layer_10_mlp_fc2_bias: \"f32[768]\", p_backbone_encoder_layer_10_layer_scale2_lambda1: \"f32[768]\", p_backbone_encoder_layer_11_norm1_weight: \"f32[768]\", p_backbone_encoder_layer_11_norm1_bias: \"f32[768]\", p_backbone_encoder_layer_11_attention_attention_query_weight: \"f32[768, 768]\", p_backbone_encoder_layer_11_attention_attention_query_bias: \"f32[768]\", p_backbone_encoder_layer_11_attention_attention_key_weight: \"f32[768, 768]\", p_backbone_encoder_layer_11_attention_attention_key_bias: \"f32[768]\", p_backbone_encoder_layer_11_attention_attention_value_weight: \"f32[768, 768]\", p_backbone_encoder_layer_11_attention_attention_value_bias: \"f32[768]\", p_backbone_encoder_layer_11_attention_output_dense_weight: \"f32[768, 768]\", p_backbone_encoder_layer_11_attention_output_dense_bias: \"f32[768]\", p_backbone_encoder_layer_11_layer_scale1_lambda1: \"f32[768]\", p_backbone_encoder_layer_11_norm2_weight: \"f32[768]\", p_backbone_encoder_layer_11_norm2_bias: \"f32[768]\", p_backbone_encoder_layer_11_mlp_fc1_weight: \"f32[3072, 768]\", p_backbone_encoder_layer_11_mlp_fc1_bias: \"f32[3072]\", p_backbone_encoder_layer_11_mlp_fc2_weight: \"f32[768, 3072]\", p_backbone_encoder_layer_11_mlp_fc2_bias: \"f32[768]\", p_backbone_encoder_layer_11_layer_scale2_lambda1: \"f32[768]\", p_backbone_layernorm_weight: \"f32[768]\", p_backbone_layernorm_bias: \"f32[768]\", p_classifier_0_weight: \"f32[256, 1536]\", p_classifier_0_bias: \"f32[256]\", p_classifier_1_weight: \"f32[256]\", p_classifier_1_bias: \"f32[256]\", p_classifier_4_weight: \"f32[131, 256]\", p_classifier_4_bias: \"f32[131]\", pixel_values: \"f32[1, 3, 224, 224]\"):\n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:150 in forward, code: embeddings = self.patch_embeddings(pixel_values.to(dtype=target_dtype))\n",
       "                    _to_copy: \"f32[1, 3, 224, 224]\" = torch.ops.aten._to_copy.default(pixel_values, dtype = torch.float32);  pixel_values = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d: \"f32[1, 768, 16, 16]\" = torch.ops.aten.conv2d.default(_to_copy, p_backbone_embeddings_patch_embeddings_projection_weight, p_backbone_embeddings_patch_embeddings_projection_bias, [14, 14]);  _to_copy = p_backbone_embeddings_patch_embeddings_projection_weight = p_backbone_embeddings_patch_embeddings_projection_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:71 in forward, code: embeddings = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
       "                    view: \"f32[1, 768, 256]\" = torch.ops.aten.view.default(conv2d, [1, 768, 256]);  conv2d = None\n",
       "                    transpose: \"f32[1, 256, 768]\" = torch.ops.aten.transpose.int(view, 1, 2);  view = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:158 in forward, code: cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
       "                    expand: \"f32[1, 1, 768]\" = torch.ops.aten.expand.default(p_backbone_embeddings_cls_token, [1, -1, -1]);  p_backbone_embeddings_cls_token = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:159 in forward, code: embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
       "                    cat: \"f32[1, 257, 768]\" = torch.ops.aten.cat.default([expand, transpose], 1);  expand = transpose = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:162 in forward, code: embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n",
       "                    slice_1: \"f32[1, 1370, 768]\" = torch.ops.aten.slice.Tensor(p_backbone_embeddings_position_embeddings, 0, 0, 9223372036854775807)\n",
       "                    select: \"f32[1, 768]\" = torch.ops.aten.select.int(slice_1, 1, 0);  slice_1 = None\n",
       "                    slice_2: \"f32[1, 1370, 768]\" = torch.ops.aten.slice.Tensor(p_backbone_embeddings_position_embeddings, 0, 0, 9223372036854775807);  p_backbone_embeddings_position_embeddings = None\n",
       "                    slice_3: \"f32[1, 1369, 768]\" = torch.ops.aten.slice.Tensor(slice_2, 1, 1, 9223372036854775807);  slice_2 = None\n",
       "                    view_1: \"f32[1, 37, 37, 768]\" = torch.ops.aten.view.default(slice_3, [1, 37, 37, 768]);  slice_3 = None\n",
       "                    permute: \"f32[1, 768, 37, 37]\" = torch.ops.aten.permute.default(view_1, [0, 3, 1, 2]);  view_1 = None\n",
       "                    _to_copy_1: \"f32[1, 768, 37, 37]\" = torch.ops.aten._to_copy.default(permute, dtype = torch.float32);  permute = None\n",
       "                    upsample_bicubic2d: \"f32[1, 768, 16, 16]\" = torch.ops.aten.upsample_bicubic2d.vec(_to_copy_1, [16, 16], False, None);  _to_copy_1 = None\n",
       "                    _to_copy_2: \"f32[1, 768, 16, 16]\" = torch.ops.aten._to_copy.default(upsample_bicubic2d, dtype = torch.float32);  upsample_bicubic2d = None\n",
       "                    permute_1: \"f32[1, 16, 16, 768]\" = torch.ops.aten.permute.default(_to_copy_2, [0, 2, 3, 1]);  _to_copy_2 = None\n",
       "                    view_2: \"f32[1, 256, 768]\" = torch.ops.aten.view.default(permute_1, [1, -1, 768]);  permute_1 = None\n",
       "                    unsqueeze: \"f32[1, 1, 768]\" = torch.ops.aten.unsqueeze.default(select, 0);  select = None\n",
       "                    cat_1: \"f32[1, 257, 768]\" = torch.ops.aten.cat.default([unsqueeze, view_2], 1);  unsqueeze = view_2 = None\n",
       "                    add: \"f32[1, 257, 768]\" = torch.ops.aten.add.Tensor(cat, cat_1);  cat = cat_1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:166 in forward, code: (embeddings[:, :1], self.register_tokens.expand(embeddings.shape[0], -1, -1), embeddings[:, 1:]), dim=1\n",
       "                    slice_4: \"f32[1, 257, 768]\" = torch.ops.aten.slice.Tensor(add, 0, 0, 9223372036854775807)\n",
       "                    slice_5: \"f32[1, 1, 768]\" = torch.ops.aten.slice.Tensor(slice_4, 1, 0, 1);  slice_4 = None\n",
       "                    expand_1: \"f32[1, 4, 768]\" = torch.ops.aten.expand.default(p_backbone_embeddings_register_tokens, [1, -1, -1]);  p_backbone_embeddings_register_tokens = None\n",
       "                    slice_6: \"f32[1, 257, 768]\" = torch.ops.aten.slice.Tensor(add, 0, 0, 9223372036854775807);  add = None\n",
       "                    slice_7: \"f32[1, 256, 768]\" = torch.ops.aten.slice.Tensor(slice_6, 1, 1, 9223372036854775807);  slice_6 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:165 in forward, code: embeddings = torch.cat(\n",
       "                    cat_2: \"f32[1, 261, 768]\" = torch.ops.aten.cat.default([slice_5, expand_1, slice_7], 1);  slice_5 = expand_1 = slice_7 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone: \"f32[1, 261, 768]\" = torch.ops.aten.clone.default(cat_2);  cat_2 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(clone, [768], p_backbone_encoder_layer_0_norm1_weight, p_backbone_encoder_layer_0_norm1_bias, 1e-06);  p_backbone_encoder_layer_0_norm1_weight = p_backbone_encoder_layer_0_norm1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm, p_backbone_encoder_layer_0_attention_attention_key_weight, p_backbone_encoder_layer_0_attention_attention_key_bias);  p_backbone_encoder_layer_0_attention_attention_key_weight = p_backbone_encoder_layer_0_attention_attention_key_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:233 in forward, code: key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
       "                    view_3: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear, [1, 261, 12, 64]);  linear = None\n",
       "                    permute_2: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_3, [0, 2, 1, 3]);  view_3 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_1: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm, p_backbone_encoder_layer_0_attention_attention_value_weight, p_backbone_encoder_layer_0_attention_attention_value_bias);  p_backbone_encoder_layer_0_attention_attention_value_weight = p_backbone_encoder_layer_0_attention_attention_value_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:234 in forward, code: value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
       "                    view_4: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_1, [1, 261, 12, 64]);  linear_1 = None\n",
       "                    permute_3: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_4, [0, 2, 1, 3]);  view_4 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_2: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm, p_backbone_encoder_layer_0_attention_attention_query_weight, p_backbone_encoder_layer_0_attention_attention_query_bias);  layer_norm = p_backbone_encoder_layer_0_attention_attention_query_weight = p_backbone_encoder_layer_0_attention_attention_query_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:235 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
       "                    view_5: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_2, [1, 261, 12, 64]);  linear_2 = None\n",
       "                    permute_4: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:247 in forward, code: context_layer, attention_probs = attention_interface(\n",
       "                    clone_1: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
       "                    clone_2: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
       "                    clone_3: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
       "                    scaled_dot_product_attention: \"f32[1, 12, 261, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_1, clone_2, clone_3, scale = 0.125);  clone_1 = clone_2 = clone_3 = None\n",
       "                    transpose_1: \"f32[1, 261, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None\n",
       "                    clone_4: \"f32[1, 261, 12, 64]\" = torch.ops.aten.clone.default(transpose_1, memory_format = torch.contiguous_format);  transpose_1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:259 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)\n",
       "                    view_6: \"f32[1, 261, 768]\" = torch.ops.aten.view.default(clone_4, [1, 261, 768]);  clone_4 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_3: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(view_6, p_backbone_encoder_layer_0_attention_output_dense_weight, p_backbone_encoder_layer_0_attention_output_dense_bias);  view_6 = p_backbone_encoder_layer_0_attention_output_dense_weight = p_backbone_encoder_layer_0_attention_output_dense_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_5: \"f32[1, 261, 768]\" = torch.ops.aten.clone.default(linear_3);  linear_3 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(clone_5, p_backbone_encoder_layer_0_layer_scale1_lambda1);  clone_5 = p_backbone_encoder_layer_0_layer_scale1_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:440 in forward, code: hidden_states = self.drop_path(attention_output) + hidden_states\n",
       "                    add_1: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul, clone);  mul = clone = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_1: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_1, [768], p_backbone_encoder_layer_0_norm2_weight, p_backbone_encoder_layer_0_norm2_bias, 1e-06);  p_backbone_encoder_layer_0_norm2_weight = p_backbone_encoder_layer_0_norm2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_4: \"f32[1, 261, 3072]\" = torch.ops.aten.linear.default(layer_norm_1, p_backbone_encoder_layer_0_mlp_fc1_weight, p_backbone_encoder_layer_0_mlp_fc1_bias);  layer_norm_1 = p_backbone_encoder_layer_0_mlp_fc1_weight = p_backbone_encoder_layer_0_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\activations.py:69 in forward, code: return self.act(input)\n",
       "                    gelu: \"f32[1, 261, 3072]\" = torch.ops.aten.gelu.default(linear_4);  linear_4 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_5: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(gelu, p_backbone_encoder_layer_0_mlp_fc2_weight, p_backbone_encoder_layer_0_mlp_fc2_bias);  gelu = p_backbone_encoder_layer_0_mlp_fc2_weight = p_backbone_encoder_layer_0_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_1: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(linear_5, p_backbone_encoder_layer_0_layer_scale2_lambda1);  linear_5 = p_backbone_encoder_layer_0_layer_scale2_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:448 in forward, code: layer_output = self.drop_path(layer_output) + hidden_states\n",
       "                    add_2: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_1, add_1);  mul_1 = add_1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_2: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_2, [768], p_backbone_encoder_layer_1_norm1_weight, p_backbone_encoder_layer_1_norm1_bias, 1e-06);  p_backbone_encoder_layer_1_norm1_weight = p_backbone_encoder_layer_1_norm1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_6: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_2, p_backbone_encoder_layer_1_attention_attention_key_weight, p_backbone_encoder_layer_1_attention_attention_key_bias);  p_backbone_encoder_layer_1_attention_attention_key_weight = p_backbone_encoder_layer_1_attention_attention_key_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:233 in forward, code: key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
       "                    view_7: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_6, [1, 261, 12, 64]);  linear_6 = None\n",
       "                    permute_5: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_7: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_2, p_backbone_encoder_layer_1_attention_attention_value_weight, p_backbone_encoder_layer_1_attention_attention_value_bias);  p_backbone_encoder_layer_1_attention_attention_value_weight = p_backbone_encoder_layer_1_attention_attention_value_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:234 in forward, code: value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
       "                    view_8: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_7, [1, 261, 12, 64]);  linear_7 = None\n",
       "                    permute_6: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_8, [0, 2, 1, 3]);  view_8 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_8: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_2, p_backbone_encoder_layer_1_attention_attention_query_weight, p_backbone_encoder_layer_1_attention_attention_query_bias);  layer_norm_2 = p_backbone_encoder_layer_1_attention_attention_query_weight = p_backbone_encoder_layer_1_attention_attention_query_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:235 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
       "                    view_9: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_8, [1, 261, 12, 64]);  linear_8 = None\n",
       "                    permute_7: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_9, [0, 2, 1, 3]);  view_9 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:247 in forward, code: context_layer, attention_probs = attention_interface(\n",
       "                    clone_6: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
       "                    clone_7: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
       "                    clone_8: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
       "                    scaled_dot_product_attention_1: \"f32[1, 12, 261, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_6, clone_7, clone_8, scale = 0.125);  clone_6 = clone_7 = clone_8 = None\n",
       "                    transpose_2: \"f32[1, 261, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_1, 1, 2);  scaled_dot_product_attention_1 = None\n",
       "                    clone_9: \"f32[1, 261, 12, 64]\" = torch.ops.aten.clone.default(transpose_2, memory_format = torch.contiguous_format);  transpose_2 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:259 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)\n",
       "                    view_10: \"f32[1, 261, 768]\" = torch.ops.aten.view.default(clone_9, [1, 261, 768]);  clone_9 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_9: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(view_10, p_backbone_encoder_layer_1_attention_output_dense_weight, p_backbone_encoder_layer_1_attention_output_dense_bias);  view_10 = p_backbone_encoder_layer_1_attention_output_dense_weight = p_backbone_encoder_layer_1_attention_output_dense_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_10: \"f32[1, 261, 768]\" = torch.ops.aten.clone.default(linear_9);  linear_9 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_2: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(clone_10, p_backbone_encoder_layer_1_layer_scale1_lambda1);  clone_10 = p_backbone_encoder_layer_1_layer_scale1_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:440 in forward, code: hidden_states = self.drop_path(attention_output) + hidden_states\n",
       "                    add_3: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_2, add_2);  mul_2 = add_2 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_3: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_3, [768], p_backbone_encoder_layer_1_norm2_weight, p_backbone_encoder_layer_1_norm2_bias, 1e-06);  p_backbone_encoder_layer_1_norm2_weight = p_backbone_encoder_layer_1_norm2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_10: \"f32[1, 261, 3072]\" = torch.ops.aten.linear.default(layer_norm_3, p_backbone_encoder_layer_1_mlp_fc1_weight, p_backbone_encoder_layer_1_mlp_fc1_bias);  layer_norm_3 = p_backbone_encoder_layer_1_mlp_fc1_weight = p_backbone_encoder_layer_1_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\activations.py:69 in forward, code: return self.act(input)\n",
       "                    gelu_1: \"f32[1, 261, 3072]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_11: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(gelu_1, p_backbone_encoder_layer_1_mlp_fc2_weight, p_backbone_encoder_layer_1_mlp_fc2_bias);  gelu_1 = p_backbone_encoder_layer_1_mlp_fc2_weight = p_backbone_encoder_layer_1_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_3: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(linear_11, p_backbone_encoder_layer_1_layer_scale2_lambda1);  linear_11 = p_backbone_encoder_layer_1_layer_scale2_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:448 in forward, code: layer_output = self.drop_path(layer_output) + hidden_states\n",
       "                    add_4: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_3, add_3);  mul_3 = add_3 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_4: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_4, [768], p_backbone_encoder_layer_2_norm1_weight, p_backbone_encoder_layer_2_norm1_bias, 1e-06);  p_backbone_encoder_layer_2_norm1_weight = p_backbone_encoder_layer_2_norm1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_12: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_4, p_backbone_encoder_layer_2_attention_attention_key_weight, p_backbone_encoder_layer_2_attention_attention_key_bias);  p_backbone_encoder_layer_2_attention_attention_key_weight = p_backbone_encoder_layer_2_attention_attention_key_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:233 in forward, code: key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
       "                    view_11: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_12, [1, 261, 12, 64]);  linear_12 = None\n",
       "                    permute_8: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_13: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_4, p_backbone_encoder_layer_2_attention_attention_value_weight, p_backbone_encoder_layer_2_attention_attention_value_bias);  p_backbone_encoder_layer_2_attention_attention_value_weight = p_backbone_encoder_layer_2_attention_attention_value_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:234 in forward, code: value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
       "                    view_12: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_13, [1, 261, 12, 64]);  linear_13 = None\n",
       "                    permute_9: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_12, [0, 2, 1, 3]);  view_12 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_14: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_4, p_backbone_encoder_layer_2_attention_attention_query_weight, p_backbone_encoder_layer_2_attention_attention_query_bias);  layer_norm_4 = p_backbone_encoder_layer_2_attention_attention_query_weight = p_backbone_encoder_layer_2_attention_attention_query_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:235 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
       "                    view_13: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_14, [1, 261, 12, 64]);  linear_14 = None\n",
       "                    permute_10: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_13, [0, 2, 1, 3]);  view_13 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:247 in forward, code: context_layer, attention_probs = attention_interface(\n",
       "                    clone_11: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_10, memory_format = torch.contiguous_format);  permute_10 = None\n",
       "                    clone_12: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
       "                    clone_13: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
       "                    scaled_dot_product_attention_2: \"f32[1, 12, 261, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_11, clone_12, clone_13, scale = 0.125);  clone_11 = clone_12 = clone_13 = None\n",
       "                    transpose_3: \"f32[1, 261, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_2, 1, 2);  scaled_dot_product_attention_2 = None\n",
       "                    clone_14: \"f32[1, 261, 12, 64]\" = torch.ops.aten.clone.default(transpose_3, memory_format = torch.contiguous_format);  transpose_3 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:259 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)\n",
       "                    view_14: \"f32[1, 261, 768]\" = torch.ops.aten.view.default(clone_14, [1, 261, 768]);  clone_14 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_15: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(view_14, p_backbone_encoder_layer_2_attention_output_dense_weight, p_backbone_encoder_layer_2_attention_output_dense_bias);  view_14 = p_backbone_encoder_layer_2_attention_output_dense_weight = p_backbone_encoder_layer_2_attention_output_dense_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_15: \"f32[1, 261, 768]\" = torch.ops.aten.clone.default(linear_15);  linear_15 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_4: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(clone_15, p_backbone_encoder_layer_2_layer_scale1_lambda1);  clone_15 = p_backbone_encoder_layer_2_layer_scale1_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:440 in forward, code: hidden_states = self.drop_path(attention_output) + hidden_states\n",
       "                    add_5: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_4, add_4);  mul_4 = add_4 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_5: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_5, [768], p_backbone_encoder_layer_2_norm2_weight, p_backbone_encoder_layer_2_norm2_bias, 1e-06);  p_backbone_encoder_layer_2_norm2_weight = p_backbone_encoder_layer_2_norm2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_16: \"f32[1, 261, 3072]\" = torch.ops.aten.linear.default(layer_norm_5, p_backbone_encoder_layer_2_mlp_fc1_weight, p_backbone_encoder_layer_2_mlp_fc1_bias);  layer_norm_5 = p_backbone_encoder_layer_2_mlp_fc1_weight = p_backbone_encoder_layer_2_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\activations.py:69 in forward, code: return self.act(input)\n",
       "                    gelu_2: \"f32[1, 261, 3072]\" = torch.ops.aten.gelu.default(linear_16);  linear_16 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_17: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(gelu_2, p_backbone_encoder_layer_2_mlp_fc2_weight, p_backbone_encoder_layer_2_mlp_fc2_bias);  gelu_2 = p_backbone_encoder_layer_2_mlp_fc2_weight = p_backbone_encoder_layer_2_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_5: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(linear_17, p_backbone_encoder_layer_2_layer_scale2_lambda1);  linear_17 = p_backbone_encoder_layer_2_layer_scale2_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:448 in forward, code: layer_output = self.drop_path(layer_output) + hidden_states\n",
       "                    add_6: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_5, add_5);  mul_5 = add_5 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_6: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_6, [768], p_backbone_encoder_layer_3_norm1_weight, p_backbone_encoder_layer_3_norm1_bias, 1e-06);  p_backbone_encoder_layer_3_norm1_weight = p_backbone_encoder_layer_3_norm1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_18: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_6, p_backbone_encoder_layer_3_attention_attention_key_weight, p_backbone_encoder_layer_3_attention_attention_key_bias);  p_backbone_encoder_layer_3_attention_attention_key_weight = p_backbone_encoder_layer_3_attention_attention_key_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:233 in forward, code: key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
       "                    view_15: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_18, [1, 261, 12, 64]);  linear_18 = None\n",
       "                    permute_11: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_19: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_6, p_backbone_encoder_layer_3_attention_attention_value_weight, p_backbone_encoder_layer_3_attention_attention_value_bias);  p_backbone_encoder_layer_3_attention_attention_value_weight = p_backbone_encoder_layer_3_attention_attention_value_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:234 in forward, code: value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
       "                    view_16: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_19, [1, 261, 12, 64]);  linear_19 = None\n",
       "                    permute_12: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_20: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_6, p_backbone_encoder_layer_3_attention_attention_query_weight, p_backbone_encoder_layer_3_attention_attention_query_bias);  layer_norm_6 = p_backbone_encoder_layer_3_attention_attention_query_weight = p_backbone_encoder_layer_3_attention_attention_query_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:235 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
       "                    view_17: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_20, [1, 261, 12, 64]);  linear_20 = None\n",
       "                    permute_13: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_17, [0, 2, 1, 3]);  view_17 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:247 in forward, code: context_layer, attention_probs = attention_interface(\n",
       "                    clone_16: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_13, memory_format = torch.contiguous_format);  permute_13 = None\n",
       "                    clone_17: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_11, memory_format = torch.contiguous_format);  permute_11 = None\n",
       "                    clone_18: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_12, memory_format = torch.contiguous_format);  permute_12 = None\n",
       "                    scaled_dot_product_attention_3: \"f32[1, 12, 261, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_16, clone_17, clone_18, scale = 0.125);  clone_16 = clone_17 = clone_18 = None\n",
       "                    transpose_4: \"f32[1, 261, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_3, 1, 2);  scaled_dot_product_attention_3 = None\n",
       "                    clone_19: \"f32[1, 261, 12, 64]\" = torch.ops.aten.clone.default(transpose_4, memory_format = torch.contiguous_format);  transpose_4 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:259 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)\n",
       "                    view_18: \"f32[1, 261, 768]\" = torch.ops.aten.view.default(clone_19, [1, 261, 768]);  clone_19 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_21: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(view_18, p_backbone_encoder_layer_3_attention_output_dense_weight, p_backbone_encoder_layer_3_attention_output_dense_bias);  view_18 = p_backbone_encoder_layer_3_attention_output_dense_weight = p_backbone_encoder_layer_3_attention_output_dense_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_20: \"f32[1, 261, 768]\" = torch.ops.aten.clone.default(linear_21);  linear_21 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_6: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(clone_20, p_backbone_encoder_layer_3_layer_scale1_lambda1);  clone_20 = p_backbone_encoder_layer_3_layer_scale1_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:440 in forward, code: hidden_states = self.drop_path(attention_output) + hidden_states\n",
       "                    add_7: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_6, add_6);  mul_6 = add_6 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_7: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_7, [768], p_backbone_encoder_layer_3_norm2_weight, p_backbone_encoder_layer_3_norm2_bias, 1e-06);  p_backbone_encoder_layer_3_norm2_weight = p_backbone_encoder_layer_3_norm2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_22: \"f32[1, 261, 3072]\" = torch.ops.aten.linear.default(layer_norm_7, p_backbone_encoder_layer_3_mlp_fc1_weight, p_backbone_encoder_layer_3_mlp_fc1_bias);  layer_norm_7 = p_backbone_encoder_layer_3_mlp_fc1_weight = p_backbone_encoder_layer_3_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\activations.py:69 in forward, code: return self.act(input)\n",
       "                    gelu_3: \"f32[1, 261, 3072]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_23: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(gelu_3, p_backbone_encoder_layer_3_mlp_fc2_weight, p_backbone_encoder_layer_3_mlp_fc2_bias);  gelu_3 = p_backbone_encoder_layer_3_mlp_fc2_weight = p_backbone_encoder_layer_3_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_7: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(linear_23, p_backbone_encoder_layer_3_layer_scale2_lambda1);  linear_23 = p_backbone_encoder_layer_3_layer_scale2_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:448 in forward, code: layer_output = self.drop_path(layer_output) + hidden_states\n",
       "                    add_8: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_7, add_7);  mul_7 = add_7 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_8: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_8, [768], p_backbone_encoder_layer_4_norm1_weight, p_backbone_encoder_layer_4_norm1_bias, 1e-06);  p_backbone_encoder_layer_4_norm1_weight = p_backbone_encoder_layer_4_norm1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_24: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_8, p_backbone_encoder_layer_4_attention_attention_key_weight, p_backbone_encoder_layer_4_attention_attention_key_bias);  p_backbone_encoder_layer_4_attention_attention_key_weight = p_backbone_encoder_layer_4_attention_attention_key_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:233 in forward, code: key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
       "                    view_19: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_24, [1, 261, 12, 64]);  linear_24 = None\n",
       "                    permute_14: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_19, [0, 2, 1, 3]);  view_19 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_25: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_8, p_backbone_encoder_layer_4_attention_attention_value_weight, p_backbone_encoder_layer_4_attention_attention_value_bias);  p_backbone_encoder_layer_4_attention_attention_value_weight = p_backbone_encoder_layer_4_attention_attention_value_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:234 in forward, code: value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
       "                    view_20: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_25, [1, 261, 12, 64]);  linear_25 = None\n",
       "                    permute_15: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_20, [0, 2, 1, 3]);  view_20 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_26: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_8, p_backbone_encoder_layer_4_attention_attention_query_weight, p_backbone_encoder_layer_4_attention_attention_query_bias);  layer_norm_8 = p_backbone_encoder_layer_4_attention_attention_query_weight = p_backbone_encoder_layer_4_attention_attention_query_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:235 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
       "                    view_21: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_26, [1, 261, 12, 64]);  linear_26 = None\n",
       "                    permute_16: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_21, [0, 2, 1, 3]);  view_21 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:247 in forward, code: context_layer, attention_probs = attention_interface(\n",
       "                    clone_21: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_16, memory_format = torch.contiguous_format);  permute_16 = None\n",
       "                    clone_22: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_14, memory_format = torch.contiguous_format);  permute_14 = None\n",
       "                    clone_23: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_15, memory_format = torch.contiguous_format);  permute_15 = None\n",
       "                    scaled_dot_product_attention_4: \"f32[1, 12, 261, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_21, clone_22, clone_23, scale = 0.125);  clone_21 = clone_22 = clone_23 = None\n",
       "                    transpose_5: \"f32[1, 261, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_4, 1, 2);  scaled_dot_product_attention_4 = None\n",
       "                    clone_24: \"f32[1, 261, 12, 64]\" = torch.ops.aten.clone.default(transpose_5, memory_format = torch.contiguous_format);  transpose_5 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:259 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)\n",
       "                    view_22: \"f32[1, 261, 768]\" = torch.ops.aten.view.default(clone_24, [1, 261, 768]);  clone_24 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_27: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(view_22, p_backbone_encoder_layer_4_attention_output_dense_weight, p_backbone_encoder_layer_4_attention_output_dense_bias);  view_22 = p_backbone_encoder_layer_4_attention_output_dense_weight = p_backbone_encoder_layer_4_attention_output_dense_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_25: \"f32[1, 261, 768]\" = torch.ops.aten.clone.default(linear_27);  linear_27 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_8: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(clone_25, p_backbone_encoder_layer_4_layer_scale1_lambda1);  clone_25 = p_backbone_encoder_layer_4_layer_scale1_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:440 in forward, code: hidden_states = self.drop_path(attention_output) + hidden_states\n",
       "                    add_9: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_8, add_8);  mul_8 = add_8 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_9: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_9, [768], p_backbone_encoder_layer_4_norm2_weight, p_backbone_encoder_layer_4_norm2_bias, 1e-06);  p_backbone_encoder_layer_4_norm2_weight = p_backbone_encoder_layer_4_norm2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_28: \"f32[1, 261, 3072]\" = torch.ops.aten.linear.default(layer_norm_9, p_backbone_encoder_layer_4_mlp_fc1_weight, p_backbone_encoder_layer_4_mlp_fc1_bias);  layer_norm_9 = p_backbone_encoder_layer_4_mlp_fc1_weight = p_backbone_encoder_layer_4_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\activations.py:69 in forward, code: return self.act(input)\n",
       "                    gelu_4: \"f32[1, 261, 3072]\" = torch.ops.aten.gelu.default(linear_28);  linear_28 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_29: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(gelu_4, p_backbone_encoder_layer_4_mlp_fc2_weight, p_backbone_encoder_layer_4_mlp_fc2_bias);  gelu_4 = p_backbone_encoder_layer_4_mlp_fc2_weight = p_backbone_encoder_layer_4_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_9: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(linear_29, p_backbone_encoder_layer_4_layer_scale2_lambda1);  linear_29 = p_backbone_encoder_layer_4_layer_scale2_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:448 in forward, code: layer_output = self.drop_path(layer_output) + hidden_states\n",
       "                    add_10: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_9, add_9);  mul_9 = add_9 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_10: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_10, [768], p_backbone_encoder_layer_5_norm1_weight, p_backbone_encoder_layer_5_norm1_bias, 1e-06);  p_backbone_encoder_layer_5_norm1_weight = p_backbone_encoder_layer_5_norm1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_30: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_10, p_backbone_encoder_layer_5_attention_attention_key_weight, p_backbone_encoder_layer_5_attention_attention_key_bias);  p_backbone_encoder_layer_5_attention_attention_key_weight = p_backbone_encoder_layer_5_attention_attention_key_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:233 in forward, code: key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
       "                    view_23: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_30, [1, 261, 12, 64]);  linear_30 = None\n",
       "                    permute_17: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_23, [0, 2, 1, 3]);  view_23 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_31: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_10, p_backbone_encoder_layer_5_attention_attention_value_weight, p_backbone_encoder_layer_5_attention_attention_value_bias);  p_backbone_encoder_layer_5_attention_attention_value_weight = p_backbone_encoder_layer_5_attention_attention_value_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:234 in forward, code: value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
       "                    view_24: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_31, [1, 261, 12, 64]);  linear_31 = None\n",
       "                    permute_18: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_24, [0, 2, 1, 3]);  view_24 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_32: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_10, p_backbone_encoder_layer_5_attention_attention_query_weight, p_backbone_encoder_layer_5_attention_attention_query_bias);  layer_norm_10 = p_backbone_encoder_layer_5_attention_attention_query_weight = p_backbone_encoder_layer_5_attention_attention_query_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:235 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
       "                    view_25: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_32, [1, 261, 12, 64]);  linear_32 = None\n",
       "                    permute_19: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_25, [0, 2, 1, 3]);  view_25 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:247 in forward, code: context_layer, attention_probs = attention_interface(\n",
       "                    clone_26: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_19, memory_format = torch.contiguous_format);  permute_19 = None\n",
       "                    clone_27: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_17, memory_format = torch.contiguous_format);  permute_17 = None\n",
       "                    clone_28: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_18, memory_format = torch.contiguous_format);  permute_18 = None\n",
       "                    scaled_dot_product_attention_5: \"f32[1, 12, 261, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_26, clone_27, clone_28, scale = 0.125);  clone_26 = clone_27 = clone_28 = None\n",
       "                    transpose_6: \"f32[1, 261, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_5, 1, 2);  scaled_dot_product_attention_5 = None\n",
       "                    clone_29: \"f32[1, 261, 12, 64]\" = torch.ops.aten.clone.default(transpose_6, memory_format = torch.contiguous_format);  transpose_6 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:259 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)\n",
       "                    view_26: \"f32[1, 261, 768]\" = torch.ops.aten.view.default(clone_29, [1, 261, 768]);  clone_29 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_33: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(view_26, p_backbone_encoder_layer_5_attention_output_dense_weight, p_backbone_encoder_layer_5_attention_output_dense_bias);  view_26 = p_backbone_encoder_layer_5_attention_output_dense_weight = p_backbone_encoder_layer_5_attention_output_dense_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_30: \"f32[1, 261, 768]\" = torch.ops.aten.clone.default(linear_33);  linear_33 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_10: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(clone_30, p_backbone_encoder_layer_5_layer_scale1_lambda1);  clone_30 = p_backbone_encoder_layer_5_layer_scale1_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:440 in forward, code: hidden_states = self.drop_path(attention_output) + hidden_states\n",
       "                    add_11: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_10, add_10);  mul_10 = add_10 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_11: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_11, [768], p_backbone_encoder_layer_5_norm2_weight, p_backbone_encoder_layer_5_norm2_bias, 1e-06);  p_backbone_encoder_layer_5_norm2_weight = p_backbone_encoder_layer_5_norm2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_34: \"f32[1, 261, 3072]\" = torch.ops.aten.linear.default(layer_norm_11, p_backbone_encoder_layer_5_mlp_fc1_weight, p_backbone_encoder_layer_5_mlp_fc1_bias);  layer_norm_11 = p_backbone_encoder_layer_5_mlp_fc1_weight = p_backbone_encoder_layer_5_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\activations.py:69 in forward, code: return self.act(input)\n",
       "                    gelu_5: \"f32[1, 261, 3072]\" = torch.ops.aten.gelu.default(linear_34);  linear_34 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_35: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(gelu_5, p_backbone_encoder_layer_5_mlp_fc2_weight, p_backbone_encoder_layer_5_mlp_fc2_bias);  gelu_5 = p_backbone_encoder_layer_5_mlp_fc2_weight = p_backbone_encoder_layer_5_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_11: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(linear_35, p_backbone_encoder_layer_5_layer_scale2_lambda1);  linear_35 = p_backbone_encoder_layer_5_layer_scale2_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:448 in forward, code: layer_output = self.drop_path(layer_output) + hidden_states\n",
       "                    add_12: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_11, add_11);  mul_11 = add_11 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_12: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_12, [768], p_backbone_encoder_layer_6_norm1_weight, p_backbone_encoder_layer_6_norm1_bias, 1e-06);  p_backbone_encoder_layer_6_norm1_weight = p_backbone_encoder_layer_6_norm1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_36: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_12, p_backbone_encoder_layer_6_attention_attention_key_weight, p_backbone_encoder_layer_6_attention_attention_key_bias);  p_backbone_encoder_layer_6_attention_attention_key_weight = p_backbone_encoder_layer_6_attention_attention_key_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:233 in forward, code: key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
       "                    view_27: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_36, [1, 261, 12, 64]);  linear_36 = None\n",
       "                    permute_20: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_27, [0, 2, 1, 3]);  view_27 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_37: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_12, p_backbone_encoder_layer_6_attention_attention_value_weight, p_backbone_encoder_layer_6_attention_attention_value_bias);  p_backbone_encoder_layer_6_attention_attention_value_weight = p_backbone_encoder_layer_6_attention_attention_value_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:234 in forward, code: value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
       "                    view_28: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_37, [1, 261, 12, 64]);  linear_37 = None\n",
       "                    permute_21: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_28, [0, 2, 1, 3]);  view_28 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_38: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_12, p_backbone_encoder_layer_6_attention_attention_query_weight, p_backbone_encoder_layer_6_attention_attention_query_bias);  layer_norm_12 = p_backbone_encoder_layer_6_attention_attention_query_weight = p_backbone_encoder_layer_6_attention_attention_query_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:235 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
       "                    view_29: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_38, [1, 261, 12, 64]);  linear_38 = None\n",
       "                    permute_22: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_29, [0, 2, 1, 3]);  view_29 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:247 in forward, code: context_layer, attention_probs = attention_interface(\n",
       "                    clone_31: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_22, memory_format = torch.contiguous_format);  permute_22 = None\n",
       "                    clone_32: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_20, memory_format = torch.contiguous_format);  permute_20 = None\n",
       "                    clone_33: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_21, memory_format = torch.contiguous_format);  permute_21 = None\n",
       "                    scaled_dot_product_attention_6: \"f32[1, 12, 261, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_31, clone_32, clone_33, scale = 0.125);  clone_31 = clone_32 = clone_33 = None\n",
       "                    transpose_7: \"f32[1, 261, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_6, 1, 2);  scaled_dot_product_attention_6 = None\n",
       "                    clone_34: \"f32[1, 261, 12, 64]\" = torch.ops.aten.clone.default(transpose_7, memory_format = torch.contiguous_format);  transpose_7 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:259 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)\n",
       "                    view_30: \"f32[1, 261, 768]\" = torch.ops.aten.view.default(clone_34, [1, 261, 768]);  clone_34 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_39: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(view_30, p_backbone_encoder_layer_6_attention_output_dense_weight, p_backbone_encoder_layer_6_attention_output_dense_bias);  view_30 = p_backbone_encoder_layer_6_attention_output_dense_weight = p_backbone_encoder_layer_6_attention_output_dense_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_35: \"f32[1, 261, 768]\" = torch.ops.aten.clone.default(linear_39);  linear_39 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_12: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(clone_35, p_backbone_encoder_layer_6_layer_scale1_lambda1);  clone_35 = p_backbone_encoder_layer_6_layer_scale1_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:440 in forward, code: hidden_states = self.drop_path(attention_output) + hidden_states\n",
       "                    add_13: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_12, add_12);  mul_12 = add_12 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_13: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_13, [768], p_backbone_encoder_layer_6_norm2_weight, p_backbone_encoder_layer_6_norm2_bias, 1e-06);  p_backbone_encoder_layer_6_norm2_weight = p_backbone_encoder_layer_6_norm2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_40: \"f32[1, 261, 3072]\" = torch.ops.aten.linear.default(layer_norm_13, p_backbone_encoder_layer_6_mlp_fc1_weight, p_backbone_encoder_layer_6_mlp_fc1_bias);  layer_norm_13 = p_backbone_encoder_layer_6_mlp_fc1_weight = p_backbone_encoder_layer_6_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\activations.py:69 in forward, code: return self.act(input)\n",
       "                    gelu_6: \"f32[1, 261, 3072]\" = torch.ops.aten.gelu.default(linear_40);  linear_40 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_41: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(gelu_6, p_backbone_encoder_layer_6_mlp_fc2_weight, p_backbone_encoder_layer_6_mlp_fc2_bias);  gelu_6 = p_backbone_encoder_layer_6_mlp_fc2_weight = p_backbone_encoder_layer_6_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_13: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(linear_41, p_backbone_encoder_layer_6_layer_scale2_lambda1);  linear_41 = p_backbone_encoder_layer_6_layer_scale2_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:448 in forward, code: layer_output = self.drop_path(layer_output) + hidden_states\n",
       "                    add_14: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_13, add_13);  mul_13 = add_13 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_14: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_14, [768], p_backbone_encoder_layer_7_norm1_weight, p_backbone_encoder_layer_7_norm1_bias, 1e-06);  p_backbone_encoder_layer_7_norm1_weight = p_backbone_encoder_layer_7_norm1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_42: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_14, p_backbone_encoder_layer_7_attention_attention_key_weight, p_backbone_encoder_layer_7_attention_attention_key_bias);  p_backbone_encoder_layer_7_attention_attention_key_weight = p_backbone_encoder_layer_7_attention_attention_key_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:233 in forward, code: key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
       "                    view_31: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_42, [1, 261, 12, 64]);  linear_42 = None\n",
       "                    permute_23: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_31, [0, 2, 1, 3]);  view_31 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_43: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_14, p_backbone_encoder_layer_7_attention_attention_value_weight, p_backbone_encoder_layer_7_attention_attention_value_bias);  p_backbone_encoder_layer_7_attention_attention_value_weight = p_backbone_encoder_layer_7_attention_attention_value_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:234 in forward, code: value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
       "                    view_32: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_43, [1, 261, 12, 64]);  linear_43 = None\n",
       "                    permute_24: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_32, [0, 2, 1, 3]);  view_32 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_44: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_14, p_backbone_encoder_layer_7_attention_attention_query_weight, p_backbone_encoder_layer_7_attention_attention_query_bias);  layer_norm_14 = p_backbone_encoder_layer_7_attention_attention_query_weight = p_backbone_encoder_layer_7_attention_attention_query_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:235 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
       "                    view_33: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_44, [1, 261, 12, 64]);  linear_44 = None\n",
       "                    permute_25: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_33, [0, 2, 1, 3]);  view_33 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:247 in forward, code: context_layer, attention_probs = attention_interface(\n",
       "                    clone_36: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_25, memory_format = torch.contiguous_format);  permute_25 = None\n",
       "                    clone_37: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_23, memory_format = torch.contiguous_format);  permute_23 = None\n",
       "                    clone_38: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_24, memory_format = torch.contiguous_format);  permute_24 = None\n",
       "                    scaled_dot_product_attention_7: \"f32[1, 12, 261, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_36, clone_37, clone_38, scale = 0.125);  clone_36 = clone_37 = clone_38 = None\n",
       "                    transpose_8: \"f32[1, 261, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_7, 1, 2);  scaled_dot_product_attention_7 = None\n",
       "                    clone_39: \"f32[1, 261, 12, 64]\" = torch.ops.aten.clone.default(transpose_8, memory_format = torch.contiguous_format);  transpose_8 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:259 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)\n",
       "                    view_34: \"f32[1, 261, 768]\" = torch.ops.aten.view.default(clone_39, [1, 261, 768]);  clone_39 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_45: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(view_34, p_backbone_encoder_layer_7_attention_output_dense_weight, p_backbone_encoder_layer_7_attention_output_dense_bias);  view_34 = p_backbone_encoder_layer_7_attention_output_dense_weight = p_backbone_encoder_layer_7_attention_output_dense_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_40: \"f32[1, 261, 768]\" = torch.ops.aten.clone.default(linear_45);  linear_45 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_14: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(clone_40, p_backbone_encoder_layer_7_layer_scale1_lambda1);  clone_40 = p_backbone_encoder_layer_7_layer_scale1_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:440 in forward, code: hidden_states = self.drop_path(attention_output) + hidden_states\n",
       "                    add_15: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_14, add_14);  mul_14 = add_14 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_15: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_15, [768], p_backbone_encoder_layer_7_norm2_weight, p_backbone_encoder_layer_7_norm2_bias, 1e-06);  p_backbone_encoder_layer_7_norm2_weight = p_backbone_encoder_layer_7_norm2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_46: \"f32[1, 261, 3072]\" = torch.ops.aten.linear.default(layer_norm_15, p_backbone_encoder_layer_7_mlp_fc1_weight, p_backbone_encoder_layer_7_mlp_fc1_bias);  layer_norm_15 = p_backbone_encoder_layer_7_mlp_fc1_weight = p_backbone_encoder_layer_7_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\activations.py:69 in forward, code: return self.act(input)\n",
       "                    gelu_7: \"f32[1, 261, 3072]\" = torch.ops.aten.gelu.default(linear_46);  linear_46 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_47: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(gelu_7, p_backbone_encoder_layer_7_mlp_fc2_weight, p_backbone_encoder_layer_7_mlp_fc2_bias);  gelu_7 = p_backbone_encoder_layer_7_mlp_fc2_weight = p_backbone_encoder_layer_7_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_15: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(linear_47, p_backbone_encoder_layer_7_layer_scale2_lambda1);  linear_47 = p_backbone_encoder_layer_7_layer_scale2_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:448 in forward, code: layer_output = self.drop_path(layer_output) + hidden_states\n",
       "                    add_16: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_15, add_15);  mul_15 = add_15 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_16: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_16, [768], p_backbone_encoder_layer_8_norm1_weight, p_backbone_encoder_layer_8_norm1_bias, 1e-06);  p_backbone_encoder_layer_8_norm1_weight = p_backbone_encoder_layer_8_norm1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_48: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_16, p_backbone_encoder_layer_8_attention_attention_key_weight, p_backbone_encoder_layer_8_attention_attention_key_bias);  p_backbone_encoder_layer_8_attention_attention_key_weight = p_backbone_encoder_layer_8_attention_attention_key_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:233 in forward, code: key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
       "                    view_35: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_48, [1, 261, 12, 64]);  linear_48 = None\n",
       "                    permute_26: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_35, [0, 2, 1, 3]);  view_35 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_49: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_16, p_backbone_encoder_layer_8_attention_attention_value_weight, p_backbone_encoder_layer_8_attention_attention_value_bias);  p_backbone_encoder_layer_8_attention_attention_value_weight = p_backbone_encoder_layer_8_attention_attention_value_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:234 in forward, code: value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
       "                    view_36: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_49, [1, 261, 12, 64]);  linear_49 = None\n",
       "                    permute_27: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_36, [0, 2, 1, 3]);  view_36 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_50: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_16, p_backbone_encoder_layer_8_attention_attention_query_weight, p_backbone_encoder_layer_8_attention_attention_query_bias);  layer_norm_16 = p_backbone_encoder_layer_8_attention_attention_query_weight = p_backbone_encoder_layer_8_attention_attention_query_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:235 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
       "                    view_37: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_50, [1, 261, 12, 64]);  linear_50 = None\n",
       "                    permute_28: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_37, [0, 2, 1, 3]);  view_37 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:247 in forward, code: context_layer, attention_probs = attention_interface(\n",
       "                    clone_41: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_28, memory_format = torch.contiguous_format);  permute_28 = None\n",
       "                    clone_42: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_26, memory_format = torch.contiguous_format);  permute_26 = None\n",
       "                    clone_43: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_27, memory_format = torch.contiguous_format);  permute_27 = None\n",
       "                    scaled_dot_product_attention_8: \"f32[1, 12, 261, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_41, clone_42, clone_43, scale = 0.125);  clone_41 = clone_42 = clone_43 = None\n",
       "                    transpose_9: \"f32[1, 261, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_8, 1, 2);  scaled_dot_product_attention_8 = None\n",
       "                    clone_44: \"f32[1, 261, 12, 64]\" = torch.ops.aten.clone.default(transpose_9, memory_format = torch.contiguous_format);  transpose_9 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:259 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)\n",
       "                    view_38: \"f32[1, 261, 768]\" = torch.ops.aten.view.default(clone_44, [1, 261, 768]);  clone_44 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_51: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(view_38, p_backbone_encoder_layer_8_attention_output_dense_weight, p_backbone_encoder_layer_8_attention_output_dense_bias);  view_38 = p_backbone_encoder_layer_8_attention_output_dense_weight = p_backbone_encoder_layer_8_attention_output_dense_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_45: \"f32[1, 261, 768]\" = torch.ops.aten.clone.default(linear_51);  linear_51 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_16: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(clone_45, p_backbone_encoder_layer_8_layer_scale1_lambda1);  clone_45 = p_backbone_encoder_layer_8_layer_scale1_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:440 in forward, code: hidden_states = self.drop_path(attention_output) + hidden_states\n",
       "                    add_17: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_16, add_16);  mul_16 = add_16 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_17: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_17, [768], p_backbone_encoder_layer_8_norm2_weight, p_backbone_encoder_layer_8_norm2_bias, 1e-06);  p_backbone_encoder_layer_8_norm2_weight = p_backbone_encoder_layer_8_norm2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_52: \"f32[1, 261, 3072]\" = torch.ops.aten.linear.default(layer_norm_17, p_backbone_encoder_layer_8_mlp_fc1_weight, p_backbone_encoder_layer_8_mlp_fc1_bias);  layer_norm_17 = p_backbone_encoder_layer_8_mlp_fc1_weight = p_backbone_encoder_layer_8_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\activations.py:69 in forward, code: return self.act(input)\n",
       "                    gelu_8: \"f32[1, 261, 3072]\" = torch.ops.aten.gelu.default(linear_52);  linear_52 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_53: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(gelu_8, p_backbone_encoder_layer_8_mlp_fc2_weight, p_backbone_encoder_layer_8_mlp_fc2_bias);  gelu_8 = p_backbone_encoder_layer_8_mlp_fc2_weight = p_backbone_encoder_layer_8_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_17: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(linear_53, p_backbone_encoder_layer_8_layer_scale2_lambda1);  linear_53 = p_backbone_encoder_layer_8_layer_scale2_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:448 in forward, code: layer_output = self.drop_path(layer_output) + hidden_states\n",
       "                    add_18: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_17, add_17);  mul_17 = add_17 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_18: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_18, [768], p_backbone_encoder_layer_9_norm1_weight, p_backbone_encoder_layer_9_norm1_bias, 1e-06);  p_backbone_encoder_layer_9_norm1_weight = p_backbone_encoder_layer_9_norm1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_54: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_18, p_backbone_encoder_layer_9_attention_attention_key_weight, p_backbone_encoder_layer_9_attention_attention_key_bias);  p_backbone_encoder_layer_9_attention_attention_key_weight = p_backbone_encoder_layer_9_attention_attention_key_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:233 in forward, code: key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
       "                    view_39: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_54, [1, 261, 12, 64]);  linear_54 = None\n",
       "                    permute_29: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_39, [0, 2, 1, 3]);  view_39 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_55: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_18, p_backbone_encoder_layer_9_attention_attention_value_weight, p_backbone_encoder_layer_9_attention_attention_value_bias);  p_backbone_encoder_layer_9_attention_attention_value_weight = p_backbone_encoder_layer_9_attention_attention_value_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:234 in forward, code: value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
       "                    view_40: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_55, [1, 261, 12, 64]);  linear_55 = None\n",
       "                    permute_30: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_40, [0, 2, 1, 3]);  view_40 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_56: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_18, p_backbone_encoder_layer_9_attention_attention_query_weight, p_backbone_encoder_layer_9_attention_attention_query_bias);  layer_norm_18 = p_backbone_encoder_layer_9_attention_attention_query_weight = p_backbone_encoder_layer_9_attention_attention_query_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:235 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
       "                    view_41: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_56, [1, 261, 12, 64]);  linear_56 = None\n",
       "                    permute_31: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_41, [0, 2, 1, 3]);  view_41 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:247 in forward, code: context_layer, attention_probs = attention_interface(\n",
       "                    clone_46: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_31, memory_format = torch.contiguous_format);  permute_31 = None\n",
       "                    clone_47: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_29, memory_format = torch.contiguous_format);  permute_29 = None\n",
       "                    clone_48: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_30, memory_format = torch.contiguous_format);  permute_30 = None\n",
       "                    scaled_dot_product_attention_9: \"f32[1, 12, 261, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_46, clone_47, clone_48, scale = 0.125);  clone_46 = clone_47 = clone_48 = None\n",
       "                    transpose_10: \"f32[1, 261, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_9, 1, 2);  scaled_dot_product_attention_9 = None\n",
       "                    clone_49: \"f32[1, 261, 12, 64]\" = torch.ops.aten.clone.default(transpose_10, memory_format = torch.contiguous_format);  transpose_10 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:259 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)\n",
       "                    view_42: \"f32[1, 261, 768]\" = torch.ops.aten.view.default(clone_49, [1, 261, 768]);  clone_49 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_57: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(view_42, p_backbone_encoder_layer_9_attention_output_dense_weight, p_backbone_encoder_layer_9_attention_output_dense_bias);  view_42 = p_backbone_encoder_layer_9_attention_output_dense_weight = p_backbone_encoder_layer_9_attention_output_dense_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_50: \"f32[1, 261, 768]\" = torch.ops.aten.clone.default(linear_57);  linear_57 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_18: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(clone_50, p_backbone_encoder_layer_9_layer_scale1_lambda1);  clone_50 = p_backbone_encoder_layer_9_layer_scale1_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:440 in forward, code: hidden_states = self.drop_path(attention_output) + hidden_states\n",
       "                    add_19: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_18, add_18);  mul_18 = add_18 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_19: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_19, [768], p_backbone_encoder_layer_9_norm2_weight, p_backbone_encoder_layer_9_norm2_bias, 1e-06);  p_backbone_encoder_layer_9_norm2_weight = p_backbone_encoder_layer_9_norm2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_58: \"f32[1, 261, 3072]\" = torch.ops.aten.linear.default(layer_norm_19, p_backbone_encoder_layer_9_mlp_fc1_weight, p_backbone_encoder_layer_9_mlp_fc1_bias);  layer_norm_19 = p_backbone_encoder_layer_9_mlp_fc1_weight = p_backbone_encoder_layer_9_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\activations.py:69 in forward, code: return self.act(input)\n",
       "                    gelu_9: \"f32[1, 261, 3072]\" = torch.ops.aten.gelu.default(linear_58);  linear_58 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_59: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(gelu_9, p_backbone_encoder_layer_9_mlp_fc2_weight, p_backbone_encoder_layer_9_mlp_fc2_bias);  gelu_9 = p_backbone_encoder_layer_9_mlp_fc2_weight = p_backbone_encoder_layer_9_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_19: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(linear_59, p_backbone_encoder_layer_9_layer_scale2_lambda1);  linear_59 = p_backbone_encoder_layer_9_layer_scale2_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:448 in forward, code: layer_output = self.drop_path(layer_output) + hidden_states\n",
       "                    add_20: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_19, add_19);  mul_19 = add_19 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_20: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_20, [768], p_backbone_encoder_layer_10_norm1_weight, p_backbone_encoder_layer_10_norm1_bias, 1e-06);  p_backbone_encoder_layer_10_norm1_weight = p_backbone_encoder_layer_10_norm1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_60: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_20, p_backbone_encoder_layer_10_attention_attention_key_weight, p_backbone_encoder_layer_10_attention_attention_key_bias);  p_backbone_encoder_layer_10_attention_attention_key_weight = p_backbone_encoder_layer_10_attention_attention_key_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:233 in forward, code: key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
       "                    view_43: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_60, [1, 261, 12, 64]);  linear_60 = None\n",
       "                    permute_32: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_43, [0, 2, 1, 3]);  view_43 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_61: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_20, p_backbone_encoder_layer_10_attention_attention_value_weight, p_backbone_encoder_layer_10_attention_attention_value_bias);  p_backbone_encoder_layer_10_attention_attention_value_weight = p_backbone_encoder_layer_10_attention_attention_value_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:234 in forward, code: value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
       "                    view_44: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_61, [1, 261, 12, 64]);  linear_61 = None\n",
       "                    permute_33: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_44, [0, 2, 1, 3]);  view_44 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_62: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_20, p_backbone_encoder_layer_10_attention_attention_query_weight, p_backbone_encoder_layer_10_attention_attention_query_bias);  layer_norm_20 = p_backbone_encoder_layer_10_attention_attention_query_weight = p_backbone_encoder_layer_10_attention_attention_query_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:235 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
       "                    view_45: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_62, [1, 261, 12, 64]);  linear_62 = None\n",
       "                    permute_34: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_45, [0, 2, 1, 3]);  view_45 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:247 in forward, code: context_layer, attention_probs = attention_interface(\n",
       "                    clone_51: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_34, memory_format = torch.contiguous_format);  permute_34 = None\n",
       "                    clone_52: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_32, memory_format = torch.contiguous_format);  permute_32 = None\n",
       "                    clone_53: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_33, memory_format = torch.contiguous_format);  permute_33 = None\n",
       "                    scaled_dot_product_attention_10: \"f32[1, 12, 261, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_51, clone_52, clone_53, scale = 0.125);  clone_51 = clone_52 = clone_53 = None\n",
       "                    transpose_11: \"f32[1, 261, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_10, 1, 2);  scaled_dot_product_attention_10 = None\n",
       "                    clone_54: \"f32[1, 261, 12, 64]\" = torch.ops.aten.clone.default(transpose_11, memory_format = torch.contiguous_format);  transpose_11 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:259 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)\n",
       "                    view_46: \"f32[1, 261, 768]\" = torch.ops.aten.view.default(clone_54, [1, 261, 768]);  clone_54 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_63: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(view_46, p_backbone_encoder_layer_10_attention_output_dense_weight, p_backbone_encoder_layer_10_attention_output_dense_bias);  view_46 = p_backbone_encoder_layer_10_attention_output_dense_weight = p_backbone_encoder_layer_10_attention_output_dense_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_55: \"f32[1, 261, 768]\" = torch.ops.aten.clone.default(linear_63);  linear_63 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_20: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(clone_55, p_backbone_encoder_layer_10_layer_scale1_lambda1);  clone_55 = p_backbone_encoder_layer_10_layer_scale1_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:440 in forward, code: hidden_states = self.drop_path(attention_output) + hidden_states\n",
       "                    add_21: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_20, add_20);  mul_20 = add_20 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_21: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_21, [768], p_backbone_encoder_layer_10_norm2_weight, p_backbone_encoder_layer_10_norm2_bias, 1e-06);  p_backbone_encoder_layer_10_norm2_weight = p_backbone_encoder_layer_10_norm2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_64: \"f32[1, 261, 3072]\" = torch.ops.aten.linear.default(layer_norm_21, p_backbone_encoder_layer_10_mlp_fc1_weight, p_backbone_encoder_layer_10_mlp_fc1_bias);  layer_norm_21 = p_backbone_encoder_layer_10_mlp_fc1_weight = p_backbone_encoder_layer_10_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\activations.py:69 in forward, code: return self.act(input)\n",
       "                    gelu_10: \"f32[1, 261, 3072]\" = torch.ops.aten.gelu.default(linear_64);  linear_64 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_65: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(gelu_10, p_backbone_encoder_layer_10_mlp_fc2_weight, p_backbone_encoder_layer_10_mlp_fc2_bias);  gelu_10 = p_backbone_encoder_layer_10_mlp_fc2_weight = p_backbone_encoder_layer_10_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_21: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(linear_65, p_backbone_encoder_layer_10_layer_scale2_lambda1);  linear_65 = p_backbone_encoder_layer_10_layer_scale2_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:448 in forward, code: layer_output = self.drop_path(layer_output) + hidden_states\n",
       "                    add_22: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_21, add_21);  mul_21 = add_21 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_22: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_22, [768], p_backbone_encoder_layer_11_norm1_weight, p_backbone_encoder_layer_11_norm1_bias, 1e-06);  p_backbone_encoder_layer_11_norm1_weight = p_backbone_encoder_layer_11_norm1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_66: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_22, p_backbone_encoder_layer_11_attention_attention_key_weight, p_backbone_encoder_layer_11_attention_attention_key_bias);  p_backbone_encoder_layer_11_attention_attention_key_weight = p_backbone_encoder_layer_11_attention_attention_key_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:233 in forward, code: key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
       "                    view_47: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_66, [1, 261, 12, 64]);  linear_66 = None\n",
       "                    permute_35: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_47, [0, 2, 1, 3]);  view_47 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_67: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_22, p_backbone_encoder_layer_11_attention_attention_value_weight, p_backbone_encoder_layer_11_attention_attention_value_bias);  p_backbone_encoder_layer_11_attention_attention_value_weight = p_backbone_encoder_layer_11_attention_attention_value_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:234 in forward, code: value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
       "                    view_48: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_67, [1, 261, 12, 64]);  linear_67 = None\n",
       "                    permute_36: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_48, [0, 2, 1, 3]);  view_48 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_68: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(layer_norm_22, p_backbone_encoder_layer_11_attention_attention_query_weight, p_backbone_encoder_layer_11_attention_attention_query_bias);  layer_norm_22 = p_backbone_encoder_layer_11_attention_attention_query_weight = p_backbone_encoder_layer_11_attention_attention_query_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:235 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
       "                    view_49: \"f32[1, 261, 12, 64]\" = torch.ops.aten.view.default(linear_68, [1, 261, 12, 64]);  linear_68 = None\n",
       "                    permute_37: \"f32[1, 12, 261, 64]\" = torch.ops.aten.permute.default(view_49, [0, 2, 1, 3]);  view_49 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:247 in forward, code: context_layer, attention_probs = attention_interface(\n",
       "                    clone_56: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_37, memory_format = torch.contiguous_format);  permute_37 = None\n",
       "                    clone_57: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_35, memory_format = torch.contiguous_format);  permute_35 = None\n",
       "                    clone_58: \"f32[1, 12, 261, 64]\" = torch.ops.aten.clone.default(permute_36, memory_format = torch.contiguous_format);  permute_36 = None\n",
       "                    scaled_dot_product_attention_11: \"f32[1, 12, 261, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(clone_56, clone_57, clone_58, scale = 0.125);  clone_56 = clone_57 = clone_58 = None\n",
       "                    transpose_12: \"f32[1, 261, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_11, 1, 2);  scaled_dot_product_attention_11 = None\n",
       "                    clone_59: \"f32[1, 261, 12, 64]\" = torch.ops.aten.clone.default(transpose_12, memory_format = torch.contiguous_format);  transpose_12 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:259 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)\n",
       "                    view_50: \"f32[1, 261, 768]\" = torch.ops.aten.view.default(clone_59, [1, 261, 768]);  clone_59 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_69: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(view_50, p_backbone_encoder_layer_11_attention_output_dense_weight, p_backbone_encoder_layer_11_attention_output_dense_bias);  view_50 = p_backbone_encoder_layer_11_attention_output_dense_weight = p_backbone_encoder_layer_11_attention_output_dense_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_60: \"f32[1, 261, 768]\" = torch.ops.aten.clone.default(linear_69);  linear_69 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_22: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(clone_60, p_backbone_encoder_layer_11_layer_scale1_lambda1);  clone_60 = p_backbone_encoder_layer_11_layer_scale1_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:440 in forward, code: hidden_states = self.drop_path(attention_output) + hidden_states\n",
       "                    add_23: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_22, add_22);  mul_22 = add_22 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_23: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_23, [768], p_backbone_encoder_layer_11_norm2_weight, p_backbone_encoder_layer_11_norm2_bias, 1e-06);  p_backbone_encoder_layer_11_norm2_weight = p_backbone_encoder_layer_11_norm2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_70: \"f32[1, 261, 3072]\" = torch.ops.aten.linear.default(layer_norm_23, p_backbone_encoder_layer_11_mlp_fc1_weight, p_backbone_encoder_layer_11_mlp_fc1_bias);  layer_norm_23 = p_backbone_encoder_layer_11_mlp_fc1_weight = p_backbone_encoder_layer_11_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\activations.py:69 in forward, code: return self.act(input)\n",
       "                    gelu_11: \"f32[1, 261, 3072]\" = torch.ops.aten.gelu.default(linear_70);  linear_70 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_71: \"f32[1, 261, 768]\" = torch.ops.aten.linear.default(gelu_11, p_backbone_encoder_layer_11_mlp_fc2_weight, p_backbone_encoder_layer_11_mlp_fc2_bias);  gelu_11 = p_backbone_encoder_layer_11_mlp_fc2_weight = p_backbone_encoder_layer_11_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:329 in forward, code: return hidden_state * self.lambda1\n",
       "                    mul_23: \"f32[1, 261, 768]\" = torch.ops.aten.mul.Tensor(linear_71, p_backbone_encoder_layer_11_layer_scale2_lambda1);  linear_71 = p_backbone_encoder_layer_11_layer_scale2_lambda1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py:448 in forward, code: layer_output = self.drop_path(layer_output) + hidden_states\n",
       "                    add_24: \"f32[1, 261, 768]\" = torch.ops.aten.add.Tensor(mul_23, add_23);  mul_23 = add_23 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_24: \"f32[1, 261, 768]\" = torch.ops.aten.layer_norm.default(add_24, [768], p_backbone_layernorm_weight, p_backbone_layernorm_bias, 1e-06);  add_24 = p_backbone_layernorm_weight = p_backbone_layernorm_bias = None\n",
       "            \n",
       "                     # File: d:\\ITS\\Semester 6\\Mobile Programming\\Final Project\\Birdy\\Bird Species Classification\\Training\\model.py:46 in forward, code: cls_token = sequence_output[:, 0]\n",
       "                    slice_10: \"f32[1, 261, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_24, 0, 0, 9223372036854775807)\n",
       "                    select_2: \"f32[1, 768]\" = torch.ops.aten.select.int(slice_10, 1, 0);  slice_10 = None\n",
       "            \n",
       "                     # File: d:\\ITS\\Semester 6\\Mobile Programming\\Final Project\\Birdy\\Bird Species Classification\\Training\\model.py:48 in forward, code: patch_tokens = sequence_output[:, 1 + self.config.num_register_tokens :]  # Exclude register tokens\n",
       "                    slice_11: \"f32[1, 261, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_24, 0, 0, 9223372036854775807);  layer_norm_24 = None\n",
       "                    slice_12: \"f32[1, 256, 768]\" = torch.ops.aten.slice.Tensor(slice_11, 1, 5, 9223372036854775807);  slice_11 = None\n",
       "            \n",
       "                     # File: d:\\ITS\\Semester 6\\Mobile Programming\\Final Project\\Birdy\\Bird Species Classification\\Training\\model.py:51 in forward, code: linear_input = torch.cat([cls_token, patch_tokens.mean(dim=1)], dim=1)\n",
       "                    mean: \"f32[1, 768]\" = torch.ops.aten.mean.dim(slice_12, [1]);  slice_12 = None\n",
       "                    cat_3: \"f32[1, 1536]\" = torch.ops.aten.cat.default([select_2, mean], 1);  select_2 = mean = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_72: \"f32[1, 256]\" = torch.ops.aten.linear.default(cat_3, p_classifier_0_weight, p_classifier_0_bias);  cat_3 = p_classifier_0_weight = p_classifier_0_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_25: \"f32[1, 256]\" = torch.ops.aten.layer_norm.default(linear_72, [256], p_classifier_1_weight, p_classifier_1_bias);  linear_72 = p_classifier_1_weight = p_classifier_1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:432 in forward, code: return F.silu(input, inplace=self.inplace)\n",
       "                    silu: \"f32[1, 256]\" = torch.ops.aten.silu.default(layer_norm_25);  layer_norm_25 = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_61: \"f32[1, 256]\" = torch.ops.aten.clone.default(silu);  silu = None\n",
       "            \n",
       "                     # File: c:\\Users\\Johannes\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_73: \"f32[1, 131]\" = torch.ops.aten.linear.default(clone_61, p_classifier_4_weight, p_classifier_4_bias);  clone_61 = p_classifier_4_weight = p_classifier_4_bias = None\n",
       "                    return (linear_73,)\n",
       "            \n",
       "        Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_embeddings_cls_token'), target='backbone.embeddings.cls_token', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_embeddings_mask_token'), target='backbone.embeddings.mask_token', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_embeddings_register_tokens'), target='backbone.embeddings.register_tokens', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_embeddings_position_embeddings'), target='backbone.embeddings.position_embeddings', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_embeddings_patch_embeddings_projection_weight'), target='backbone.embeddings.patch_embeddings.projection.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_embeddings_patch_embeddings_projection_bias'), target='backbone.embeddings.patch_embeddings.projection.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_norm1_weight'), target='backbone.encoder.layer.0.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_norm1_bias'), target='backbone.encoder.layer.0.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_attention_attention_query_weight'), target='backbone.encoder.layer.0.attention.attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_attention_attention_query_bias'), target='backbone.encoder.layer.0.attention.attention.query.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_attention_attention_key_weight'), target='backbone.encoder.layer.0.attention.attention.key.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_attention_attention_key_bias'), target='backbone.encoder.layer.0.attention.attention.key.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_attention_attention_value_weight'), target='backbone.encoder.layer.0.attention.attention.value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_attention_attention_value_bias'), target='backbone.encoder.layer.0.attention.attention.value.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_attention_output_dense_weight'), target='backbone.encoder.layer.0.attention.output.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_attention_output_dense_bias'), target='backbone.encoder.layer.0.attention.output.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_layer_scale1_lambda1'), target='backbone.encoder.layer.0.layer_scale1.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_norm2_weight'), target='backbone.encoder.layer.0.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_norm2_bias'), target='backbone.encoder.layer.0.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_mlp_fc1_weight'), target='backbone.encoder.layer.0.mlp.fc1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_mlp_fc1_bias'), target='backbone.encoder.layer.0.mlp.fc1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_mlp_fc2_weight'), target='backbone.encoder.layer.0.mlp.fc2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_mlp_fc2_bias'), target='backbone.encoder.layer.0.mlp.fc2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_0_layer_scale2_lambda1'), target='backbone.encoder.layer.0.layer_scale2.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_norm1_weight'), target='backbone.encoder.layer.1.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_norm1_bias'), target='backbone.encoder.layer.1.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_attention_attention_query_weight'), target='backbone.encoder.layer.1.attention.attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_attention_attention_query_bias'), target='backbone.encoder.layer.1.attention.attention.query.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_attention_attention_key_weight'), target='backbone.encoder.layer.1.attention.attention.key.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_attention_attention_key_bias'), target='backbone.encoder.layer.1.attention.attention.key.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_attention_attention_value_weight'), target='backbone.encoder.layer.1.attention.attention.value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_attention_attention_value_bias'), target='backbone.encoder.layer.1.attention.attention.value.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_attention_output_dense_weight'), target='backbone.encoder.layer.1.attention.output.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_attention_output_dense_bias'), target='backbone.encoder.layer.1.attention.output.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_layer_scale1_lambda1'), target='backbone.encoder.layer.1.layer_scale1.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_norm2_weight'), target='backbone.encoder.layer.1.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_norm2_bias'), target='backbone.encoder.layer.1.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_mlp_fc1_weight'), target='backbone.encoder.layer.1.mlp.fc1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_mlp_fc1_bias'), target='backbone.encoder.layer.1.mlp.fc1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_mlp_fc2_weight'), target='backbone.encoder.layer.1.mlp.fc2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_mlp_fc2_bias'), target='backbone.encoder.layer.1.mlp.fc2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_1_layer_scale2_lambda1'), target='backbone.encoder.layer.1.layer_scale2.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_norm1_weight'), target='backbone.encoder.layer.2.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_norm1_bias'), target='backbone.encoder.layer.2.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_attention_attention_query_weight'), target='backbone.encoder.layer.2.attention.attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_attention_attention_query_bias'), target='backbone.encoder.layer.2.attention.attention.query.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_attention_attention_key_weight'), target='backbone.encoder.layer.2.attention.attention.key.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_attention_attention_key_bias'), target='backbone.encoder.layer.2.attention.attention.key.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_attention_attention_value_weight'), target='backbone.encoder.layer.2.attention.attention.value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_attention_attention_value_bias'), target='backbone.encoder.layer.2.attention.attention.value.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_attention_output_dense_weight'), target='backbone.encoder.layer.2.attention.output.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_attention_output_dense_bias'), target='backbone.encoder.layer.2.attention.output.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_layer_scale1_lambda1'), target='backbone.encoder.layer.2.layer_scale1.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_norm2_weight'), target='backbone.encoder.layer.2.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_norm2_bias'), target='backbone.encoder.layer.2.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_mlp_fc1_weight'), target='backbone.encoder.layer.2.mlp.fc1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_mlp_fc1_bias'), target='backbone.encoder.layer.2.mlp.fc1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_mlp_fc2_weight'), target='backbone.encoder.layer.2.mlp.fc2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_mlp_fc2_bias'), target='backbone.encoder.layer.2.mlp.fc2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_2_layer_scale2_lambda1'), target='backbone.encoder.layer.2.layer_scale2.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_norm1_weight'), target='backbone.encoder.layer.3.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_norm1_bias'), target='backbone.encoder.layer.3.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_attention_attention_query_weight'), target='backbone.encoder.layer.3.attention.attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_attention_attention_query_bias'), target='backbone.encoder.layer.3.attention.attention.query.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_attention_attention_key_weight'), target='backbone.encoder.layer.3.attention.attention.key.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_attention_attention_key_bias'), target='backbone.encoder.layer.3.attention.attention.key.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_attention_attention_value_weight'), target='backbone.encoder.layer.3.attention.attention.value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_attention_attention_value_bias'), target='backbone.encoder.layer.3.attention.attention.value.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_attention_output_dense_weight'), target='backbone.encoder.layer.3.attention.output.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_attention_output_dense_bias'), target='backbone.encoder.layer.3.attention.output.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_layer_scale1_lambda1'), target='backbone.encoder.layer.3.layer_scale1.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_norm2_weight'), target='backbone.encoder.layer.3.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_norm2_bias'), target='backbone.encoder.layer.3.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_mlp_fc1_weight'), target='backbone.encoder.layer.3.mlp.fc1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_mlp_fc1_bias'), target='backbone.encoder.layer.3.mlp.fc1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_mlp_fc2_weight'), target='backbone.encoder.layer.3.mlp.fc2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_mlp_fc2_bias'), target='backbone.encoder.layer.3.mlp.fc2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_3_layer_scale2_lambda1'), target='backbone.encoder.layer.3.layer_scale2.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_norm1_weight'), target='backbone.encoder.layer.4.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_norm1_bias'), target='backbone.encoder.layer.4.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_attention_attention_query_weight'), target='backbone.encoder.layer.4.attention.attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_attention_attention_query_bias'), target='backbone.encoder.layer.4.attention.attention.query.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_attention_attention_key_weight'), target='backbone.encoder.layer.4.attention.attention.key.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_attention_attention_key_bias'), target='backbone.encoder.layer.4.attention.attention.key.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_attention_attention_value_weight'), target='backbone.encoder.layer.4.attention.attention.value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_attention_attention_value_bias'), target='backbone.encoder.layer.4.attention.attention.value.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_attention_output_dense_weight'), target='backbone.encoder.layer.4.attention.output.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_attention_output_dense_bias'), target='backbone.encoder.layer.4.attention.output.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_layer_scale1_lambda1'), target='backbone.encoder.layer.4.layer_scale1.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_norm2_weight'), target='backbone.encoder.layer.4.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_norm2_bias'), target='backbone.encoder.layer.4.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_mlp_fc1_weight'), target='backbone.encoder.layer.4.mlp.fc1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_mlp_fc1_bias'), target='backbone.encoder.layer.4.mlp.fc1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_mlp_fc2_weight'), target='backbone.encoder.layer.4.mlp.fc2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_mlp_fc2_bias'), target='backbone.encoder.layer.4.mlp.fc2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_4_layer_scale2_lambda1'), target='backbone.encoder.layer.4.layer_scale2.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_norm1_weight'), target='backbone.encoder.layer.5.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_norm1_bias'), target='backbone.encoder.layer.5.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_attention_attention_query_weight'), target='backbone.encoder.layer.5.attention.attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_attention_attention_query_bias'), target='backbone.encoder.layer.5.attention.attention.query.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_attention_attention_key_weight'), target='backbone.encoder.layer.5.attention.attention.key.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_attention_attention_key_bias'), target='backbone.encoder.layer.5.attention.attention.key.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_attention_attention_value_weight'), target='backbone.encoder.layer.5.attention.attention.value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_attention_attention_value_bias'), target='backbone.encoder.layer.5.attention.attention.value.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_attention_output_dense_weight'), target='backbone.encoder.layer.5.attention.output.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_attention_output_dense_bias'), target='backbone.encoder.layer.5.attention.output.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_layer_scale1_lambda1'), target='backbone.encoder.layer.5.layer_scale1.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_norm2_weight'), target='backbone.encoder.layer.5.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_norm2_bias'), target='backbone.encoder.layer.5.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_mlp_fc1_weight'), target='backbone.encoder.layer.5.mlp.fc1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_mlp_fc1_bias'), target='backbone.encoder.layer.5.mlp.fc1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_mlp_fc2_weight'), target='backbone.encoder.layer.5.mlp.fc2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_mlp_fc2_bias'), target='backbone.encoder.layer.5.mlp.fc2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_5_layer_scale2_lambda1'), target='backbone.encoder.layer.5.layer_scale2.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_norm1_weight'), target='backbone.encoder.layer.6.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_norm1_bias'), target='backbone.encoder.layer.6.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_attention_attention_query_weight'), target='backbone.encoder.layer.6.attention.attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_attention_attention_query_bias'), target='backbone.encoder.layer.6.attention.attention.query.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_attention_attention_key_weight'), target='backbone.encoder.layer.6.attention.attention.key.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_attention_attention_key_bias'), target='backbone.encoder.layer.6.attention.attention.key.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_attention_attention_value_weight'), target='backbone.encoder.layer.6.attention.attention.value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_attention_attention_value_bias'), target='backbone.encoder.layer.6.attention.attention.value.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_attention_output_dense_weight'), target='backbone.encoder.layer.6.attention.output.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_attention_output_dense_bias'), target='backbone.encoder.layer.6.attention.output.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_layer_scale1_lambda1'), target='backbone.encoder.layer.6.layer_scale1.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_norm2_weight'), target='backbone.encoder.layer.6.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_norm2_bias'), target='backbone.encoder.layer.6.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_mlp_fc1_weight'), target='backbone.encoder.layer.6.mlp.fc1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_mlp_fc1_bias'), target='backbone.encoder.layer.6.mlp.fc1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_mlp_fc2_weight'), target='backbone.encoder.layer.6.mlp.fc2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_mlp_fc2_bias'), target='backbone.encoder.layer.6.mlp.fc2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_6_layer_scale2_lambda1'), target='backbone.encoder.layer.6.layer_scale2.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_norm1_weight'), target='backbone.encoder.layer.7.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_norm1_bias'), target='backbone.encoder.layer.7.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_attention_attention_query_weight'), target='backbone.encoder.layer.7.attention.attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_attention_attention_query_bias'), target='backbone.encoder.layer.7.attention.attention.query.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_attention_attention_key_weight'), target='backbone.encoder.layer.7.attention.attention.key.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_attention_attention_key_bias'), target='backbone.encoder.layer.7.attention.attention.key.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_attention_attention_value_weight'), target='backbone.encoder.layer.7.attention.attention.value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_attention_attention_value_bias'), target='backbone.encoder.layer.7.attention.attention.value.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_attention_output_dense_weight'), target='backbone.encoder.layer.7.attention.output.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_attention_output_dense_bias'), target='backbone.encoder.layer.7.attention.output.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_layer_scale1_lambda1'), target='backbone.encoder.layer.7.layer_scale1.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_norm2_weight'), target='backbone.encoder.layer.7.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_norm2_bias'), target='backbone.encoder.layer.7.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_mlp_fc1_weight'), target='backbone.encoder.layer.7.mlp.fc1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_mlp_fc1_bias'), target='backbone.encoder.layer.7.mlp.fc1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_mlp_fc2_weight'), target='backbone.encoder.layer.7.mlp.fc2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_mlp_fc2_bias'), target='backbone.encoder.layer.7.mlp.fc2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_7_layer_scale2_lambda1'), target='backbone.encoder.layer.7.layer_scale2.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_norm1_weight'), target='backbone.encoder.layer.8.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_norm1_bias'), target='backbone.encoder.layer.8.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_attention_attention_query_weight'), target='backbone.encoder.layer.8.attention.attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_attention_attention_query_bias'), target='backbone.encoder.layer.8.attention.attention.query.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_attention_attention_key_weight'), target='backbone.encoder.layer.8.attention.attention.key.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_attention_attention_key_bias'), target='backbone.encoder.layer.8.attention.attention.key.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_attention_attention_value_weight'), target='backbone.encoder.layer.8.attention.attention.value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_attention_attention_value_bias'), target='backbone.encoder.layer.8.attention.attention.value.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_attention_output_dense_weight'), target='backbone.encoder.layer.8.attention.output.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_attention_output_dense_bias'), target='backbone.encoder.layer.8.attention.output.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_layer_scale1_lambda1'), target='backbone.encoder.layer.8.layer_scale1.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_norm2_weight'), target='backbone.encoder.layer.8.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_norm2_bias'), target='backbone.encoder.layer.8.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_mlp_fc1_weight'), target='backbone.encoder.layer.8.mlp.fc1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_mlp_fc1_bias'), target='backbone.encoder.layer.8.mlp.fc1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_mlp_fc2_weight'), target='backbone.encoder.layer.8.mlp.fc2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_mlp_fc2_bias'), target='backbone.encoder.layer.8.mlp.fc2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_8_layer_scale2_lambda1'), target='backbone.encoder.layer.8.layer_scale2.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_norm1_weight'), target='backbone.encoder.layer.9.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_norm1_bias'), target='backbone.encoder.layer.9.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_attention_attention_query_weight'), target='backbone.encoder.layer.9.attention.attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_attention_attention_query_bias'), target='backbone.encoder.layer.9.attention.attention.query.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_attention_attention_key_weight'), target='backbone.encoder.layer.9.attention.attention.key.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_attention_attention_key_bias'), target='backbone.encoder.layer.9.attention.attention.key.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_attention_attention_value_weight'), target='backbone.encoder.layer.9.attention.attention.value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_attention_attention_value_bias'), target='backbone.encoder.layer.9.attention.attention.value.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_attention_output_dense_weight'), target='backbone.encoder.layer.9.attention.output.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_attention_output_dense_bias'), target='backbone.encoder.layer.9.attention.output.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_layer_scale1_lambda1'), target='backbone.encoder.layer.9.layer_scale1.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_norm2_weight'), target='backbone.encoder.layer.9.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_norm2_bias'), target='backbone.encoder.layer.9.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_mlp_fc1_weight'), target='backbone.encoder.layer.9.mlp.fc1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_mlp_fc1_bias'), target='backbone.encoder.layer.9.mlp.fc1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_mlp_fc2_weight'), target='backbone.encoder.layer.9.mlp.fc2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_mlp_fc2_bias'), target='backbone.encoder.layer.9.mlp.fc2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_9_layer_scale2_lambda1'), target='backbone.encoder.layer.9.layer_scale2.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_norm1_weight'), target='backbone.encoder.layer.10.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_norm1_bias'), target='backbone.encoder.layer.10.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_attention_attention_query_weight'), target='backbone.encoder.layer.10.attention.attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_attention_attention_query_bias'), target='backbone.encoder.layer.10.attention.attention.query.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_attention_attention_key_weight'), target='backbone.encoder.layer.10.attention.attention.key.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_attention_attention_key_bias'), target='backbone.encoder.layer.10.attention.attention.key.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_attention_attention_value_weight'), target='backbone.encoder.layer.10.attention.attention.value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_attention_attention_value_bias'), target='backbone.encoder.layer.10.attention.attention.value.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_attention_output_dense_weight'), target='backbone.encoder.layer.10.attention.output.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_attention_output_dense_bias'), target='backbone.encoder.layer.10.attention.output.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_layer_scale1_lambda1'), target='backbone.encoder.layer.10.layer_scale1.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_norm2_weight'), target='backbone.encoder.layer.10.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_norm2_bias'), target='backbone.encoder.layer.10.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_mlp_fc1_weight'), target='backbone.encoder.layer.10.mlp.fc1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_mlp_fc1_bias'), target='backbone.encoder.layer.10.mlp.fc1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_mlp_fc2_weight'), target='backbone.encoder.layer.10.mlp.fc2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_mlp_fc2_bias'), target='backbone.encoder.layer.10.mlp.fc2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_10_layer_scale2_lambda1'), target='backbone.encoder.layer.10.layer_scale2.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_norm1_weight'), target='backbone.encoder.layer.11.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_norm1_bias'), target='backbone.encoder.layer.11.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_attention_attention_query_weight'), target='backbone.encoder.layer.11.attention.attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_attention_attention_query_bias'), target='backbone.encoder.layer.11.attention.attention.query.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_attention_attention_key_weight'), target='backbone.encoder.layer.11.attention.attention.key.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_attention_attention_key_bias'), target='backbone.encoder.layer.11.attention.attention.key.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_attention_attention_value_weight'), target='backbone.encoder.layer.11.attention.attention.value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_attention_attention_value_bias'), target='backbone.encoder.layer.11.attention.attention.value.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_attention_output_dense_weight'), target='backbone.encoder.layer.11.attention.output.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_attention_output_dense_bias'), target='backbone.encoder.layer.11.attention.output.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_layer_scale1_lambda1'), target='backbone.encoder.layer.11.layer_scale1.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_norm2_weight'), target='backbone.encoder.layer.11.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_norm2_bias'), target='backbone.encoder.layer.11.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_mlp_fc1_weight'), target='backbone.encoder.layer.11.mlp.fc1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_mlp_fc1_bias'), target='backbone.encoder.layer.11.mlp.fc1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_mlp_fc2_weight'), target='backbone.encoder.layer.11.mlp.fc2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_mlp_fc2_bias'), target='backbone.encoder.layer.11.mlp.fc2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_encoder_layer_11_layer_scale2_lambda1'), target='backbone.encoder.layer.11.layer_scale2.lambda1', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_layernorm_weight'), target='backbone.layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_backbone_layernorm_bias'), target='backbone.layernorm.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_classifier_0_weight'), target='classifier.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_classifier_0_bias'), target='classifier.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_classifier_1_weight'), target='classifier.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_classifier_1_bias'), target='classifier.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_classifier_4_weight'), target='classifier.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_classifier_4_bias'), target='classifier.4.bias', persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='pixel_values'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='linear_73'), target=None)])\n",
       "        Range constraints: {}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import cast\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, Dinov2WithRegistersConfig\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from model import CustomDinoV2ClassifierWithReg\n",
    "from config import BASE_MODEL_NAME, NUM_CLASSES , HIDDEN_DIM\n",
    "\n",
    "checkpoint_path = \"./configs/\"\n",
    "\n",
    "# Load processor and fine-tuned DinoV2\n",
    "processor = AutoImageProcessor.from_pretrained(BASE_MODEL_NAME, cache_dir=\"./cache\", use_fast=True)\n",
    "config = cast(Dinov2WithRegistersConfig, Dinov2WithRegistersConfig.from_pretrained(checkpoint_path, cache_dir=\"./cache\"))\n",
    "model = CustomDinoV2ClassifierWithReg.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    config=config,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    cache_dir=\"./cache\"\n",
    ")\n",
    "\n",
    "# If our model is only weight (Not exported with base model, we should load the weight)\n",
    "'''\n",
    "# Load weights from checkpoint\n",
    "checkpoint_path = \"./backup/results/checkpoint-602/model.safetensors\"\n",
    "state_dict = load_file(checkpoint_path)\n",
    "model.load_state_dict(state_dict)\n",
    "'''\n",
    "\n",
    "# Set model to eval mode and move to GPU if available\n",
    "model.eval()\n",
    "model.to('cpu') # type: ignore\n",
    "\n",
    "# Test an image\n",
    "image = Image.open(r\".\\Dataset\\Acridotheres javanicus\\Javan Myna_Acridotheres javanicus_1.jpg\").convert(\"RGB\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to('cpu')['pixel_values']  # type: ignore\n",
    "print(f\"{inputs.shape, inputs.dtype}\")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,                             # model\n",
    "    (inputs, ),                         # input tuple\n",
    "    \"./quant/basefp32.onnx\",                     # output path\n",
    "    export_params=True,\n",
    "    opset_version=20,\n",
    "    input_names=[\"pixel_values\"],             # <- match the actual argument name\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_shapes={\n",
    "        \"pixel_values\": {0: \"batch_size\"}, # Only input is allowed here\n",
    "    },\n",
    "    dynamo=True,\n",
    "    optimize=True,\n",
    "    report=False,\n",
    "    do_constant_folding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04eba205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1: Conv - inputs: ['pixel_values', 'backbone.embeddings.patch_embeddings.projection.weight', 'backbone.embeddings.patch_embeddings.projection.bias'] - outputs: ['conv2d']\n",
      "#2: Transpose - inputs: ['backbone.encoder.layer.0.attention.attention.key.weight'] - outputs: ['val_91']\n",
      "#3: Transpose - inputs: ['backbone.encoder.layer.0.attention.attention.value.weight'] - outputs: ['val_95']\n",
      "#4: Transpose - inputs: ['backbone.encoder.layer.0.attention.attention.query.weight'] - outputs: ['val_98']\n",
      "#5: Transpose - inputs: ['backbone.encoder.layer.0.attention.output.dense.weight'] - outputs: ['val_129']\n",
      "#6: Transpose - inputs: ['backbone.encoder.layer.0.mlp.fc1.weight'] - outputs: ['val_133']\n",
      "#7: Transpose - inputs: ['backbone.encoder.layer.0.mlp.fc2.weight'] - outputs: ['val_142']\n",
      "#8: Transpose - inputs: ['backbone.encoder.layer.1.attention.attention.key.weight'] - outputs: ['val_146']\n",
      "#9: Transpose - inputs: ['backbone.encoder.layer.1.attention.attention.value.weight'] - outputs: ['val_149']\n",
      "#10: Transpose - inputs: ['backbone.encoder.layer.1.attention.attention.query.weight'] - outputs: ['val_152']\n",
      "#11: Transpose - inputs: ['backbone.encoder.layer.1.attention.output.dense.weight'] - outputs: ['val_178']\n",
      "#12: Transpose - inputs: ['backbone.encoder.layer.1.mlp.fc1.weight'] - outputs: ['val_182']\n",
      "#13: Transpose - inputs: ['backbone.encoder.layer.1.mlp.fc2.weight'] - outputs: ['val_191']\n",
      "#14: Transpose - inputs: ['backbone.encoder.layer.2.attention.attention.key.weight'] - outputs: ['val_195']\n",
      "#15: Transpose - inputs: ['backbone.encoder.layer.2.attention.attention.value.weight'] - outputs: ['val_198']\n",
      "#16: Transpose - inputs: ['backbone.encoder.layer.2.attention.attention.query.weight'] - outputs: ['val_201']\n",
      "#17: Transpose - inputs: ['backbone.encoder.layer.2.attention.output.dense.weight'] - outputs: ['val_227']\n",
      "#18: Transpose - inputs: ['backbone.encoder.layer.2.mlp.fc1.weight'] - outputs: ['val_231']\n",
      "#19: Transpose - inputs: ['backbone.encoder.layer.2.mlp.fc2.weight'] - outputs: ['val_240']\n",
      "#20: Transpose - inputs: ['backbone.encoder.layer.3.attention.attention.key.weight'] - outputs: ['val_244']\n",
      "#21: Transpose - inputs: ['backbone.encoder.layer.3.attention.attention.value.weight'] - outputs: ['val_247']\n",
      "#22: Transpose - inputs: ['backbone.encoder.layer.3.attention.attention.query.weight'] - outputs: ['val_250']\n",
      "#23: Transpose - inputs: ['backbone.encoder.layer.3.attention.output.dense.weight'] - outputs: ['val_276']\n",
      "#24: Transpose - inputs: ['backbone.encoder.layer.3.mlp.fc1.weight'] - outputs: ['val_280']\n",
      "#25: Transpose - inputs: ['backbone.encoder.layer.3.mlp.fc2.weight'] - outputs: ['val_289']\n",
      "#26: Transpose - inputs: ['backbone.encoder.layer.4.attention.attention.key.weight'] - outputs: ['val_293']\n",
      "#27: Transpose - inputs: ['backbone.encoder.layer.4.attention.attention.value.weight'] - outputs: ['val_296']\n",
      "#28: Transpose - inputs: ['backbone.encoder.layer.4.attention.attention.query.weight'] - outputs: ['val_299']\n",
      "#29: Transpose - inputs: ['backbone.encoder.layer.4.attention.output.dense.weight'] - outputs: ['val_325']\n",
      "#30: Transpose - inputs: ['backbone.encoder.layer.4.mlp.fc1.weight'] - outputs: ['val_329']\n",
      "#31: Transpose - inputs: ['backbone.encoder.layer.4.mlp.fc2.weight'] - outputs: ['val_338']\n",
      "#32: Transpose - inputs: ['backbone.encoder.layer.5.attention.attention.key.weight'] - outputs: ['val_342']\n",
      "#33: Transpose - inputs: ['backbone.encoder.layer.5.attention.attention.value.weight'] - outputs: ['val_345']\n",
      "#34: Transpose - inputs: ['backbone.encoder.layer.5.attention.attention.query.weight'] - outputs: ['val_348']\n",
      "#35: Transpose - inputs: ['backbone.encoder.layer.5.attention.output.dense.weight'] - outputs: ['val_374']\n",
      "#36: Transpose - inputs: ['backbone.encoder.layer.5.mlp.fc1.weight'] - outputs: ['val_378']\n",
      "#37: Transpose - inputs: ['backbone.encoder.layer.5.mlp.fc2.weight'] - outputs: ['val_387']\n",
      "#38: Transpose - inputs: ['backbone.encoder.layer.6.attention.attention.key.weight'] - outputs: ['val_391']\n",
      "#39: Transpose - inputs: ['backbone.encoder.layer.6.attention.attention.value.weight'] - outputs: ['val_394']\n",
      "#40: Transpose - inputs: ['backbone.encoder.layer.6.attention.attention.query.weight'] - outputs: ['val_397']\n",
      "#41: Transpose - inputs: ['backbone.encoder.layer.6.attention.output.dense.weight'] - outputs: ['val_423']\n",
      "#42: Transpose - inputs: ['backbone.encoder.layer.6.mlp.fc1.weight'] - outputs: ['val_427']\n",
      "#43: Transpose - inputs: ['backbone.encoder.layer.6.mlp.fc2.weight'] - outputs: ['val_436']\n",
      "#44: Transpose - inputs: ['backbone.encoder.layer.7.attention.attention.key.weight'] - outputs: ['val_440']\n",
      "#45: Transpose - inputs: ['backbone.encoder.layer.7.attention.attention.value.weight'] - outputs: ['val_443']\n",
      "#46: Transpose - inputs: ['backbone.encoder.layer.7.attention.attention.query.weight'] - outputs: ['val_446']\n",
      "#47: Transpose - inputs: ['backbone.encoder.layer.7.attention.output.dense.weight'] - outputs: ['val_472']\n",
      "#48: Transpose - inputs: ['backbone.encoder.layer.7.mlp.fc1.weight'] - outputs: ['val_476']\n",
      "#49: Transpose - inputs: ['backbone.encoder.layer.7.mlp.fc2.weight'] - outputs: ['val_485']\n",
      "#50: Transpose - inputs: ['backbone.encoder.layer.8.attention.attention.key.weight'] - outputs: ['val_489']\n",
      "#51: Transpose - inputs: ['backbone.encoder.layer.8.attention.attention.value.weight'] - outputs: ['val_492']\n",
      "#52: Transpose - inputs: ['backbone.encoder.layer.8.attention.attention.query.weight'] - outputs: ['val_495']\n",
      "#53: Transpose - inputs: ['backbone.encoder.layer.8.attention.output.dense.weight'] - outputs: ['val_521']\n",
      "#54: Transpose - inputs: ['backbone.encoder.layer.8.mlp.fc1.weight'] - outputs: ['val_525']\n",
      "#55: Transpose - inputs: ['backbone.encoder.layer.8.mlp.fc2.weight'] - outputs: ['val_534']\n",
      "#56: Transpose - inputs: ['backbone.encoder.layer.9.attention.attention.key.weight'] - outputs: ['val_538']\n",
      "#57: Transpose - inputs: ['backbone.encoder.layer.9.attention.attention.value.weight'] - outputs: ['val_541']\n",
      "#58: Transpose - inputs: ['backbone.encoder.layer.9.attention.attention.query.weight'] - outputs: ['val_544']\n",
      "#59: Transpose - inputs: ['backbone.encoder.layer.9.attention.output.dense.weight'] - outputs: ['val_570']\n",
      "#60: Transpose - inputs: ['backbone.encoder.layer.9.mlp.fc1.weight'] - outputs: ['val_574']\n",
      "#61: Transpose - inputs: ['backbone.encoder.layer.9.mlp.fc2.weight'] - outputs: ['val_583']\n",
      "#62: Transpose - inputs: ['backbone.encoder.layer.10.attention.attention.key.weight'] - outputs: ['val_587']\n",
      "#63: Transpose - inputs: ['backbone.encoder.layer.10.attention.attention.value.weight'] - outputs: ['val_590']\n",
      "#64: Transpose - inputs: ['backbone.encoder.layer.10.attention.attention.query.weight'] - outputs: ['val_593']\n",
      "#65: Transpose - inputs: ['backbone.encoder.layer.10.attention.output.dense.weight'] - outputs: ['val_619']\n",
      "#66: Transpose - inputs: ['backbone.encoder.layer.10.mlp.fc1.weight'] - outputs: ['val_623']\n",
      "#67: Transpose - inputs: ['backbone.encoder.layer.10.mlp.fc2.weight'] - outputs: ['val_632']\n",
      "#68: Transpose - inputs: ['backbone.encoder.layer.11.attention.attention.key.weight'] - outputs: ['val_636']\n",
      "#69: Transpose - inputs: ['backbone.encoder.layer.11.attention.attention.value.weight'] - outputs: ['val_639']\n",
      "#70: Transpose - inputs: ['backbone.encoder.layer.11.attention.attention.query.weight'] - outputs: ['val_642']\n",
      "#71: Transpose - inputs: ['backbone.encoder.layer.11.attention.output.dense.weight'] - outputs: ['val_668']\n",
      "#72: Transpose - inputs: ['backbone.encoder.layer.11.mlp.fc1.weight'] - outputs: ['val_672']\n",
      "#73: Transpose - inputs: ['backbone.encoder.layer.11.mlp.fc2.weight'] - outputs: ['val_681']\n",
      "#74: Expand - inputs: ['backbone.embeddings.cls_token', 'val_4'] - outputs: ['expand']\n",
      "#75: Gather - inputs: ['backbone.embeddings.position_embeddings', 'val_5'] - outputs: ['select']\n",
      "#76: Slice - inputs: ['backbone.embeddings.position_embeddings', 'val_30', 'val_33', 'val_30', 'val_30'] - outputs: ['slice_3']\n",
      "#77: Expand - inputs: ['backbone.embeddings.register_tokens', 'val_4'] - outputs: ['expand_1']\n",
      "#78: Reshape - inputs: ['conv2d', 'val_1'] - outputs: ['view']\n",
      "#79: Reshape - inputs: ['slice_3', 'val_39'] - outputs: ['view_1']\n",
      "#80: Unsqueeze - inputs: ['select', 'val_46'] - outputs: ['unsqueeze']\n",
      "#81: Transpose - inputs: ['view'] - outputs: ['transpose']\n",
      "#82: Transpose - inputs: ['view_1'] - outputs: ['permute']\n",
      "#83: Concat - inputs: ['expand', 'transpose'] - outputs: ['cat']\n",
      "#84: Resize - inputs: ['permute', '', '', 'val_43'] - outputs: ['upsample_bicubic2d']\n",
      "#85: Transpose - inputs: ['upsample_bicubic2d'] - outputs: ['permute_1']\n",
      "#86: Reshape - inputs: ['permute_1', 'val_45'] - outputs: ['view_2']\n",
      "#87: Concat - inputs: ['unsqueeze', 'view_2'] - outputs: ['cat_1']\n",
      "#88: Add - inputs: ['cat', 'cat_1'] - outputs: ['add']\n",
      "#89: Slice - inputs: ['add', 'val_46', 'val_30', 'val_30', 'val_30'] - outputs: ['slice_5']\n",
      "#90: Slice - inputs: ['add', 'val_30', 'val_33', 'val_30', 'val_30'] - outputs: ['slice_7']\n",
      "#91: Concat - inputs: ['slice_5', 'expand_1', 'slice_7'] - outputs: ['cat_2']\n",
      "#92: LayerNormalization - inputs: ['cat_2', 'backbone.encoder.layer.0.norm1.weight', 'backbone.encoder.layer.0.norm1.bias'] - outputs: ['layer_norm']\n",
      "#93: MatMul - inputs: ['layer_norm', 'val_91'] - outputs: ['val_92']\n",
      "#94: MatMul - inputs: ['layer_norm', 'val_95'] - outputs: ['val_96']\n",
      "#95: MatMul - inputs: ['layer_norm', 'val_98'] - outputs: ['val_99']\n",
      "#96: Add - inputs: ['val_92', 'backbone.encoder.layer.0.attention.attention.key.bias'] - outputs: ['linear']\n",
      "#97: Add - inputs: ['val_96', 'backbone.encoder.layer.0.attention.attention.value.bias'] - outputs: ['linear_1']\n",
      "#98: Add - inputs: ['val_99', 'backbone.encoder.layer.0.attention.attention.query.bias'] - outputs: ['linear_2']\n",
      "#99: Reshape - inputs: ['linear', 'val_94'] - outputs: ['view_3']\n",
      "#100: Reshape - inputs: ['linear_1', 'val_94'] - outputs: ['view_4']\n",
      "#101: Reshape - inputs: ['linear_2', 'val_94'] - outputs: ['view_5']\n",
      "#102: Transpose - inputs: ['view_3'] - outputs: ['permute_2']\n",
      "#103: Transpose - inputs: ['view_4'] - outputs: ['permute_3']\n",
      "#104: Transpose - inputs: ['view_5'] - outputs: ['permute_4']\n",
      "#105: Reshape - inputs: ['permute_2', 'val_112'] - outputs: ['val_113']\n",
      "#106: Mul - inputs: ['permute_4', 'val_117'] - outputs: ['val_118']\n",
      "#107: Transpose - inputs: ['val_113'] - outputs: ['val_114']\n",
      "#108: Reshape - inputs: ['val_114', 'val_115'] - outputs: ['val_116']\n",
      "#109: Mul - inputs: ['val_116', 'val_117'] - outputs: ['val_121']\n",
      "#110: MatMul - inputs: ['val_118', 'val_121'] - outputs: ['val_122']\n",
      "#111: Softmax - inputs: ['val_122'] - outputs: ['val_123']\n",
      "#112: MatMul - inputs: ['val_123', 'permute_3'] - outputs: ['scaled_dot_product_attention']\n",
      "#113: Transpose - inputs: ['scaled_dot_product_attention'] - outputs: ['transpose_1']\n",
      "#114: Reshape - inputs: ['transpose_1', 'val_128'] - outputs: ['view_6']\n",
      "#115: MatMul - inputs: ['view_6', 'val_129'] - outputs: ['val_130']\n",
      "#116: Add - inputs: ['val_130', 'backbone.encoder.layer.0.attention.output.dense.bias'] - outputs: ['linear_3']\n",
      "#117: Mul - inputs: ['linear_3', 'backbone.encoder.layer.0.layer_scale1.lambda1'] - outputs: ['mul']\n",
      "#118: Add - inputs: ['mul', 'cat_2'] - outputs: ['add_1']\n",
      "#119: LayerNormalization - inputs: ['add_1', 'backbone.encoder.layer.0.norm2.weight', 'backbone.encoder.layer.0.norm2.bias'] - outputs: ['layer_norm_1']\n",
      "#120: MatMul - inputs: ['layer_norm_1', 'val_133'] - outputs: ['val_134']\n",
      "#121: Add - inputs: ['val_134', 'backbone.encoder.layer.0.mlp.fc1.bias'] - outputs: ['linear_4']\n",
      "#122: Div - inputs: ['linear_4', 'val_135'] - outputs: ['val_136']\n",
      "#123: Erf - inputs: ['val_136'] - outputs: ['val_137']\n",
      "#124: Add - inputs: ['val_137', 'val_138'] - outputs: ['val_139']\n",
      "#125: Mul - inputs: ['val_140', 'val_139'] - outputs: ['val_141']\n",
      "#126: Mul - inputs: ['linear_4', 'val_141'] - outputs: ['gelu']\n",
      "#127: MatMul - inputs: ['gelu', 'val_142'] - outputs: ['val_143']\n",
      "#128: Add - inputs: ['val_143', 'backbone.encoder.layer.0.mlp.fc2.bias'] - outputs: ['linear_5']\n",
      "#129: Mul - inputs: ['linear_5', 'backbone.encoder.layer.0.layer_scale2.lambda1'] - outputs: ['mul_1']\n",
      "#130: Add - inputs: ['mul_1', 'add_1'] - outputs: ['add_2']\n",
      "#131: LayerNormalization - inputs: ['add_2', 'backbone.encoder.layer.1.norm1.weight', 'backbone.encoder.layer.1.norm1.bias'] - outputs: ['layer_norm_2']\n",
      "#132: MatMul - inputs: ['layer_norm_2', 'val_146'] - outputs: ['val_147']\n",
      "#133: MatMul - inputs: ['layer_norm_2', 'val_149'] - outputs: ['val_150']\n",
      "#134: MatMul - inputs: ['layer_norm_2', 'val_152'] - outputs: ['val_153']\n",
      "#135: Add - inputs: ['val_147', 'backbone.encoder.layer.1.attention.attention.key.bias'] - outputs: ['linear_6']\n",
      "#136: Add - inputs: ['val_150', 'backbone.encoder.layer.1.attention.attention.value.bias'] - outputs: ['linear_7']\n",
      "#137: Add - inputs: ['val_153', 'backbone.encoder.layer.1.attention.attention.query.bias'] - outputs: ['linear_8']\n",
      "#138: Reshape - inputs: ['linear_6', 'val_94'] - outputs: ['view_7']\n",
      "#139: Reshape - inputs: ['linear_7', 'val_94'] - outputs: ['view_8']\n",
      "#140: Reshape - inputs: ['linear_8', 'val_94'] - outputs: ['view_9']\n",
      "#141: Transpose - inputs: ['view_7'] - outputs: ['permute_5']\n",
      "#142: Transpose - inputs: ['view_8'] - outputs: ['permute_6']\n",
      "#143: Transpose - inputs: ['view_9'] - outputs: ['permute_7']\n",
      "#144: Reshape - inputs: ['permute_5', 'val_112'] - outputs: ['val_164']\n",
      "#145: Mul - inputs: ['permute_7', 'val_117'] - outputs: ['val_169']\n",
      "#146: Transpose - inputs: ['val_164'] - outputs: ['val_165']\n",
      "#147: Reshape - inputs: ['val_165', 'val_115'] - outputs: ['val_167']\n",
      "#148: Mul - inputs: ['val_167', 'val_117'] - outputs: ['val_172']\n",
      "#149: MatMul - inputs: ['val_169', 'val_172'] - outputs: ['val_173']\n",
      "#150: Softmax - inputs: ['val_173'] - outputs: ['val_174']\n",
      "#151: MatMul - inputs: ['val_174', 'permute_6'] - outputs: ['scaled_dot_product_attention_1']\n",
      "#152: Transpose - inputs: ['scaled_dot_product_attention_1'] - outputs: ['transpose_2']\n",
      "#153: Reshape - inputs: ['transpose_2', 'val_128'] - outputs: ['view_10']\n",
      "#154: MatMul - inputs: ['view_10', 'val_178'] - outputs: ['val_179']\n",
      "#155: Add - inputs: ['val_179', 'backbone.encoder.layer.1.attention.output.dense.bias'] - outputs: ['linear_9']\n",
      "#156: Mul - inputs: ['linear_9', 'backbone.encoder.layer.1.layer_scale1.lambda1'] - outputs: ['mul_2']\n",
      "#157: Add - inputs: ['mul_2', 'add_2'] - outputs: ['add_3']\n",
      "#158: LayerNormalization - inputs: ['add_3', 'backbone.encoder.layer.1.norm2.weight', 'backbone.encoder.layer.1.norm2.bias'] - outputs: ['layer_norm_3']\n",
      "#159: MatMul - inputs: ['layer_norm_3', 'val_182'] - outputs: ['val_183']\n",
      "#160: Add - inputs: ['val_183', 'backbone.encoder.layer.1.mlp.fc1.bias'] - outputs: ['linear_10']\n",
      "#161: Div - inputs: ['linear_10', 'val_135'] - outputs: ['val_185']\n",
      "#162: Erf - inputs: ['val_185'] - outputs: ['val_186']\n",
      "#163: Add - inputs: ['val_186', 'val_138'] - outputs: ['val_188']\n",
      "#164: Mul - inputs: ['val_140', 'val_188'] - outputs: ['val_190']\n",
      "#165: Mul - inputs: ['linear_10', 'val_190'] - outputs: ['gelu_1']\n",
      "#166: MatMul - inputs: ['gelu_1', 'val_191'] - outputs: ['val_192']\n",
      "#167: Add - inputs: ['val_192', 'backbone.encoder.layer.1.mlp.fc2.bias'] - outputs: ['linear_11']\n",
      "#168: Mul - inputs: ['linear_11', 'backbone.encoder.layer.1.layer_scale2.lambda1'] - outputs: ['mul_3']\n",
      "#169: Add - inputs: ['mul_3', 'add_3'] - outputs: ['add_4']\n",
      "#170: LayerNormalization - inputs: ['add_4', 'backbone.encoder.layer.2.norm1.weight', 'backbone.encoder.layer.2.norm1.bias'] - outputs: ['layer_norm_4']\n",
      "#171: MatMul - inputs: ['layer_norm_4', 'val_195'] - outputs: ['val_196']\n",
      "#172: MatMul - inputs: ['layer_norm_4', 'val_198'] - outputs: ['val_199']\n",
      "#173: MatMul - inputs: ['layer_norm_4', 'val_201'] - outputs: ['val_202']\n",
      "#174: Add - inputs: ['val_196', 'backbone.encoder.layer.2.attention.attention.key.bias'] - outputs: ['linear_12']\n",
      "#175: Add - inputs: ['val_199', 'backbone.encoder.layer.2.attention.attention.value.bias'] - outputs: ['linear_13']\n",
      "#176: Add - inputs: ['val_202', 'backbone.encoder.layer.2.attention.attention.query.bias'] - outputs: ['linear_14']\n",
      "#177: Reshape - inputs: ['linear_12', 'val_94'] - outputs: ['view_11']\n",
      "#178: Reshape - inputs: ['linear_13', 'val_94'] - outputs: ['view_12']\n",
      "#179: Reshape - inputs: ['linear_14', 'val_94'] - outputs: ['view_13']\n",
      "#180: Transpose - inputs: ['view_11'] - outputs: ['permute_8']\n",
      "#181: Transpose - inputs: ['view_12'] - outputs: ['permute_9']\n",
      "#182: Transpose - inputs: ['view_13'] - outputs: ['permute_10']\n",
      "#183: Reshape - inputs: ['permute_8', 'val_112'] - outputs: ['val_213']\n",
      "#184: Mul - inputs: ['permute_10', 'val_117'] - outputs: ['val_218']\n",
      "#185: Transpose - inputs: ['val_213'] - outputs: ['val_214']\n",
      "#186: Reshape - inputs: ['val_214', 'val_115'] - outputs: ['val_216']\n",
      "#187: Mul - inputs: ['val_216', 'val_117'] - outputs: ['val_221']\n",
      "#188: MatMul - inputs: ['val_218', 'val_221'] - outputs: ['val_222']\n",
      "#189: Softmax - inputs: ['val_222'] - outputs: ['val_223']\n",
      "#190: MatMul - inputs: ['val_223', 'permute_9'] - outputs: ['scaled_dot_product_attention_2']\n",
      "#191: Transpose - inputs: ['scaled_dot_product_attention_2'] - outputs: ['transpose_3']\n",
      "#192: Reshape - inputs: ['transpose_3', 'val_128'] - outputs: ['view_14']\n",
      "#193: MatMul - inputs: ['view_14', 'val_227'] - outputs: ['val_228']\n",
      "#194: Add - inputs: ['val_228', 'backbone.encoder.layer.2.attention.output.dense.bias'] - outputs: ['linear_15']\n",
      "#195: Mul - inputs: ['linear_15', 'backbone.encoder.layer.2.layer_scale1.lambda1'] - outputs: ['mul_4']\n",
      "#196: Add - inputs: ['mul_4', 'add_4'] - outputs: ['add_5']\n",
      "#197: LayerNormalization - inputs: ['add_5', 'backbone.encoder.layer.2.norm2.weight', 'backbone.encoder.layer.2.norm2.bias'] - outputs: ['layer_norm_5']\n",
      "#198: MatMul - inputs: ['layer_norm_5', 'val_231'] - outputs: ['val_232']\n",
      "#199: Add - inputs: ['val_232', 'backbone.encoder.layer.2.mlp.fc1.bias'] - outputs: ['linear_16']\n",
      "#200: Div - inputs: ['linear_16', 'val_135'] - outputs: ['val_234']\n",
      "#201: Erf - inputs: ['val_234'] - outputs: ['val_235']\n",
      "#202: Add - inputs: ['val_235', 'val_138'] - outputs: ['val_237']\n",
      "#203: Mul - inputs: ['val_140', 'val_237'] - outputs: ['val_239']\n",
      "#204: Mul - inputs: ['linear_16', 'val_239'] - outputs: ['gelu_2']\n",
      "#205: MatMul - inputs: ['gelu_2', 'val_240'] - outputs: ['val_241']\n",
      "#206: Add - inputs: ['val_241', 'backbone.encoder.layer.2.mlp.fc2.bias'] - outputs: ['linear_17']\n",
      "#207: Mul - inputs: ['linear_17', 'backbone.encoder.layer.2.layer_scale2.lambda1'] - outputs: ['mul_5']\n",
      "#208: Add - inputs: ['mul_5', 'add_5'] - outputs: ['add_6']\n",
      "#209: LayerNormalization - inputs: ['add_6', 'backbone.encoder.layer.3.norm1.weight', 'backbone.encoder.layer.3.norm1.bias'] - outputs: ['layer_norm_6']\n",
      "#210: MatMul - inputs: ['layer_norm_6', 'val_244'] - outputs: ['val_245']\n",
      "#211: MatMul - inputs: ['layer_norm_6', 'val_247'] - outputs: ['val_248']\n",
      "#212: MatMul - inputs: ['layer_norm_6', 'val_250'] - outputs: ['val_251']\n",
      "#213: Add - inputs: ['val_245', 'backbone.encoder.layer.3.attention.attention.key.bias'] - outputs: ['linear_18']\n",
      "#214: Add - inputs: ['val_248', 'backbone.encoder.layer.3.attention.attention.value.bias'] - outputs: ['linear_19']\n",
      "#215: Add - inputs: ['val_251', 'backbone.encoder.layer.3.attention.attention.query.bias'] - outputs: ['linear_20']\n",
      "#216: Reshape - inputs: ['linear_18', 'val_94'] - outputs: ['view_15']\n",
      "#217: Reshape - inputs: ['linear_19', 'val_94'] - outputs: ['view_16']\n",
      "#218: Reshape - inputs: ['linear_20', 'val_94'] - outputs: ['view_17']\n",
      "#219: Transpose - inputs: ['view_15'] - outputs: ['permute_11']\n",
      "#220: Transpose - inputs: ['view_16'] - outputs: ['permute_12']\n",
      "#221: Transpose - inputs: ['view_17'] - outputs: ['permute_13']\n",
      "#222: Reshape - inputs: ['permute_11', 'val_112'] - outputs: ['val_262']\n",
      "#223: Mul - inputs: ['permute_13', 'val_117'] - outputs: ['val_267']\n",
      "#224: Transpose - inputs: ['val_262'] - outputs: ['val_263']\n",
      "#225: Reshape - inputs: ['val_263', 'val_115'] - outputs: ['val_265']\n",
      "#226: Mul - inputs: ['val_265', 'val_117'] - outputs: ['val_270']\n",
      "#227: MatMul - inputs: ['val_267', 'val_270'] - outputs: ['val_271']\n",
      "#228: Softmax - inputs: ['val_271'] - outputs: ['val_272']\n",
      "#229: MatMul - inputs: ['val_272', 'permute_12'] - outputs: ['scaled_dot_product_attention_3']\n",
      "#230: Transpose - inputs: ['scaled_dot_product_attention_3'] - outputs: ['transpose_4']\n",
      "#231: Reshape - inputs: ['transpose_4', 'val_128'] - outputs: ['view_18']\n",
      "#232: MatMul - inputs: ['view_18', 'val_276'] - outputs: ['val_277']\n",
      "#233: Add - inputs: ['val_277', 'backbone.encoder.layer.3.attention.output.dense.bias'] - outputs: ['linear_21']\n",
      "#234: Mul - inputs: ['linear_21', 'backbone.encoder.layer.3.layer_scale1.lambda1'] - outputs: ['mul_6']\n",
      "#235: Add - inputs: ['mul_6', 'add_6'] - outputs: ['add_7']\n",
      "#236: LayerNormalization - inputs: ['add_7', 'backbone.encoder.layer.3.norm2.weight', 'backbone.encoder.layer.3.norm2.bias'] - outputs: ['layer_norm_7']\n",
      "#237: MatMul - inputs: ['layer_norm_7', 'val_280'] - outputs: ['val_281']\n",
      "#238: Add - inputs: ['val_281', 'backbone.encoder.layer.3.mlp.fc1.bias'] - outputs: ['linear_22']\n",
      "#239: Div - inputs: ['linear_22', 'val_135'] - outputs: ['val_283']\n",
      "#240: Erf - inputs: ['val_283'] - outputs: ['val_284']\n",
      "#241: Add - inputs: ['val_284', 'val_138'] - outputs: ['val_286']\n",
      "#242: Mul - inputs: ['val_140', 'val_286'] - outputs: ['val_288']\n",
      "#243: Mul - inputs: ['linear_22', 'val_288'] - outputs: ['gelu_3']\n",
      "#244: MatMul - inputs: ['gelu_3', 'val_289'] - outputs: ['val_290']\n",
      "#245: Add - inputs: ['val_290', 'backbone.encoder.layer.3.mlp.fc2.bias'] - outputs: ['linear_23']\n",
      "#246: Mul - inputs: ['linear_23', 'backbone.encoder.layer.3.layer_scale2.lambda1'] - outputs: ['mul_7']\n",
      "#247: Add - inputs: ['mul_7', 'add_7'] - outputs: ['add_8']\n",
      "#248: LayerNormalization - inputs: ['add_8', 'backbone.encoder.layer.4.norm1.weight', 'backbone.encoder.layer.4.norm1.bias'] - outputs: ['layer_norm_8']\n",
      "#249: MatMul - inputs: ['layer_norm_8', 'val_293'] - outputs: ['val_294']\n",
      "#250: MatMul - inputs: ['layer_norm_8', 'val_296'] - outputs: ['val_297']\n",
      "#251: MatMul - inputs: ['layer_norm_8', 'val_299'] - outputs: ['val_300']\n",
      "#252: Add - inputs: ['val_294', 'backbone.encoder.layer.4.attention.attention.key.bias'] - outputs: ['linear_24']\n",
      "#253: Add - inputs: ['val_297', 'backbone.encoder.layer.4.attention.attention.value.bias'] - outputs: ['linear_25']\n",
      "#254: Add - inputs: ['val_300', 'backbone.encoder.layer.4.attention.attention.query.bias'] - outputs: ['linear_26']\n",
      "#255: Reshape - inputs: ['linear_24', 'val_94'] - outputs: ['view_19']\n",
      "#256: Reshape - inputs: ['linear_25', 'val_94'] - outputs: ['view_20']\n",
      "#257: Reshape - inputs: ['linear_26', 'val_94'] - outputs: ['view_21']\n",
      "#258: Transpose - inputs: ['view_19'] - outputs: ['permute_14']\n",
      "#259: Transpose - inputs: ['view_20'] - outputs: ['permute_15']\n",
      "#260: Transpose - inputs: ['view_21'] - outputs: ['permute_16']\n",
      "#261: Reshape - inputs: ['permute_14', 'val_112'] - outputs: ['val_311']\n",
      "#262: Mul - inputs: ['permute_16', 'val_117'] - outputs: ['val_316']\n",
      "#263: Transpose - inputs: ['val_311'] - outputs: ['val_312']\n",
      "#264: Reshape - inputs: ['val_312', 'val_115'] - outputs: ['val_314']\n",
      "#265: Mul - inputs: ['val_314', 'val_117'] - outputs: ['val_319']\n",
      "#266: MatMul - inputs: ['val_316', 'val_319'] - outputs: ['val_320']\n",
      "#267: Softmax - inputs: ['val_320'] - outputs: ['val_321']\n",
      "#268: MatMul - inputs: ['val_321', 'permute_15'] - outputs: ['scaled_dot_product_attention_4']\n",
      "#269: Transpose - inputs: ['scaled_dot_product_attention_4'] - outputs: ['transpose_5']\n",
      "#270: Reshape - inputs: ['transpose_5', 'val_128'] - outputs: ['view_22']\n",
      "#271: MatMul - inputs: ['view_22', 'val_325'] - outputs: ['val_326']\n",
      "#272: Add - inputs: ['val_326', 'backbone.encoder.layer.4.attention.output.dense.bias'] - outputs: ['linear_27']\n",
      "#273: Mul - inputs: ['linear_27', 'backbone.encoder.layer.4.layer_scale1.lambda1'] - outputs: ['mul_8']\n",
      "#274: Add - inputs: ['mul_8', 'add_8'] - outputs: ['add_9']\n",
      "#275: LayerNormalization - inputs: ['add_9', 'backbone.encoder.layer.4.norm2.weight', 'backbone.encoder.layer.4.norm2.bias'] - outputs: ['layer_norm_9']\n",
      "#276: MatMul - inputs: ['layer_norm_9', 'val_329'] - outputs: ['val_330']\n",
      "#277: Add - inputs: ['val_330', 'backbone.encoder.layer.4.mlp.fc1.bias'] - outputs: ['linear_28']\n",
      "#278: Div - inputs: ['linear_28', 'val_135'] - outputs: ['val_332']\n",
      "#279: Erf - inputs: ['val_332'] - outputs: ['val_333']\n",
      "#280: Add - inputs: ['val_333', 'val_138'] - outputs: ['val_335']\n",
      "#281: Mul - inputs: ['val_140', 'val_335'] - outputs: ['val_337']\n",
      "#282: Mul - inputs: ['linear_28', 'val_337'] - outputs: ['gelu_4']\n",
      "#283: MatMul - inputs: ['gelu_4', 'val_338'] - outputs: ['val_339']\n",
      "#284: Add - inputs: ['val_339', 'backbone.encoder.layer.4.mlp.fc2.bias'] - outputs: ['linear_29']\n",
      "#285: Mul - inputs: ['linear_29', 'backbone.encoder.layer.4.layer_scale2.lambda1'] - outputs: ['mul_9']\n",
      "#286: Add - inputs: ['mul_9', 'add_9'] - outputs: ['add_10']\n",
      "#287: LayerNormalization - inputs: ['add_10', 'backbone.encoder.layer.5.norm1.weight', 'backbone.encoder.layer.5.norm1.bias'] - outputs: ['layer_norm_10']\n",
      "#288: MatMul - inputs: ['layer_norm_10', 'val_342'] - outputs: ['val_343']\n",
      "#289: MatMul - inputs: ['layer_norm_10', 'val_345'] - outputs: ['val_346']\n",
      "#290: MatMul - inputs: ['layer_norm_10', 'val_348'] - outputs: ['val_349']\n",
      "#291: Add - inputs: ['val_343', 'backbone.encoder.layer.5.attention.attention.key.bias'] - outputs: ['linear_30']\n",
      "#292: Add - inputs: ['val_346', 'backbone.encoder.layer.5.attention.attention.value.bias'] - outputs: ['linear_31']\n",
      "#293: Add - inputs: ['val_349', 'backbone.encoder.layer.5.attention.attention.query.bias'] - outputs: ['linear_32']\n",
      "#294: Reshape - inputs: ['linear_30', 'val_94'] - outputs: ['view_23']\n",
      "#295: Reshape - inputs: ['linear_31', 'val_94'] - outputs: ['view_24']\n",
      "#296: Reshape - inputs: ['linear_32', 'val_94'] - outputs: ['view_25']\n",
      "#297: Transpose - inputs: ['view_23'] - outputs: ['permute_17']\n",
      "#298: Transpose - inputs: ['view_24'] - outputs: ['permute_18']\n",
      "#299: Transpose - inputs: ['view_25'] - outputs: ['permute_19']\n",
      "#300: Reshape - inputs: ['permute_17', 'val_112'] - outputs: ['val_360']\n",
      "#301: Mul - inputs: ['permute_19', 'val_117'] - outputs: ['val_365']\n",
      "#302: Transpose - inputs: ['val_360'] - outputs: ['val_361']\n",
      "#303: Reshape - inputs: ['val_361', 'val_115'] - outputs: ['val_363']\n",
      "#304: Mul - inputs: ['val_363', 'val_117'] - outputs: ['val_368']\n",
      "#305: MatMul - inputs: ['val_365', 'val_368'] - outputs: ['val_369']\n",
      "#306: Softmax - inputs: ['val_369'] - outputs: ['val_370']\n",
      "#307: MatMul - inputs: ['val_370', 'permute_18'] - outputs: ['scaled_dot_product_attention_5']\n",
      "#308: Transpose - inputs: ['scaled_dot_product_attention_5'] - outputs: ['transpose_6']\n",
      "#309: Reshape - inputs: ['transpose_6', 'val_128'] - outputs: ['view_26']\n",
      "#310: MatMul - inputs: ['view_26', 'val_374'] - outputs: ['val_375']\n",
      "#311: Add - inputs: ['val_375', 'backbone.encoder.layer.5.attention.output.dense.bias'] - outputs: ['linear_33']\n",
      "#312: Mul - inputs: ['linear_33', 'backbone.encoder.layer.5.layer_scale1.lambda1'] - outputs: ['mul_10']\n",
      "#313: Add - inputs: ['mul_10', 'add_10'] - outputs: ['add_11']\n",
      "#314: LayerNormalization - inputs: ['add_11', 'backbone.encoder.layer.5.norm2.weight', 'backbone.encoder.layer.5.norm2.bias'] - outputs: ['layer_norm_11']\n",
      "#315: MatMul - inputs: ['layer_norm_11', 'val_378'] - outputs: ['val_379']\n",
      "#316: Add - inputs: ['val_379', 'backbone.encoder.layer.5.mlp.fc1.bias'] - outputs: ['linear_34']\n",
      "#317: Div - inputs: ['linear_34', 'val_135'] - outputs: ['val_381']\n",
      "#318: Erf - inputs: ['val_381'] - outputs: ['val_382']\n",
      "#319: Add - inputs: ['val_382', 'val_138'] - outputs: ['val_384']\n",
      "#320: Mul - inputs: ['val_140', 'val_384'] - outputs: ['val_386']\n",
      "#321: Mul - inputs: ['linear_34', 'val_386'] - outputs: ['gelu_5']\n",
      "#322: MatMul - inputs: ['gelu_5', 'val_387'] - outputs: ['val_388']\n",
      "#323: Add - inputs: ['val_388', 'backbone.encoder.layer.5.mlp.fc2.bias'] - outputs: ['linear_35']\n",
      "#324: Mul - inputs: ['linear_35', 'backbone.encoder.layer.5.layer_scale2.lambda1'] - outputs: ['mul_11']\n",
      "#325: Add - inputs: ['mul_11', 'add_11'] - outputs: ['add_12']\n",
      "#326: LayerNormalization - inputs: ['add_12', 'backbone.encoder.layer.6.norm1.weight', 'backbone.encoder.layer.6.norm1.bias'] - outputs: ['layer_norm_12']\n",
      "#327: MatMul - inputs: ['layer_norm_12', 'val_391'] - outputs: ['val_392']\n",
      "#328: MatMul - inputs: ['layer_norm_12', 'val_394'] - outputs: ['val_395']\n",
      "#329: MatMul - inputs: ['layer_norm_12', 'val_397'] - outputs: ['val_398']\n",
      "#330: Add - inputs: ['val_392', 'backbone.encoder.layer.6.attention.attention.key.bias'] - outputs: ['linear_36']\n",
      "#331: Add - inputs: ['val_395', 'backbone.encoder.layer.6.attention.attention.value.bias'] - outputs: ['linear_37']\n",
      "#332: Add - inputs: ['val_398', 'backbone.encoder.layer.6.attention.attention.query.bias'] - outputs: ['linear_38']\n",
      "#333: Reshape - inputs: ['linear_36', 'val_94'] - outputs: ['view_27']\n",
      "#334: Reshape - inputs: ['linear_37', 'val_94'] - outputs: ['view_28']\n",
      "#335: Reshape - inputs: ['linear_38', 'val_94'] - outputs: ['view_29']\n",
      "#336: Transpose - inputs: ['view_27'] - outputs: ['permute_20']\n",
      "#337: Transpose - inputs: ['view_28'] - outputs: ['permute_21']\n",
      "#338: Transpose - inputs: ['view_29'] - outputs: ['permute_22']\n",
      "#339: Reshape - inputs: ['permute_20', 'val_112'] - outputs: ['val_409']\n",
      "#340: Mul - inputs: ['permute_22', 'val_117'] - outputs: ['val_414']\n",
      "#341: Transpose - inputs: ['val_409'] - outputs: ['val_410']\n",
      "#342: Reshape - inputs: ['val_410', 'val_115'] - outputs: ['val_412']\n",
      "#343: Mul - inputs: ['val_412', 'val_117'] - outputs: ['val_417']\n",
      "#344: MatMul - inputs: ['val_414', 'val_417'] - outputs: ['val_418']\n",
      "#345: Softmax - inputs: ['val_418'] - outputs: ['val_419']\n",
      "#346: MatMul - inputs: ['val_419', 'permute_21'] - outputs: ['scaled_dot_product_attention_6']\n",
      "#347: Transpose - inputs: ['scaled_dot_product_attention_6'] - outputs: ['transpose_7']\n",
      "#348: Reshape - inputs: ['transpose_7', 'val_128'] - outputs: ['view_30']\n",
      "#349: MatMul - inputs: ['view_30', 'val_423'] - outputs: ['val_424']\n",
      "#350: Add - inputs: ['val_424', 'backbone.encoder.layer.6.attention.output.dense.bias'] - outputs: ['linear_39']\n",
      "#351: Mul - inputs: ['linear_39', 'backbone.encoder.layer.6.layer_scale1.lambda1'] - outputs: ['mul_12']\n",
      "#352: Add - inputs: ['mul_12', 'add_12'] - outputs: ['add_13']\n",
      "#353: LayerNormalization - inputs: ['add_13', 'backbone.encoder.layer.6.norm2.weight', 'backbone.encoder.layer.6.norm2.bias'] - outputs: ['layer_norm_13']\n",
      "#354: MatMul - inputs: ['layer_norm_13', 'val_427'] - outputs: ['val_428']\n",
      "#355: Add - inputs: ['val_428', 'backbone.encoder.layer.6.mlp.fc1.bias'] - outputs: ['linear_40']\n",
      "#356: Div - inputs: ['linear_40', 'val_135'] - outputs: ['val_430']\n",
      "#357: Erf - inputs: ['val_430'] - outputs: ['val_431']\n",
      "#358: Add - inputs: ['val_431', 'val_138'] - outputs: ['val_433']\n",
      "#359: Mul - inputs: ['val_140', 'val_433'] - outputs: ['val_435']\n",
      "#360: Mul - inputs: ['linear_40', 'val_435'] - outputs: ['gelu_6']\n",
      "#361: MatMul - inputs: ['gelu_6', 'val_436'] - outputs: ['val_437']\n",
      "#362: Add - inputs: ['val_437', 'backbone.encoder.layer.6.mlp.fc2.bias'] - outputs: ['linear_41']\n",
      "#363: Mul - inputs: ['linear_41', 'backbone.encoder.layer.6.layer_scale2.lambda1'] - outputs: ['mul_13']\n",
      "#364: Add - inputs: ['mul_13', 'add_13'] - outputs: ['add_14']\n",
      "#365: LayerNormalization - inputs: ['add_14', 'backbone.encoder.layer.7.norm1.weight', 'backbone.encoder.layer.7.norm1.bias'] - outputs: ['layer_norm_14']\n",
      "#366: MatMul - inputs: ['layer_norm_14', 'val_440'] - outputs: ['val_441']\n",
      "#367: MatMul - inputs: ['layer_norm_14', 'val_443'] - outputs: ['val_444']\n",
      "#368: MatMul - inputs: ['layer_norm_14', 'val_446'] - outputs: ['val_447']\n",
      "#369: Add - inputs: ['val_441', 'backbone.encoder.layer.7.attention.attention.key.bias'] - outputs: ['linear_42']\n",
      "#370: Add - inputs: ['val_444', 'backbone.encoder.layer.7.attention.attention.value.bias'] - outputs: ['linear_43']\n",
      "#371: Add - inputs: ['val_447', 'backbone.encoder.layer.7.attention.attention.query.bias'] - outputs: ['linear_44']\n",
      "#372: Reshape - inputs: ['linear_42', 'val_94'] - outputs: ['view_31']\n",
      "#373: Reshape - inputs: ['linear_43', 'val_94'] - outputs: ['view_32']\n",
      "#374: Reshape - inputs: ['linear_44', 'val_94'] - outputs: ['view_33']\n",
      "#375: Transpose - inputs: ['view_31'] - outputs: ['permute_23']\n",
      "#376: Transpose - inputs: ['view_32'] - outputs: ['permute_24']\n",
      "#377: Transpose - inputs: ['view_33'] - outputs: ['permute_25']\n",
      "#378: Reshape - inputs: ['permute_23', 'val_112'] - outputs: ['val_458']\n",
      "#379: Mul - inputs: ['permute_25', 'val_117'] - outputs: ['val_463']\n",
      "#380: Transpose - inputs: ['val_458'] - outputs: ['val_459']\n",
      "#381: Reshape - inputs: ['val_459', 'val_115'] - outputs: ['val_461']\n",
      "#382: Mul - inputs: ['val_461', 'val_117'] - outputs: ['val_466']\n",
      "#383: MatMul - inputs: ['val_463', 'val_466'] - outputs: ['val_467']\n",
      "#384: Softmax - inputs: ['val_467'] - outputs: ['val_468']\n",
      "#385: MatMul - inputs: ['val_468', 'permute_24'] - outputs: ['scaled_dot_product_attention_7']\n",
      "#386: Transpose - inputs: ['scaled_dot_product_attention_7'] - outputs: ['transpose_8']\n",
      "#387: Reshape - inputs: ['transpose_8', 'val_128'] - outputs: ['view_34']\n",
      "#388: MatMul - inputs: ['view_34', 'val_472'] - outputs: ['val_473']\n",
      "#389: Add - inputs: ['val_473', 'backbone.encoder.layer.7.attention.output.dense.bias'] - outputs: ['linear_45']\n",
      "#390: Mul - inputs: ['linear_45', 'backbone.encoder.layer.7.layer_scale1.lambda1'] - outputs: ['mul_14']\n",
      "#391: Add - inputs: ['mul_14', 'add_14'] - outputs: ['add_15']\n",
      "#392: LayerNormalization - inputs: ['add_15', 'backbone.encoder.layer.7.norm2.weight', 'backbone.encoder.layer.7.norm2.bias'] - outputs: ['layer_norm_15']\n",
      "#393: MatMul - inputs: ['layer_norm_15', 'val_476'] - outputs: ['val_477']\n",
      "#394: Add - inputs: ['val_477', 'backbone.encoder.layer.7.mlp.fc1.bias'] - outputs: ['linear_46']\n",
      "#395: Div - inputs: ['linear_46', 'val_135'] - outputs: ['val_479']\n",
      "#396: Erf - inputs: ['val_479'] - outputs: ['val_480']\n",
      "#397: Add - inputs: ['val_480', 'val_138'] - outputs: ['val_482']\n",
      "#398: Mul - inputs: ['val_140', 'val_482'] - outputs: ['val_484']\n",
      "#399: Mul - inputs: ['linear_46', 'val_484'] - outputs: ['gelu_7']\n",
      "#400: MatMul - inputs: ['gelu_7', 'val_485'] - outputs: ['val_486']\n",
      "#401: Add - inputs: ['val_486', 'backbone.encoder.layer.7.mlp.fc2.bias'] - outputs: ['linear_47']\n",
      "#402: Mul - inputs: ['linear_47', 'backbone.encoder.layer.7.layer_scale2.lambda1'] - outputs: ['mul_15']\n",
      "#403: Add - inputs: ['mul_15', 'add_15'] - outputs: ['add_16']\n",
      "#404: LayerNormalization - inputs: ['add_16', 'backbone.encoder.layer.8.norm1.weight', 'backbone.encoder.layer.8.norm1.bias'] - outputs: ['layer_norm_16']\n",
      "#405: MatMul - inputs: ['layer_norm_16', 'val_489'] - outputs: ['val_490']\n",
      "#406: MatMul - inputs: ['layer_norm_16', 'val_492'] - outputs: ['val_493']\n",
      "#407: MatMul - inputs: ['layer_norm_16', 'val_495'] - outputs: ['val_496']\n",
      "#408: Add - inputs: ['val_490', 'backbone.encoder.layer.8.attention.attention.key.bias'] - outputs: ['linear_48']\n",
      "#409: Add - inputs: ['val_493', 'backbone.encoder.layer.8.attention.attention.value.bias'] - outputs: ['linear_49']\n",
      "#410: Add - inputs: ['val_496', 'backbone.encoder.layer.8.attention.attention.query.bias'] - outputs: ['linear_50']\n",
      "#411: Reshape - inputs: ['linear_48', 'val_94'] - outputs: ['view_35']\n",
      "#412: Reshape - inputs: ['linear_49', 'val_94'] - outputs: ['view_36']\n",
      "#413: Reshape - inputs: ['linear_50', 'val_94'] - outputs: ['view_37']\n",
      "#414: Transpose - inputs: ['view_35'] - outputs: ['permute_26']\n",
      "#415: Transpose - inputs: ['view_36'] - outputs: ['permute_27']\n",
      "#416: Transpose - inputs: ['view_37'] - outputs: ['permute_28']\n",
      "#417: Reshape - inputs: ['permute_26', 'val_112'] - outputs: ['val_507']\n",
      "#418: Mul - inputs: ['permute_28', 'val_117'] - outputs: ['val_512']\n",
      "#419: Transpose - inputs: ['val_507'] - outputs: ['val_508']\n",
      "#420: Reshape - inputs: ['val_508', 'val_115'] - outputs: ['val_510']\n",
      "#421: Mul - inputs: ['val_510', 'val_117'] - outputs: ['val_515']\n",
      "#422: MatMul - inputs: ['val_512', 'val_515'] - outputs: ['val_516']\n",
      "#423: Softmax - inputs: ['val_516'] - outputs: ['val_517']\n",
      "#424: MatMul - inputs: ['val_517', 'permute_27'] - outputs: ['scaled_dot_product_attention_8']\n",
      "#425: Transpose - inputs: ['scaled_dot_product_attention_8'] - outputs: ['transpose_9']\n",
      "#426: Reshape - inputs: ['transpose_9', 'val_128'] - outputs: ['view_38']\n",
      "#427: MatMul - inputs: ['view_38', 'val_521'] - outputs: ['val_522']\n",
      "#428: Add - inputs: ['val_522', 'backbone.encoder.layer.8.attention.output.dense.bias'] - outputs: ['linear_51']\n",
      "#429: Mul - inputs: ['linear_51', 'backbone.encoder.layer.8.layer_scale1.lambda1'] - outputs: ['mul_16']\n",
      "#430: Add - inputs: ['mul_16', 'add_16'] - outputs: ['add_17']\n",
      "#431: LayerNormalization - inputs: ['add_17', 'backbone.encoder.layer.8.norm2.weight', 'backbone.encoder.layer.8.norm2.bias'] - outputs: ['layer_norm_17']\n",
      "#432: MatMul - inputs: ['layer_norm_17', 'val_525'] - outputs: ['val_526']\n",
      "#433: Add - inputs: ['val_526', 'backbone.encoder.layer.8.mlp.fc1.bias'] - outputs: ['linear_52']\n",
      "#434: Div - inputs: ['linear_52', 'val_135'] - outputs: ['val_528']\n",
      "#435: Erf - inputs: ['val_528'] - outputs: ['val_529']\n",
      "#436: Add - inputs: ['val_529', 'val_138'] - outputs: ['val_531']\n",
      "#437: Mul - inputs: ['val_140', 'val_531'] - outputs: ['val_533']\n",
      "#438: Mul - inputs: ['linear_52', 'val_533'] - outputs: ['gelu_8']\n",
      "#439: MatMul - inputs: ['gelu_8', 'val_534'] - outputs: ['val_535']\n",
      "#440: Add - inputs: ['val_535', 'backbone.encoder.layer.8.mlp.fc2.bias'] - outputs: ['linear_53']\n",
      "#441: Mul - inputs: ['linear_53', 'backbone.encoder.layer.8.layer_scale2.lambda1'] - outputs: ['mul_17']\n",
      "#442: Add - inputs: ['mul_17', 'add_17'] - outputs: ['add_18']\n",
      "#443: LayerNormalization - inputs: ['add_18', 'backbone.encoder.layer.9.norm1.weight', 'backbone.encoder.layer.9.norm1.bias'] - outputs: ['layer_norm_18']\n",
      "#444: MatMul - inputs: ['layer_norm_18', 'val_538'] - outputs: ['val_539']\n",
      "#445: MatMul - inputs: ['layer_norm_18', 'val_541'] - outputs: ['val_542']\n",
      "#446: MatMul - inputs: ['layer_norm_18', 'val_544'] - outputs: ['val_545']\n",
      "#447: Add - inputs: ['val_539', 'backbone.encoder.layer.9.attention.attention.key.bias'] - outputs: ['linear_54']\n",
      "#448: Add - inputs: ['val_542', 'backbone.encoder.layer.9.attention.attention.value.bias'] - outputs: ['linear_55']\n",
      "#449: Add - inputs: ['val_545', 'backbone.encoder.layer.9.attention.attention.query.bias'] - outputs: ['linear_56']\n",
      "#450: Reshape - inputs: ['linear_54', 'val_94'] - outputs: ['view_39']\n",
      "#451: Reshape - inputs: ['linear_55', 'val_94'] - outputs: ['view_40']\n",
      "#452: Reshape - inputs: ['linear_56', 'val_94'] - outputs: ['view_41']\n",
      "#453: Transpose - inputs: ['view_39'] - outputs: ['permute_29']\n",
      "#454: Transpose - inputs: ['view_40'] - outputs: ['permute_30']\n",
      "#455: Transpose - inputs: ['view_41'] - outputs: ['permute_31']\n",
      "#456: Reshape - inputs: ['permute_29', 'val_112'] - outputs: ['val_556']\n",
      "#457: Mul - inputs: ['permute_31', 'val_117'] - outputs: ['val_561']\n",
      "#458: Transpose - inputs: ['val_556'] - outputs: ['val_557']\n",
      "#459: Reshape - inputs: ['val_557', 'val_115'] - outputs: ['val_559']\n",
      "#460: Mul - inputs: ['val_559', 'val_117'] - outputs: ['val_564']\n",
      "#461: MatMul - inputs: ['val_561', 'val_564'] - outputs: ['val_565']\n",
      "#462: Softmax - inputs: ['val_565'] - outputs: ['val_566']\n",
      "#463: MatMul - inputs: ['val_566', 'permute_30'] - outputs: ['scaled_dot_product_attention_9']\n",
      "#464: Transpose - inputs: ['scaled_dot_product_attention_9'] - outputs: ['transpose_10']\n",
      "#465: Reshape - inputs: ['transpose_10', 'val_128'] - outputs: ['view_42']\n",
      "#466: MatMul - inputs: ['view_42', 'val_570'] - outputs: ['val_571']\n",
      "#467: Add - inputs: ['val_571', 'backbone.encoder.layer.9.attention.output.dense.bias'] - outputs: ['linear_57']\n",
      "#468: Mul - inputs: ['linear_57', 'backbone.encoder.layer.9.layer_scale1.lambda1'] - outputs: ['mul_18']\n",
      "#469: Add - inputs: ['mul_18', 'add_18'] - outputs: ['add_19']\n",
      "#470: LayerNormalization - inputs: ['add_19', 'backbone.encoder.layer.9.norm2.weight', 'backbone.encoder.layer.9.norm2.bias'] - outputs: ['layer_norm_19']\n",
      "#471: MatMul - inputs: ['layer_norm_19', 'val_574'] - outputs: ['val_575']\n",
      "#472: Add - inputs: ['val_575', 'backbone.encoder.layer.9.mlp.fc1.bias'] - outputs: ['linear_58']\n",
      "#473: Div - inputs: ['linear_58', 'val_135'] - outputs: ['val_577']\n",
      "#474: Erf - inputs: ['val_577'] - outputs: ['val_578']\n",
      "#475: Add - inputs: ['val_578', 'val_138'] - outputs: ['val_580']\n",
      "#476: Mul - inputs: ['val_140', 'val_580'] - outputs: ['val_582']\n",
      "#477: Mul - inputs: ['linear_58', 'val_582'] - outputs: ['gelu_9']\n",
      "#478: MatMul - inputs: ['gelu_9', 'val_583'] - outputs: ['val_584']\n",
      "#479: Add - inputs: ['val_584', 'backbone.encoder.layer.9.mlp.fc2.bias'] - outputs: ['linear_59']\n",
      "#480: Mul - inputs: ['linear_59', 'backbone.encoder.layer.9.layer_scale2.lambda1'] - outputs: ['mul_19']\n",
      "#481: Add - inputs: ['mul_19', 'add_19'] - outputs: ['add_20']\n",
      "#482: LayerNormalization - inputs: ['add_20', 'backbone.encoder.layer.10.norm1.weight', 'backbone.encoder.layer.10.norm1.bias'] - outputs: ['layer_norm_20']\n",
      "#483: MatMul - inputs: ['layer_norm_20', 'val_587'] - outputs: ['val_588']\n",
      "#484: MatMul - inputs: ['layer_norm_20', 'val_590'] - outputs: ['val_591']\n",
      "#485: MatMul - inputs: ['layer_norm_20', 'val_593'] - outputs: ['val_594']\n",
      "#486: Add - inputs: ['val_588', 'backbone.encoder.layer.10.attention.attention.key.bias'] - outputs: ['linear_60']\n",
      "#487: Add - inputs: ['val_591', 'backbone.encoder.layer.10.attention.attention.value.bias'] - outputs: ['linear_61']\n",
      "#488: Add - inputs: ['val_594', 'backbone.encoder.layer.10.attention.attention.query.bias'] - outputs: ['linear_62']\n",
      "#489: Reshape - inputs: ['linear_60', 'val_94'] - outputs: ['view_43']\n",
      "#490: Reshape - inputs: ['linear_61', 'val_94'] - outputs: ['view_44']\n",
      "#491: Reshape - inputs: ['linear_62', 'val_94'] - outputs: ['view_45']\n",
      "#492: Transpose - inputs: ['view_43'] - outputs: ['permute_32']\n",
      "#493: Transpose - inputs: ['view_44'] - outputs: ['permute_33']\n",
      "#494: Transpose - inputs: ['view_45'] - outputs: ['permute_34']\n",
      "#495: Reshape - inputs: ['permute_32', 'val_112'] - outputs: ['val_605']\n",
      "#496: Mul - inputs: ['permute_34', 'val_117'] - outputs: ['val_610']\n",
      "#497: Transpose - inputs: ['val_605'] - outputs: ['val_606']\n",
      "#498: Reshape - inputs: ['val_606', 'val_115'] - outputs: ['val_608']\n",
      "#499: Mul - inputs: ['val_608', 'val_117'] - outputs: ['val_613']\n",
      "#500: MatMul - inputs: ['val_610', 'val_613'] - outputs: ['val_614']\n",
      "#501: Softmax - inputs: ['val_614'] - outputs: ['val_615']\n",
      "#502: MatMul - inputs: ['val_615', 'permute_33'] - outputs: ['scaled_dot_product_attention_10']\n",
      "#503: Transpose - inputs: ['scaled_dot_product_attention_10'] - outputs: ['transpose_11']\n",
      "#504: Reshape - inputs: ['transpose_11', 'val_128'] - outputs: ['view_46']\n",
      "#505: MatMul - inputs: ['view_46', 'val_619'] - outputs: ['val_620']\n",
      "#506: Add - inputs: ['val_620', 'backbone.encoder.layer.10.attention.output.dense.bias'] - outputs: ['linear_63']\n",
      "#507: Mul - inputs: ['linear_63', 'backbone.encoder.layer.10.layer_scale1.lambda1'] - outputs: ['mul_20']\n",
      "#508: Add - inputs: ['mul_20', 'add_20'] - outputs: ['add_21']\n",
      "#509: LayerNormalization - inputs: ['add_21', 'backbone.encoder.layer.10.norm2.weight', 'backbone.encoder.layer.10.norm2.bias'] - outputs: ['layer_norm_21']\n",
      "#510: MatMul - inputs: ['layer_norm_21', 'val_623'] - outputs: ['val_624']\n",
      "#511: Add - inputs: ['val_624', 'backbone.encoder.layer.10.mlp.fc1.bias'] - outputs: ['linear_64']\n",
      "#512: Div - inputs: ['linear_64', 'val_135'] - outputs: ['val_626']\n",
      "#513: Erf - inputs: ['val_626'] - outputs: ['val_627']\n",
      "#514: Add - inputs: ['val_627', 'val_138'] - outputs: ['val_629']\n",
      "#515: Mul - inputs: ['val_140', 'val_629'] - outputs: ['val_631']\n",
      "#516: Mul - inputs: ['linear_64', 'val_631'] - outputs: ['gelu_10']\n",
      "#517: MatMul - inputs: ['gelu_10', 'val_632'] - outputs: ['val_633']\n",
      "#518: Add - inputs: ['val_633', 'backbone.encoder.layer.10.mlp.fc2.bias'] - outputs: ['linear_65']\n",
      "#519: Mul - inputs: ['linear_65', 'backbone.encoder.layer.10.layer_scale2.lambda1'] - outputs: ['mul_21']\n",
      "#520: Add - inputs: ['mul_21', 'add_21'] - outputs: ['add_22']\n",
      "#521: LayerNormalization - inputs: ['add_22', 'backbone.encoder.layer.11.norm1.weight', 'backbone.encoder.layer.11.norm1.bias'] - outputs: ['layer_norm_22']\n",
      "#522: MatMul - inputs: ['layer_norm_22', 'val_636'] - outputs: ['val_637']\n",
      "#523: MatMul - inputs: ['layer_norm_22', 'val_639'] - outputs: ['val_640']\n",
      "#524: MatMul - inputs: ['layer_norm_22', 'val_642'] - outputs: ['val_643']\n",
      "#525: Add - inputs: ['val_637', 'backbone.encoder.layer.11.attention.attention.key.bias'] - outputs: ['linear_66']\n",
      "#526: Add - inputs: ['val_640', 'backbone.encoder.layer.11.attention.attention.value.bias'] - outputs: ['linear_67']\n",
      "#527: Add - inputs: ['val_643', 'backbone.encoder.layer.11.attention.attention.query.bias'] - outputs: ['linear_68']\n",
      "#528: Reshape - inputs: ['linear_66', 'val_94'] - outputs: ['view_47']\n",
      "#529: Reshape - inputs: ['linear_67', 'val_94'] - outputs: ['view_48']\n",
      "#530: Reshape - inputs: ['linear_68', 'val_94'] - outputs: ['view_49']\n",
      "#531: Transpose - inputs: ['view_47'] - outputs: ['permute_35']\n",
      "#532: Transpose - inputs: ['view_48'] - outputs: ['permute_36']\n",
      "#533: Transpose - inputs: ['view_49'] - outputs: ['permute_37']\n",
      "#534: Reshape - inputs: ['permute_35', 'val_112'] - outputs: ['val_654']\n",
      "#535: Mul - inputs: ['permute_37', 'val_117'] - outputs: ['val_659']\n",
      "#536: Transpose - inputs: ['val_654'] - outputs: ['val_655']\n",
      "#537: Reshape - inputs: ['val_655', 'val_115'] - outputs: ['val_657']\n",
      "#538: Mul - inputs: ['val_657', 'val_117'] - outputs: ['val_662']\n",
      "#539: MatMul - inputs: ['val_659', 'val_662'] - outputs: ['val_663']\n",
      "#540: Softmax - inputs: ['val_663'] - outputs: ['val_664']\n",
      "#541: MatMul - inputs: ['val_664', 'permute_36'] - outputs: ['scaled_dot_product_attention_11']\n",
      "#542: Transpose - inputs: ['scaled_dot_product_attention_11'] - outputs: ['transpose_12']\n",
      "#543: Reshape - inputs: ['transpose_12', 'val_128'] - outputs: ['view_50']\n",
      "#544: MatMul - inputs: ['view_50', 'val_668'] - outputs: ['val_669']\n",
      "#545: Add - inputs: ['val_669', 'backbone.encoder.layer.11.attention.output.dense.bias'] - outputs: ['linear_69']\n",
      "#546: Mul - inputs: ['linear_69', 'backbone.encoder.layer.11.layer_scale1.lambda1'] - outputs: ['mul_22']\n",
      "#547: Add - inputs: ['mul_22', 'add_22'] - outputs: ['add_23']\n",
      "#548: LayerNormalization - inputs: ['add_23', 'backbone.encoder.layer.11.norm2.weight', 'backbone.encoder.layer.11.norm2.bias'] - outputs: ['layer_norm_23']\n",
      "#549: MatMul - inputs: ['layer_norm_23', 'val_672'] - outputs: ['val_673']\n",
      "#550: Add - inputs: ['val_673', 'backbone.encoder.layer.11.mlp.fc1.bias'] - outputs: ['linear_70']\n",
      "#551: Div - inputs: ['linear_70', 'val_135'] - outputs: ['val_675']\n",
      "#552: Erf - inputs: ['val_675'] - outputs: ['val_676']\n",
      "#553: Add - inputs: ['val_676', 'val_138'] - outputs: ['val_678']\n",
      "#554: Mul - inputs: ['val_140', 'val_678'] - outputs: ['val_680']\n",
      "#555: Mul - inputs: ['linear_70', 'val_680'] - outputs: ['gelu_11']\n",
      "#556: MatMul - inputs: ['gelu_11', 'val_681'] - outputs: ['val_682']\n",
      "#557: Add - inputs: ['val_682', 'backbone.encoder.layer.11.mlp.fc2.bias'] - outputs: ['linear_71']\n",
      "#558: Mul - inputs: ['linear_71', 'backbone.encoder.layer.11.layer_scale2.lambda1'] - outputs: ['mul_23']\n",
      "#559: Add - inputs: ['mul_23', 'add_23'] - outputs: ['add_24']\n",
      "#560: LayerNormalization - inputs: ['add_24', 'backbone.layernorm.weight', 'backbone.layernorm.bias'] - outputs: ['layer_norm_24']\n",
      "#561: Gather - inputs: ['layer_norm_24', 'val_5'] - outputs: ['select_2']\n",
      "#562: Slice - inputs: ['layer_norm_24', 'val_708', 'val_33', 'val_30', 'val_30'] - outputs: ['slice_12']\n",
      "#563: ReduceMean - inputs: ['slice_12', 'val_30'] - outputs: ['mean']\n",
      "#564: Concat - inputs: ['select_2', 'mean'] - outputs: ['cat_3']\n",
      "#565: Gemm - inputs: ['cat_3', 'classifier.0.weight', 'classifier.0.bias'] - outputs: ['linear_72']\n",
      "#566: LayerNormalization - inputs: ['linear_72', 'classifier.1.weight', 'classifier.1.bias'] - outputs: ['layer_norm_25']\n",
      "#567: Sigmoid - inputs: ['layer_norm_25'] - outputs: ['val_721']\n",
      "#568: Mul - inputs: ['layer_norm_25', 'val_721'] - outputs: ['silu']\n",
      "#569: Gemm - inputs: ['silu', 'classifier.4.weight', 'classifier.4.bias'] - outputs: ['logits']\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load ONNX model\n",
    "model = onnx.load(\"dinov2_classify_slimmed.onnx\")\n",
    "\n",
    "# Print all nodes (layers/ops)\n",
    "for i, node in enumerate(model.graph.node):\n",
    "    print(f\"#{i+1}: {node.op_type} - inputs: {node.input} - outputs: {node.output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4439b7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "- pixel_values\n",
      "Outputs:\n",
      "- logits\n"
     ]
    }
   ],
   "source": [
    "print(\"Inputs:\")\n",
    "for input_tensor in model.graph.input:\n",
    "    print(f\"- {input_tensor.name}\")\n",
    "\n",
    "print(\"Outputs:\")\n",
    "for output_tensor in model.graph.output:\n",
    "    print(f\"- {output_tensor.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74edbfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Inputs:\n",
      " • pixel_values: [1, 3, 224, 224]\n",
      "\n",
      "Model Outputs:\n",
      " • logits: [1, 131]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Optional: run shape inference to enrich shape info\n",
    "from onnx import shape_inference\n",
    "model = shape_inference.infer_shapes(model)\n",
    "\n",
    "# Inspect graph inputs\n",
    "print(\"Model Inputs:\")\n",
    "for inp in model.graph.input:\n",
    "    name = inp.name\n",
    "    dims = inp.type.tensor_type.shape.dim\n",
    "    shape = [\n",
    "        (d.dim_value if d.HasField(\"dim_value\") else f\"{d.dim_param if d.HasField('dim_param') else '?'}\")\n",
    "        for d in dims\n",
    "    ]\n",
    "    print(f\" • {name}: {shape}\")\n",
    "\n",
    "# Inspect graph outputs\n",
    "print(\"\\nModel Outputs:\")\n",
    "for out in model.graph.output:\n",
    "    name = out.name\n",
    "    dims = out.type.tensor_type.shape.dim\n",
    "    shape = [\n",
    "        (d.dim_value if d.HasField(\"dim_value\") else f\"{d.dim_param if d.HasField('dim_param') else '?'}\")\n",
    "        for d in dims\n",
    "    ]\n",
    "    print(f\" • {name}: {shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2001a577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model passed ONNX validation.\n",
      "IR version: 10\n",
      "Opset version for domain '': 20\n"
     ]
    }
   ],
   "source": [
    "from onnx import checker\n",
    "\n",
    "try:\n",
    "    checker.check_model(model)\n",
    "    print(\"✅ Model passed ONNX validation.\")\n",
    "except onnx.checker.ValidationError as e:\n",
    "    print(\"❌ Model failed ONNX validation:\")\n",
    "    print(e)\n",
    "\n",
    "print(\"IR version:\", model.ir_version)\n",
    "for opset in model.opset_import:\n",
    "    print(\"Opset version for domain '{}': {}\".format(opset.domain, opset.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e16d6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "model = onnx.load(\"custom_dinov2_slimmed_antialias.onnx\")\n",
    "model.ir_version = 10  # Downgrade from 11 to 10\n",
    "onnx.save(model, \"custom_dinov2_slimmed_antialias.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2ac26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class ID: 0, Confidence: 0.7195\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "from transformers import AutoImageProcessor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from config import BASE_MODEL_NAME, NUM_CLASSES , HIDDEN_DIM\n",
    "\n",
    "# Constants (same as in PyTorch)\n",
    "IMAGE_PATH = r\".\\Dataset\\Acridotheres javanicus\\Javan Myna_Acridotheres javanicus_1.jpg\"\n",
    "ONNX_PATH = \"dinov2_classify_slimmed.onnx\"  # Path to the ONNX model\n",
    "\n",
    "# Preprocess image using HuggingFace processor\n",
    "processor = AutoImageProcessor.from_pretrained(BASE_MODEL_NAME, cache_dir=\"./cache\", use_fast=False)\n",
    "image = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "inputs = processor(images=image, return_tensors=\"np\")  # use numpy instead of torch\n",
    "\n",
    "# ONNX expects float32 inputs\n",
    "pixel_values = inputs[\"pixel_values\"].astype(np.float32)  # shape: (1, 3, 224, 224)\n",
    "\n",
    "# Run inference\n",
    "session = ort.InferenceSession(ONNX_PATH, providers=[\"CPUExecutionProvider\"])  # or \"CUDAExecutionProvider\"\n",
    "input_name = session.get_inputs()[0].name  # 'pixel_values'\n",
    "output_name = session.get_outputs()[0].name  # 'logits'\n",
    "\n",
    "outputs = session.run([output_name], {input_name: pixel_values})  # list of outputs\n",
    "\n",
    "# Softmax and prediction\n",
    "logits = np.array(outputs[0])\n",
    "exp_logits = np.exp(logits - np.max(logits, axis=-1, keepdims=True))  # stable softmax\n",
    "probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "pred_id = int(np.argmax(probs))\n",
    "confidence = float(probs[0][pred_id])\n",
    "\n",
    "print(f\"Predicted class ID: {pred_id}, Confidence: {confidence:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4c209cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: node_Resize_57 -> antialias = 0\n",
      "After:  node_Resize_57 -> antialias = 1\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx_graphsurgeon as gs\n",
    "\n",
    "# Load the model\n",
    "model_path = \"./quant/basefp32.onnx\"\n",
    "graph = gs.import_onnx(onnx.load(model_path))\n",
    "\n",
    "# Update the `antialias` attribute in Resize nodes\n",
    "for node in graph.nodes:\n",
    "    if node.op == \"Resize\" and \"antialias\" in node.attrs:\n",
    "        print(f\"Before: {node.name} -> antialias = {node.attrs['antialias']}\")\n",
    "        node.attrs[\"antialias\"] = 1\n",
    "        print(f\"After:  {node.name} -> antialias = {node.attrs['antialias']}\")\n",
    "\n",
    "# Cleanup and export\n",
    "graph.cleanup()\n",
    "onnx.save(gs.export_onnx(graph), \"./quant/basefp32_slimmed.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c30d44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_Resize_57 Resize OrderedDict({'mode': 'cubic', 'cubic_coeff_a': -0.75, 'coordinate_transformation_mode': 'pytorch_half_pixel', 'exclude_outside': 0, 'nearest_mode': 'floor', 'antialias': 1, 'extrapolation_value': 0.0, 'keep_aspect_ratio_policy': 'stretch'})\n",
      "✅ Model passed ONNX validation.\n",
      "IR version: 10\n"
     ]
    }
   ],
   "source": [
    "from onnx import checker\n",
    "model_path = \"./quant/basefp32_slimmed.onnx\"\n",
    "model = onnx.load(model_path)\n",
    "graph = gs.import_onnx(model)\n",
    "\n",
    "for node in graph.nodes:\n",
    "    if \"antialias\" in node.attrs:\n",
    "        print(node.name, node.op, node.attrs)\n",
    "\n",
    "try:\n",
    "    checker.check_model(model, full_check=True, check_custom_domain=True)\n",
    "    print(\"✅ Model passed ONNX validation.\")\n",
    "except onnx.checker.ValidationError as e:\n",
    "    print(\"❌ Model failed ONNX validation:\")\n",
    "    print(e)\n",
    "print(\"IR version:\", model.ir_version)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f1ab7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model passed ONNX validation.\n",
      "IR version: 10\n",
      "Opset version for domain '': 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exported_model = gs.export_onnx(graph)\n",
    "\n",
    "# Copy IR and opset version from original\n",
    "exported_model.ir_version = 10\n",
    "#del exported_model.opset_import[:]\n",
    "#exported_model.opset_import.extend(model.opset_import)\n",
    "\n",
    "try:\n",
    "    onnx.checker.check_model(exported_model)\n",
    "    print(\"✅ Model passed ONNX validation.\")\n",
    "except onnx.checker.ValidationError as e:\n",
    "    print(\"❌ Model failed ONNX validation:\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "print(\"IR version:\", exported_model.ir_version)\n",
    "for opset in exported_model.opset_import:\n",
    "    print(\"Opset version for domain '{}': {}\".format(opset.domain, opset.version))\n",
    "\n",
    "onnx.save(exported_model, \"./quant/basefp32_slimmed.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae31628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "Variable (pixel_values): (shape=[1, 3, 224, 224], dtype=float32)\n",
      "node conv1: \n",
      "node_Conv_1 (Conv)\n",
      "\tInputs: [\n",
      "\t\tVariable (graph_input_cast_0): (shape=[1, 3, 224, 224], dtype=float16)\n",
      "\t\tConstant (backbone.embeddings.patch_embeddings.projection.weight): (shape=[768, 3, 14, 14], dtype=float16)\n",
      "\t\tConstant (backbone.embeddings.patch_embeddings.projection.bias): (shape=[768], dtype=float16)\n",
      "\t]\n",
      "\tOutputs: [\n",
      "\t\tVariable (conv2d): (shape=[1, 768, 16, 16], dtype=float16)\n",
      "\t]\n",
      "Attributes: OrderedDict({'auto_pad': 'NOTSET', 'dilations': [1, 1], 'group': 1, 'pads': [0, 0, 0, 0], 'strides': [14, 14]})\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx_graphsurgeon as gs\n",
    "\n",
    "# Load the model\n",
    "model_path = \"model_fp16.onnx\"\n",
    "graph = gs.import_onnx(onnx.load(model_path))\n",
    "\n",
    "print(\"input\")\n",
    "for inp in graph.inputs:\n",
    "    print(inp)\n",
    "\n",
    "print(\"node conv1: \")\n",
    "for node in graph.nodes:\n",
    "    if node.name == \"node_Conv_1\":\n",
    "        print(node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51db41ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model passed ONNX validation.\n",
      "IR version: 10\n",
      "Opset version for domain '': 20\n"
     ]
    }
   ],
   "source": [
    "from onnx import checker\n",
    "\n",
    "model = onnx.load(\"model_nhwc_input.onnx\")\n",
    "try:\n",
    "    checker.check_model(model)\n",
    "    print(\"✅ Model passed ONNX validation.\")\n",
    "except onnx.checker.ValidationError as e:\n",
    "    print(\"❌ Model failed ONNX validation:\")\n",
    "    print(e)\n",
    "\n",
    "print(\"IR version:\", model.ir_version)\n",
    "for opset in model.opset_import:\n",
    "    print(\"Opset version for domain '{}': {}\".format(opset.domain, opset.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4251bc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixed model saved to: model_nhwc_fixed.onnx\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx_graphsurgeon as gs\n",
    "import numpy as np\n",
    "\n",
    "def insert_nhwc_input_transpose(model_path=\"model_fp16.onnx\", output_path=\"model_nhwc_fixed.onnx\"):\n",
    "    # Load model and graph\n",
    "    model = onnx.load(model_path)\n",
    "    graph = gs.import_onnx(model)\n",
    "\n",
    "    # Find the original input\n",
    "    input_var = graph.inputs[0]  # Assumes only one input\n",
    "    assert input_var.name == \"pixel_values\", f\"Expected input named 'pixel_values', got {input_var.name}\"\n",
    "\n",
    "    # Update input shape to NHWC\n",
    "    input_var.shape = [1, 224, 224, 3]\n",
    "    input_var.dtype = np.float32  # keep as float32\n",
    "\n",
    "    # Find the original Cast node\n",
    "    cast_node = next((n for n in graph.nodes if n.op == \"Cast\" and n.outputs[0].name == \"graph_input_cast_0\"), None)\n",
    "    assert cast_node is not None, \"Original Cast node not found.\"\n",
    "\n",
    "    # Create new intermediate variable\n",
    "    nhwc_to_nchw_out = gs.Variable(name=\"transposed_nchw\", dtype=np.float32, shape=[1, 3, 224, 224])\n",
    "\n",
    "    # Create new transpose node (NHWC → NCHW)\n",
    "    transpose_node = gs.Node(\n",
    "        op=\"Transpose\",\n",
    "        name=\"Transpose_NHWC_to_NCHW\",\n",
    "        inputs=[input_var],\n",
    "        outputs=[nhwc_to_nchw_out],\n",
    "        attrs={\"perm\": [0, 3, 1, 2]}\n",
    "    )\n",
    "\n",
    "    # Patch Cast node to take Transpose output\n",
    "    cast_node.inputs[0] = nhwc_to_nchw_out\n",
    "\n",
    "    # Insert Transpose node before Cast\n",
    "    graph.nodes.insert(0, transpose_node)\n",
    "\n",
    "    # Cleanup and export\n",
    "    graph.cleanup().toposort()\n",
    "    onnx.save(gs.export_onnx(graph), output_path)\n",
    "    print(f\"✅ Fixed model saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    insert_nhwc_input_transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42dae214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved transposed model to: ./quant/basefp32_slimmed.onnx\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx_graphsurgeon as gs\n",
    "import numpy as np\n",
    "\n",
    "def insert_nhwc_to_nchw_transpose(\n",
    "    model_path=\"./quant/basefp32_slimmed.onnx\",\n",
    "    output_path=\"./quant/basefp32_slimmed.onnx\",\n",
    "    input_name=\"pixel_values\"\n",
    "):\n",
    "    # Load model and graph\n",
    "    model = onnx.load(model_path)\n",
    "    graph = gs.import_onnx(model)\n",
    "\n",
    "    # Find input\n",
    "    input_var = next((inp for inp in graph.inputs if inp.name == input_name), None)\n",
    "    assert input_var is not None, f\"❌ Input '{input_name}' not found.\"\n",
    "    \n",
    "    # Change input shape to NHWC\n",
    "    input_var.shape = [1, 224, 224, 3]\n",
    "    input_var.dtype = np.float32\n",
    "\n",
    "    # Create transpose output variable\n",
    "    transposed = gs.Variable(name=\"pixel_values_nchw\", dtype=np.float32, shape=[1, 3, 224, 224])\n",
    "\n",
    "    # Create transpose node (NHWC → NCHW)\n",
    "    transpose_node = gs.Node(\n",
    "        op=\"Transpose\",\n",
    "        name=\"Transpose_NHWC_to_NCHW\",\n",
    "        inputs=[input_var],\n",
    "        outputs=[transposed],\n",
    "        attrs={\"perm\": [0, 3, 1, 2]},\n",
    "    )\n",
    "\n",
    "    # Redirect all nodes that used input_var to use transposed\n",
    "    for node in graph.nodes:\n",
    "        node.inputs = [transposed if inp is input_var else inp for inp in node.inputs]\n",
    "\n",
    "    # Add transpose to graph\n",
    "    graph.nodes.insert(0, transpose_node)\n",
    "\n",
    "    # Optional: clean graph\n",
    "    graph.cleanup().toposort()\n",
    "\n",
    "    # Save the fixed model\n",
    "    onnx.save(gs.export_onnx(graph), output_path)\n",
    "    print(f\"✅ Saved transposed model to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    insert_nhwc_to_nchw_transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "674d0322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧵 Path from model input to `node_Conv_1`:\n",
      "\n",
      "🔹 Node: node_Conv_1 (Conv)\n",
      "  Input[0]: pixel_values, shape=[1, 3, 224, 224], dtype=float32\n",
      "  Input[1]: backbone.embeddings.patch_embeddings.projection.weight, shape=[768, 3, 14, 14], dtype=float32\n",
      "  Input[2]: backbone.embeddings.patch_embeddings.projection.bias, shape=[768], dtype=float32\n",
      "  Output[0]: conv2d, shape=[1, 768, 16, 16], dtype=float32\n",
      "  Attributes: {'group': 1, 'auto_pad': 'NOTSET', 'dilations': [1, 1], 'strides': [14, 14], 'pads': [0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx_graphsurgeon as gs\n",
    "\n",
    "def forward_trace_to_target(model_path=\"./quant/basefp32_slimmed.onnx\", target_name=\"node_Conv_1\"):\n",
    "    model = onnx.load(model_path)\n",
    "    graph = gs.import_onnx(model)\n",
    "\n",
    "    # Map input name → consumer node(s)\n",
    "    input_to_consumers = {}\n",
    "    for node in graph.nodes:\n",
    "        for inp in node.inputs:\n",
    "            input_to_consumers.setdefault(inp.name, []).append(node)\n",
    "\n",
    "    visited = set()\n",
    "    path = []\n",
    "\n",
    "    def dfs_forward(var_name):\n",
    "        consumers = input_to_consumers.get(var_name, [])\n",
    "        for consumer in consumers:\n",
    "            if consumer.name in visited:\n",
    "                continue\n",
    "            visited.add(consumer.name)\n",
    "            path.append(consumer)\n",
    "\n",
    "            if consumer.name == target_name:\n",
    "                return True  # Stop once we reach target\n",
    "\n",
    "            # Continue recursively through this node’s outputs\n",
    "            for out in consumer.outputs:\n",
    "                if dfs_forward(out.name):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    # Start from all model inputs\n",
    "    for inp in graph.inputs:\n",
    "        if dfs_forward(inp.name):\n",
    "            break\n",
    "\n",
    "    # Print path\n",
    "    print(f\"\\n🧵 Path from model input to `{target_name}`:\")\n",
    "    for node in path:\n",
    "        print(f\"\\n🔹 Node: {node.name} ({node.op})\")\n",
    "        for i, inp in enumerate(node.inputs):\n",
    "            print(f\"  Input[{i}]: {inp.name}, shape={inp.shape}, dtype={inp.dtype}\")\n",
    "        for i, out in enumerate(node.outputs):\n",
    "            print(f\"  Output[{i}]: {out.name}, shape={out.shape}, dtype={out.dtype}\")\n",
    "        if node.attrs:\n",
    "            print(f\"  Attributes: {dict(node.attrs)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    forward_trace_to_target()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
